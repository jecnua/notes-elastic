Prometheus (OBSOLETE - TO BE CLEANED)
|| Last update: 13 Feb 2019

* Disclaimer

While this article is highly opinionated, it's based on a LOT of reading
(+ conferences) and by following this specific technology a lot.

* Intro

.image images/prometheus.png 100 _

Prometheus is a cloud aware monitoring and alerting tool.
Before docker and k8s, Soundcould had a sort of heroku infrastructure.
They used nagios, statsd and the sort but finding all the usual limitations we
all find when using these tools.

** Backing

Prometheus is the second project in [[https://www.cncf.io/][CNCF]] after k8s.
There is a lot of investments and backing behind it, being supported out of the
box by k8s, cAdvisor and other mainstream technologies.

** What it is

Prometheus is a time based monitoring ecosystem that pulls metrics out of
your systems *to* give:

- instrumentation
- storage
- querying
- alerting (via alert manager)
- dashboards

It _FOCUS_ on operation system monitoring but can be used in other contexts.

** What it's not

Prometheus is not a time series database.

To quote someone I don't remember now:

    "Prometheus is not a time series database, it just happens to use one."

** Selling points

- key=value pair on any metrics (came form, pertaining to)
- PromQL query language
- Very efficient on a single node
- Operation simplicity (single static library and small config file)
- Cloud aware (getting the metadata from autodiscovery systems)

** What it doesn't do

- no collection of individual events (ip address, exact path and so on)
- no request tracing
- no magic anomaly detection
- no durable long-term storage
- no automatic horizontal scaling
- no user/auth management

** Limitations

- Only numeric time series

* Architecture

** Text over http

Prometheus chose to use http as a transport.

** Scaling?

It doesn't scale in the sense that it doesn't shard. Every instance is self
contained and autonomous. Run 1 or more, but they should be all the same if you
want HA.

** Query language

not a sql style query language
PromQL is very good to do arithmetics between rating
will join them up in the same metrics (?)

does only read

** Pull vs push

- automatic upness monitoring
- horizontal monitoring (tell to push to different things) WRONG
- more flexyble (production monitoring on your laptop ? ) NO, NOT SG
- simple HA - two Prometheus server pulling in the same one (many of the same configured service)
- less configuration (tell my mornitoring service - do that in any case) NO. zk
- scales

From version 2.0 Prometheus DOES NOT SUPPORT PROTOBUF anymore.

Prometheus push metrics to _something_else_
replay your historical metadata (???)
flag for turning local storage
just turn local storage and only push remotely

digital ocean
VULCAN
cassandra/elasticsearch

bosun didn't take it

** Storage

Local storage is epehemerial.
They don't reccomend keeping it for years

problem: not enough disk space

you can't dump and you can't redeploy

one workaroud:
many prometheus server that federates on a single ...
one Prometheus that pulls every x minues

** Alerts

Prometheus implements alerting via an aplication called alert manager.

- [[https://prometheus.io/docs/alerting/overview/]]

In this context an alert is a query that return a timeseries.
When you add a query this way you can have two results:

- no timeseries: all ok
- some timeseries: an alert for each one of them

Alert manager is a different service but part of the same binary.

To make alert manager to point to the correct Prometheus DNS (and not the pod
id/name) in Helm you need to set the option _baseURL_:

    server:
      baseURL: https://prometheus.mydomain.com

- [[https://github.com/helm/charts/tree/master/stable/prometheus]]

** Pull method

run on every network segment
fewer step as possible

- open port in firewll/router
- open VPN tunnel

** Uber exportors or per process exporters

main uber exporter

not only one (for them)
- operation bottleneck
- spof, no isolation
- can't scrape selectively (not because is only us)
- harder to associate metadaat

phtometeus by discovering your node
with auto-discovery
so can put metadata from this process
they advice: *one*exporter*per*process*

* Exposition formats

- [[https://prometheus.io/docs/instrumenting/exposition_formats/]]
- [[https://github.com/RichiH/OpenMetrics/blob/master/markdown/protobuf_vs_text.md]]

From version 2.0 Prometheus DOES NOT SUPPORT PROTOBUF anymore.

** Why not json

- [[https://www.youtube.com/watch?v=4DzoajMs4DM&feature=youtu.be&t=11m59s]]

- json is bad for both
- json is not really streamable
- with text you can start processing line by line
- json you need it all together

** client libraries

client libs keep state but not much (no history, only current state)

    - Counter: cumulative metric that represents a single numerical value that only ever goes up
    - Gauge: represents a single numerical value that can arbitrarily go up and down
    - Histogram: samples observations and counts them in configurable buckets
    - Summary: similar to a histogram, samples observations, it calculates configurable quantiles over a sliding time window
    Source: https://blog.risingstack.com/node-js-performance-monitoring-with-prometheus/

rates not computes!!! data as raw as possible and you do in on Prometheus side

metrics for you
manage metrics for you
pre-aggregation is more efficient

* Gathering metrics

When we speak about collecting metrics we consider two types:

- _whitebox_ instrumentation: applications/systems expose their metrics
- _blackbox_ instrumentation: use exporter (or batch jobs)

There is some limitations when working with a scraping system:

CON

- *ONLY* the latest metric of a type and *NO*HYSTORY*

PRO

- idempotent
- many Prometheus scraping the smae node have the same data

Prometheus can gather data by any source that exposes metrics in it's format.
When this is not available, _exporters_ can be written to do the heavy
lifting of _translating_ the data from another format (or gather the data
directly).

Examples of application exposing metrics natively for Prometheus:

- [[https://github.com/google/cadvisor][cAdvisor]]: Container metrics
- [[http://kubernetes.io/][Kubernetes]]

** From an exporter

An examples of exporters is the [[https://github.com/prometheus/node_exporter][Node exporter]]
 to gather node metrics.

All exporters can be found [[https://prometheus.io/docs/instrumenting/exporters/][here]].

** From an application

If you are creating a new app and you want Prometheus to gather your metrics,
the convention is to expose a */metrics* endpoint with the metrics you want.

    The mentality:
    Dev own the metrics

** Prometheus federation: data from another Prometheus

Prometheus can grab metric from another instance using what it's called
[[https://prometheus.io/docs/operating/federation/][federation]]. This allow to
have redundancy (HA) in the system or machines _specialised_ in a
subset of the metrics.

*** A Prometheus on every laptop?

DEVs can spin their own Prometheus to read their own metrics. Let's say you
have a team working on a project. They can spin a Prometheus instance locally
pulling the data ONLY from these services/endpoints.

This can help if you want to isolate environments (e.s. DEVs cannot see some
metrics) or just to have a less cluttered view of what's out there.

* How to approach Prometheus

- Start with grafana
- Show how HA works
- Share the success stories
- Help DEVs see and understand how to use the tool

** Download the Grafana dashboards

They are just json so you can just pull from the grafana.net website or any
github repo. You can use the api to push them on the app or do it manually.

** Monitoring nodes (OBSOLETE - 2016)

You may (_will_) have many exporter on a single node, each of them exposing the
metrics of a single application that is not natively aware of Prometheus.
However, one exporter you want for sure is the *node-exporter*.

The node-exporter _exports_ the metrics of the node and is indeed something
you want to have on any of your machine.

* Labels

- [[https://prometheus.io/docs/practices/naming/#labels]]
- [[https://prometheus.io/docs/practices/instrumentation/#use-labels]]

Prometheus embrace the _label_data_model_ and NOT the _hierarchical_data_model_.
It is:

- more flexible (allow cross cutting)
- more efficient
- explicit dimension

What you have to do with grafite is like this:

    api-server.*.*.post.*

Unique combinations of key-value creates new time series in Prometheus.
Prometheus doesn't like is high-cardinality labels.

    Prometheus considers each unique combination of labels and label value as a
    different time series. As a result if a label has an unbounded set of
    possible values, Prometheus will have a very hard time storing all these
    time series. In order to avoid performance issues, labels should not be used
    for high cardinality data sets (e.g. Customer unique ids).
    Source: https://blog.pvincent.io/2019/05/prometheus-workshops-follow-up-frequently-asked-questions/

You should be careful with label and do not exaggerate with them:

    CAUTION: Remember that every unique combination of key-value label pairs
    represents a new time series, which can dramatically increase the amount of
    data stored. Do not use labels to store dimensions with high cardinality
    (many different label values), such as user IDs, email addresses, or other
    unbounded sets of values.
    Source: https://prometheus.io/docs/practices/naming/#labels

- [[https://prometheus.io/docs/practices/instrumentation/#do-not-overuse-labels]]

** Best practices

- Labels should rarely have more than 100 possible values
- The set of label values should be bounded (think of them as enums, and not open ended sets of strings)

* Prometheus on k8s

You can use:

- [[https://github.com/coreos/prometheus-operator][Operator]]
- Helm chart
- Manually

** Helm chart

- [[https://github.com/helm/charts/tree/master/stable/prometheus]]

You can install prometheus simply by running:

    helm install stable/prometheus

Some possible options:

    helm upgrade <name-generated> stable/prometheus \
    --set server.ingress.enabled=true \
    --set server.ingress.hosts='{prom-URI}' \
    --set alertmanager.ingress.enabled=true \
    --set alertmanager.ingress.hosts='{am-URI}' \
    -f values.yaml

*NOTE*: Array are passed via [[https://github.com/kubernetes/helm/blob/master/docs/using_helm.md#the-format-and-limitations-of---set][curly braces]].
