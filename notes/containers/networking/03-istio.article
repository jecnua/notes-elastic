Istio (NEEDS UPDATE)
|| Last update: 2 Oct 2018

* Intro

- [[https://istio.io][Istio website]]
- [[https://github.com/istio/istio][Istio github]]
- [[https://discuss.istio.io/]]

Istio is a open-source service mesh, which is architected similar to other
service-mesh implementations with a control plane and a data plane. The data
plane consists of the proxies that live with each application instance and is in
the request path. The control plane lives outside of the request path and is
used to administer and control the behavior of the data plane.

** Components

From 1.5+:

- Istiod

Before 1.5:

- Pilot - the core data-plane config (xDS) server
- Galley - configuration watching, validation, forwarding
- Injector - responsible for auto-injecting the data plane and setting up bootstrap
- Citadel - certificate signing, secret generation, integration with CAs, etc
- Telemetry - a “mixer” component responsible for aggregating and syndicating telemetry to various backends
- Policy - a request-path “mixer” component responsible for enforcing policy

* Istio 1.5+

Instead of mixer adapters in 1.5 they moved towards a model that enables
extension directly in the proxies instead.

- [[https://istio.io/docs/reference/config/telemetry/metrics]]

Allows you to send telemetry to Prometheus and Stackdriver.

    [...] the new telemetry model reduces our latency dramatically and gives us
    industry-leading performance, with 50% reductions in both latency and CPU
    consumption.
    Source: https://istio.io/blog/2020/tradewinds-2020/

    The model that replaces Mixer uses extensions in Envoy to provide even more
    capability. The Istio community is leading the implementation of a
    WebAssembly (Wasm) runtime in Envoy, which lets us implement extensions that
    are modular, sandboxed, and developed in one of over 20 languages. Extensions
    can be dynamically loaded and reloaded while the proxy continues serving
    traffic. Wasm extensions will also be able to extend the platform in ways
    that Mixer simply couldn’t. They can act as custom protocol handlers and
    transform payloads as they pass through Envoy — in short they can do the same
    things as modules built into Envoy.
    [...]
    By changing the extension model, we were also able to remove dozens of CRDs.
    You no longer need a unique CRD for every piece of software you integrate
    with Istio.
    Source: https://istio.io/blog/2020/tradewinds-2020/

To be retrocompatible you can still install mixer in 1.5:

    Installing Istio 1.5 with the ‘preview’ configuration profile won’t install
    Mixer. If you upgrade from a previous release, or install the ‘default’
    profile, we still keep Mixer around, to be safe. When using Prometheus or
    Stackdriver for metrics, we recommend you try out the new mode and see how
    much your performance improves.
    Source: https://istio.io/blog/2020/tradewinds-2020/

** The new control plane

.image images/istiod.png 600 _
.caption Source: [[https://istio.io/blog/2020/tradewinds-2020/]]

They combined several of the control plane components into a single component:
*Istiod*. It includes the features of Pilot, Citadel, Galley, and the sidecar
injector.

    To reduce the per-node footprint, we are getting rid of the node-agent, used
    to distribute certificates, and moving its functionality to the istio-agent,
    which already runs in each Pod.
    Source: https://istio.io/blog/2020/tradewinds-2020/

In the istiod approach, functionality previously assigned to various microservices
deployments will be coalesced into a single daemon.

    Note that the services that made up the previous control planes are still
    implemented as sub-modules within the project (complete with boundary and
    contracts, etc) but the operational experience is improved. An operator now
    needs to worry about running and upgrading a single binary vs a collection
    of them.
    Source: https://blog.christianposta.com/microservices/istio-as-an-example-of-when-not-to-do-microservices/

** Updates

    To improve Istio’s life-cycle management, we moved to an operator-based
    installation. We introduced the IstioOperator CRD and two installation modes:
    Human-triggered: use istioctl to apply the settings to the cluster.
    Machine-triggered: use a controller that is continually watching for changes
    in that CRD and affecting those in real time.
    In 2020 upgrades will getting easier too. We will add support for “canarying”
    new versions of the Istio control plane, which allows you to run a new version
    alongside the existing version and gradually switch your data plane over to
    use the new one.
    Source: https://istio.io/blog/2020/tradewinds-2020/

* Istio before 1.5

Istio is a:

- Network mesh
- Cluster wide policy enforcer (for access control)
- In depth telemetry exporter

Istio was announced in the May of 2017.
Istio is not a k8s technology and can work with other orchestrators.

UPDATE: Requirements wise Istio is much more costly than Linkerd2
[[https://medium.com/@michael_87395/benchmarking-istio-linkerd-cpu-c36287e32781][Benchmarking Istio & Linkerd CPU (Apr 2019)]]

* Data plane and control plane

It has received a lot of attention, and other data planes have begun
integrations as a replacement for Envoy (both Linkerd 1.x and NGINX have
demonstrated Istio integration)

- Data planes: Linkerd, NGINX, HAProxy, [[https://www.envoyproxy.io/][Envoy]], Traefik
- Control planes: Istio, Nelson, SmartStack

The goal of a control plane is to set policy that will eventually be enacted by the data plane. Provides policy and configuration for all of the running data planes in the mesh. Does not touch any packets/requests in the system.

Default proxy of Istio is Envoy.

    The network should be transparent to applications.
    When network and application problems do occur it
    should be easy to determine the source of the problem.
    — Envoy

Linkerd one of the first (early 2016 - [[https://buoyant.io/2016/02/18/linkerd-twitter-style-operability-for-microservices/]]).
Envoy 6 months later ([[https://eng.lyft.com/announcing-envoy-c-l7-proxy-and-communication-bus-92520b6c8191]]) (but in prod in left since 2015).

Nelson and SmartStack help further illustrate the control plane vs. data plane divide.
Nelson uses Envoy as its proxy and builds a robust service mesh control plane around the HashiCorp stack (i.e. Nomad, etc.).
SmartStack was perhaps the first of the new wave of service meshes. SmartStack forms a control plane around HAProxy or NGINX

* What is a service mesh

- [[https://en.wikipedia.org/wiki/Mesh_networking][Wikipedia: Mesh networking]]
- [[https://buoyant.io/2017/04/25/whats-a-service-mesh-and-why-do-i-need-one/]]

    A service mesh is a dedicated infrastructure layer for handling
    service-to-service communication. It’s responsible for the reliable
    delivery of requests through the complex topology of services that
    comprise a modern, cloud native application. In practice, the service
    mesh is typically implemented as an array of lightweight network proxies
    that are deployed alongside application code, without the application
    needing to be aware.
    Source: https://buoyant.io/2017/04/25/whats-a-service-mesh-and-why-do-i-need-one/

.image images/service_mesh_color.png
.caption An example of mesh network communication via sidecar

The control plane enables the proxies to implement among others access control and metrics collection.

Using the Istio approach, having a sidecar in each pod (in the kubernetes case), will allow you to implement a high-level and customisable mesh network being completely language agnostic and being transparent to the user and to the developer.

.image images/service_mesh_black.png

The following is instead a visual of the communication proxies have with the control plane.

.image images/service_mesh_generic.png

* What is Istio

Istio is a distribuited control plane to configure envoy system records to
achieve a service mesh.

.image images/old_istio.png _ 750
.caption Source: [[https://istio.io/blog/2020/tradewinds-2020/]]

.image images/istio_arch.svg _ 750
.caption Source: [[https://istio.io/]]

.image images/all_istio.png _ 750
.caption Source: [[https://www.youtube.com/watch?v=IfPt3z6UAGw][JavaDay UA 2017: 8 Steps to Become Awesome with Kubernetes (Burr Sutter)]] (this diagram is obsolete)

It helps to separate:

- Run-time
- Configuration
- Context

Like:

- Routes
- Firewall routes
- Security context

From the business code itself.

* Installing Istio (This section needs to be updated and may be obsolete)

** Requirements

Requires kubernetes *1.7.4+* (this information may be obsolete)

Istio uses a CRD (CustomResourceDefinitions) so it needs k8s 1.7.4+.

** Helm

- [[https://github.com/kubernetes/charts/tree/master/incubator/istio]]
- [[https://istio.io/docs/setup/kubernetes/sidecar-injection.html#automatic-sidecar-injection]]

Helm is a two step installation.

    #https://github.com/kubernetes/charts/blob/master/incubator/istio/templates/service-account/initializer.yaml
    helm install --name istio \
      incubator/istio \
      --devel \
      --namespace istio-system \
      --set istio.release=0.4.0,initializer.enabled=true,rbac.install=true
    helm upgrade istio incubator/istio --devel --reuse-values --set istio.install=true

If you are not on AWS the update can be:

    helm upgrade istio incubator/istio --devel --reuse-values --set istio.install=true --set ingress.service.type=NodePort

THIS????

    kubectl create clusterrolebinding add-on-cluster-admin --clusterrole=cluster-admin --serviceaccount=kube-system:default
    helm install --name istio incubator/istio --devel --namespace istio-system --set istio.release=0.4.0,initializer.enabled=true

*** Possible errors

    Error: UPGRADE FAILED: apiVersion "admissionregistration.k8s.io/v1alpha1"
    in istio/templates/initializer/initializer.yaml is not available

In case you don't have the initialiser (alpha) enabled.

*** How to test it worked

    cd Downloads/istio-0.4.0/
    kubectl get deployment sleep -o yaml
    kubectl apply -f samples/sleep/sleep.yaml
    kubectl get pods
    echo $(kubectl get deployment sleep -o jsonpath='{.metadata.annotations.sidecar\.istio\.io\/status}')

* Structure

There are two main concept to keep in mind. There is a control plane and a data plane.

- envoy (data plane)
- pilot
- mixer

istio is a unified control plane.

** Envoy

.image images/envoy.svg _ 750
.caption Source: [[https://istio.io/]]

- [[https://www.envoyproxy.io/]]

    Envoy is a self contained process that is designed to run alongside every
    application server. All of the Envoys form a transparent communication mesh in
    which each application sends and receives messages to and from localhost and
    is unaware of the network topology.

Envoy is a distruited data plane.
Data planes by themselves do nothing. They need to be configured by something larger.
Other example of data plane are:Linkerd, NGINX, HAProxy.

Service mesh data plane (via sidecar):

- service discovery
- health checking
- routing
- load balancing
- authentication and authorisation
- observability (statistic, trading and logging)

While _haproxy_ and _nginx_ are *config*file*hub*, envoy is made to receive
orders from pilot. The data plane is responsible for conditionally translating,
forwarding, and observing every network packet that flows to and from a service instance.

Envoy lives in a sidecar in each pod. There are two ways to configure it:

- use a tool to inject
- using an initialiser (kubernetes)

.image images/sidecar.png _ 750
.caption Source: [[https://medium.com/@mattklein123/service-mesh-data-plane-vs-control-plane-2774e720f7fc]]

The initialiser works by telling k8s that before you run a pod, inject the stuff.
The user doesn't need to know. When _things_ change, envoy changes.

envoy is both a _sidecar_ for all the data and serve as _ingress_ and _egress_.

Envoy knows of all the other envoys and so on.

*** How does it work

Sidecars in this case are network proxy.

All network traffic flows via it’s local sidecar proxy to the appropriate destination.
Service instances are not aware of the network at large and only knows about it’s local proxy.
The distributed system has been abstracted away from the service programmer.

** Pilot

.image images/pilot.svg _ 750
.caption Source: [[https://istio.io/]]

_Envoy_ has been fully rewritten in C - is connected to a distribuited system and
highly dinamically configurable.

Pilot is a system of record for the server mesh. All envoy receive the same
configuration and dinamically changes the configs for envoy.

Pilot is also abstracted from the underlying platform and use adpters to
communicate with the platform. istio has a kubernetes adapter but IS NOT made for
k8s.

.image images/control_plane.png _ 750
.caption Source: [[https://medium.com/@mattklein123/service-mesh-data-plane-vs-control-plane-2774e720f7fc]]

istio and translate in ingress and other network rules is something Istio can
understand.

Plus, it has a plugging api for routing and other stuff like:
- black hole
- ingestion

** Mixer

.image images/mixer.svg _ 250
.caption Source: [[https://istio.io/]]

Is the policy engine: telemetry, billing, quota and so on.
*Mixer* is the component of Istio that takes decisions.
Mixer has plugins for tracing, prometeus and much more.

    Mixer necessarily added overhead to requests because it required extensions
    to be out-of-process.
    Source: https://istio.io/blog/2020/tradewinds-2020/

Modularity via adapters is possible.

- [[https://istio.io/docs/reference/config/policy-and-telemetry/mixer-overview/#adapters]]

There is a lot of metadata that is aggregated and send it to mixer so you can do
rules and so on.

All expressed under the hood as CRDs.

* Capabilities

.image images/capabilities.png

** Mutual TLS

.image images/mutual_tls.png _ 800
.caption Source: [[https://www.youtube.com/watch?v=wq1nlJNvy1w][Setting Sail with Istio [B] - Lachlan Evenson, Microsoft]]

Before this was before based on top of vault, but is was clunky and not working
well.

- it uses k8s enabled stuff
- all service communication and tsl
- all communication is encrypted
- and lifecycle of certificates is managed by istio

** Deployments

Istio can give you out of the box tools like _canary_deployments_.
Sending a percentage of the traffic to a new version of your service.

K8S already have _rolling_deployments_ under the cover, but what if you want to filter traffic by:

- tokens
- headers
- network

and much more, Istio can help you.

** Access control

k8s have _network_policies_ but they have problems:

- They are op focused (and devs usually don't do them)
- They are more cluster side
- Difficult to write

** Ingress

Istio doesn't work with nginx ingress containers (http1 problem?).

Some people replaced nginx with Traefik (very low footprint)

** Egress

While k8s have ingresses, it does not have egress. *Egress* is a new way to funnel all
the traffic exiting my cluster through a deployment group.

As a DEV they know if an application needs or not external access, and this allow them to
_close_ egress access.

** Telemetry

Istio will give you access to app telemetry without having to instrument the
application itself. You can add and have this metrics without having to go
through the DEVs.

** Tracing

Tracing is not about debugging you application, is about debugging the _process_.

Debugging is hard in k8s:

- kubectl proxy
- run a debug pod
- inject debug (sidecar) containers
- mTLS is hard!

Istio will inject headers, but you need to *forward* them in the code.

    NO FRAMEWORK WILL SAVE YOU FROM BAD CODE

** API gateway

    API gateway ~= Ingress

* Technologies

- consul support
- opentracing
- vault support
- nginx
- prometheus
- spify ???? (secrets?) identy to application

sits in datapath, map events

* Istio and Rancher

It will probably be integrated in Rancher soon.

    # From a QA in October 2017
    Q: Any plans on supporting Istio in Rancher 2.0?
    A: :)

* Usage

How do you make it so that is applies?
It uses sidecars.
You build a config pushed on every node.

You don't need to instrument your code
Attach functionality to the system

attach sidecar without changing the yaml
add a second container
grab info from the data plane

add zipkin so you have tracing

** Option 1: Initialisers

*From*v0.2*only*

For the newer versions you can use [[https://kubernetes.io/docs/admin/extensible-admission-controllers/#enable-initializers-alpha-feature][_initialisers_]]
to avoid having to inject yaml with istioctl all the time. _initialisers_ are pretty new to k8s so you may
need to abilitate them via the alpha API.

_initialisers_ allow you to have *automatic*sidecar*injection* in all your pods.

    before ???

** Option 2: The old way

    istioctl kube-inject -f


* NOTES FROM WEBINAR

kubernetes primitives
istio
knative paas abstraction on top of istio

istio gives very deep observability

istio
layer 7 path based routing
traffic shaping
LB (a/b testing, canary)

manage telemetry
fleet wide visibility (zipkin, prometheus and grafana)

security: identity based service access control
service authorisation API level access control
service-service encryption with TLS (mTLS)

securing east-west traffic

https://cloud.google.com/security/encryption-in-transit/application-layer-transport-security/
https://groups.google.com/forum/#!topic/grpc-io/FRiBpXucIRk/discussion

istio implements
star xDS API

listening discorvery api

they can feed enovy wuith the data
the auto-inject6
mutatuion addition controller

they do a redirection inside the netwrok namespace
can upgrade the http protocol
manage grpc
add telemetry

the price is
network latency
cpu
memory

some protocol don't go vai the proxy (istio)
istio is not a full end to end
more sa serice to service access control

if you run pod as root
you get the ability to run users
like the user of envoy
so you can exec in the containers
create an evoy user with it's id and now you control the mesh

don't run in the container as id 1

ipv6 is another escape method
iptable no kubernetes (nor envoy) doesn't look at ipv6

run pod rootless?? requires some dev effort

citadel provision xxx
the you can provision the certificate and so on
is about getting you fqdg and gets the cert
it will uyse the service name identification

is the injection of envoy that rewrite he iptables
and not istio

* Destination Rule

- [[https://istio.io/docs/reference/config/networking/v1alpha3/destination-rule/]]

    DestinationRule defines policies that apply to traffic intended for a
    service after routing has occurred. These rules specify configuration for
    load balancing, connection pool size from the sidecar, and outlier detection
    settings to detect and evict unhealthy hosts from the load balancing pool.
    [...]
    Version specific policies can be specified by defining a named subset and
    overriding the settings specified at the service level.
    [...]
    Policies specified for subsets will not take effect until a route rule
    explicitly sends traffic to this subset.
    Source: https://istio.io/docs/reference/config/networking/v1alpha3/destination-rule/

You can also use subset to test different versions of an app.

* Notes in 2019

- [[https://itnext.io/understanding-application-routing-in-istio-aade30d594f4]]

kubectl create namespace test
kubectl label namespace test istio-injection=enabled

    apiVersion: v1
    kind: Service
    metadata:
      name: nginx
      labels:
        app: nginx
    spec:
      ports:
      - port: 80
        name: http
      selector:
        app: nginx
    ---
    apiVersion: extensions/v1beta1
    kind: Deployment
    metadata:
      name: nginx-v1
    spec:
      replicas: 1
      template:
        metadata:
          labels:
            app: nginx
            version: v1
        spec:
          containers:
          - name: nginx
            image: nginx
            imagePullPolicy: IfNotPresent
            ports:
            - containerPort: 80
    ---
    apiVersion: extensions/v1beta1
    kind: Deployment
    metadata:
      name: nginx-v2
    spec:
      replicas: 1
      template:
        metadata:
          labels:
            app: nginx
            version: v2
        spec:
          containers:
          - name: nginx
            image: nginx
            imagePullPolicy: IfNotPresent
            ports:
            - containerPort: 80

Istio specific:

    ---
    # https://istio.io/docs/reference/config/networking/v1alpha3/destination-rule/
    apiVersion: networking.istio.io/v1alpha3
    kind: DestinationRule
    metadata:
      name: fab-destination
    spec:
      host: fab-nginx
      subsets:
      - name: v1
        labels:
          version: v1
      - name: v2
        labels:
          version: v2
    ---
    apiVersion: networking.istio.io/v1alpha3
    kind: Gateway
    metadata:
      name: fab-gateway
    spec:
      selector:
        istio: ingressgateway # use istio default controller
      servers:
      - port:
          number: 80
          name: http
          protocol: HTTP
        hosts:
        - "*"
    ---
    apiVersion: networking.istio.io/v1alpha3
    kind: VirtualService
    metadata:
      name: fab-vs
    spec:
      hosts:
      - "*"
      gateways:
      - fab-gateway
      http:
      - route:
        - destination:
            host: fab-nginx
            subset: v1
          weight: 100
    #    - destination:
    #        host: webapp
    #        subset: v2
    #      weight: 50

        Once our deployment is up and running we have to add a destination
        rule so Istio knows about our application. Istio will now internally
        assign a DNS name to the application. The name will be made up of the
        application name, hostname (taken from our deployment below) and
        namespace. It will be appended like so <namespace>.svc.cluster.local.
        [...]
        Now we will create the Istio gateway. This will define the inbound
        port the application will be listening on and the hosts we will route
        to. In our case, it will be port 80 and we will use a * to hit any host.
        We are also going to tie our gateway to the default Istio ingress
        gateway.
        [...]
        Lastly, we will create our Istio virtual service. This defines how we
        are going to route the traffic on weight.



NOTE
Gateway configure the edge
VirtualService configure the routing



          To configure Istio’s Gateway to allow traffic into the cluster and through the service mesh, we’ll start by exploring two concepts: Gateway and VirtualService. Both are fundamental, in general, to getting traffic to flow in Istio, but we’ll look at them only within the context of allowing traffic into the cluster.
          Source: https://freecontent.manning.com/istio-gateway/

The Istio Gateway is what tells the
istio-ingressgateway pods which ports to open up and for which hosts.

    To configure a Gateway in Istio, we use the Gateway resource and specify which ports we wish to open on the Gateway, and what virtual hosts to allow for those ports
    This Gateway definition is intended for the istio-ingressgateway which was created when we set up Istio initially, but we could have used our own definition of Gateway. We can define to which gateway the configuration applies by using the labels in the selector section of the Gateway configuration. In this case, we’re selecting the gateway implementation with the label istio: ingressgateway which matches the default istio-ingressgateway. The Gateway for istio-ingressgateway is an instance of Istio is service proxy (Envoy)
    Source: https://freecontent.manning.com/istio-gateway/


Investigating and poking around reveals that the first gateway from bookinfo above is swallowing all traffic.
beware of using *
Better to avoid using *.domain in hosts, unless you have only a single gateway.
So this has opened up a port on our ingress gateway in Istio, but when traffic
hits this gateway, it will have no idea where to send it

Now that our Gateway is ready to receive traffic, we have to inform it where
traffic should go when it does receive it. Istio configures this with a type
called “VirtualService”. By including a list of gateways a virtual service
config should be applied to, Istio then configures those gateways with the
routes defined in the VirtualService configuration.

The http block then defines all of the matches on URI
prefixes that should then be routed to the given destination. When we apply
this, we can use the Envoy admin to see this take affect again. Open up
http://localhost:15000/config_dump and
look for the “routes” portion of the printed JSON

So Istio has taken our VirtualService definition and applied it to our gateway
pod that it matched on the name. Success!


    The Gateway: Istio Gateway’s are responsible for opening ports on relevant
    Istio gateway pods and receiving traffic for hosts. That’s it. However, they’re
    the critical link between received traffic and routing.
    The VirtualService: Istio VirtualService’s are what get “attached” to
    Gateways and are responsible defining the routes the gateway should implement.
    You can have multiple VirtualServices attached to Gateways. But not for the
    same domain.

The Ingress controller is, basically, a reverse-proxy that runs in a cluster and configures routing rules according to Ingress resources. Istio provides two ways of ingressing traffic into your cluster.

    First one, istio-ingress, is a traditional ingress controller like nginx-ingress, traefik or controur. This controller runs in your cluster and listens to all the changes to Ingress resources from the Kubernetes API and sends incoming traffic according to these rules. You can easily swap one ingress controller to another
    Istio documentation discourages use of this method as a “legacy way” and suggests using the second one.
    The second one, istio-ingressgateway, is also an ingress controller, but unlike traditional ones, it does not rely on native Kubernetes Ingress objects. It uses its own custom resources: Gateway, VirtualService, DestinationRule, etc. Using custom resources provides more flexibility in configuring network policies
    Source: https://blog.lwolf.org/post/switching_to_istio_as_the_primary_ingress/

To expose a service using ingressgateway you have to create at least 2 objects - Gateway and a VirtualService.

    port naming is important, it is used in some routing logic inside the istio.
    The port names must be of the form protocol-suffix with grpc, http, http2, https, mongo, redis, tcp, tls or udp as the protocol in order to take advantage of Istio’s routing features.
    For example, name: http2-foo or name: http are valid port names, but name: http2foo is not. If the port name does not begin with a recognized prefix or if the port is unnamed, traffic on the port will be treated as plain TCP traffic (unless the port explicitly uses Protocol: UDP to signify a UDP port).
    Source: https://blog.lwolf.org/post/switching_to_istio_as_the_primary_ingress/

You don't really need a routexxx

example with http bin
https://github.com/istio/istio/blob/master/samples/httpbin/httpbin-gateway.yaml

problem with certs https://github.com/istio/istio/issues/13113 https://github.com/istio/istio/issues/15148


    Before the 0.8 release, Istio used Kubernetes Ingress resources to configure external traffic. Kubernetes Ingress provides a single entrance for external traffic, but it also has some significant shortcomings：
    Kubernetes Ingress can’t be managed by the Istio control plane. It needs to be configured with the Kubernetes Ingress rules. As a result, there are two sets of independent routing configurations in the system, one for the entrance and one for the sidecar proxies inside the mesh. The operations of the service mesh are much more complicated in this way.
    Kubernetes Ingress can only provide very basic layer 7 capabilities. It doesn’t have the same functionalities as mesh sidecars including advanced routing rules, distributed tracing, policy checking and metrics collections.
    Source: https://medium.com/@zhaohuabing/which-one-is-the-right-choice-for-the-ingress-gateway-of-your-service-mesh-21a280d4a29c

    To address these concerns, Istio Gateway resource has been introduced in the 0.8 release to replace Kubernetes ingress.

Advanced traffic routing
fault injection
others

    This far, all we’ve done is configure the Istio Gateway to expose a specific port, expect a specific protocol on that port, and define specific hosts to serve from the port/protocol pair. When traffic comes into the gateway, we need a way to get it to a specific service within the service mesh, and to do that we’ll use the VirtualService resource. In Istio, a VirtualService resource defines how a client talks to a specific service through its fully qualified domain name, which versions of a service are available, and other routing properties (like retries and request timeouts). It’s sufficient to know that VirtualService allows us to route traffic from the ingress gateway to a specific service.
    Source: https://freecontent.manning.com/istio-gateway/

    The first issue is that the Kubernetes Ingress is a simple specification geared toward HTTP workloads. Each implementation of Kubernetes Ingress (like NGINX, Heptio Contour, etc) is geared toward HTTP traffic. In fact, Ingress specification only considers port 80 and port 443 as ingress points. This severely limits the types of traffic a cluster operator can allow into the service mesh. For example, if you’ve Kafka or NATS.io workloads, you may wish to expose direct TCP connections to these message brokers. Kubernetes Ingress doesn’t allow for that.
    [...]
    Second, the Kubernetes Ingress resource is severely underspecified. It lacks a common way to specify complex traffic routing rules, traffic splitting, or things like traffic shadowing. The lack of specification in this area causes each vendor to re-imagine how best to implement configurations for each type of Ingress implementation (HAProxy, Nginx, etc).
    Lastly, because things are underspecified, the way most vendors choose to expose configuration’s through bespoke annotations on deployments. The annotations between vendors varied and aren’t portable, and if Istio continues this trend there will be many more annotations to account for all the power of Envoy as an edge gateway.
    Ultimately, Istio decided on a clean slate for building ingress patterns and specifically separating out the layer 4 (transport) and layer 5 (session) properties from the layer 7 (application) routing concerns. Istio Gateway handles the L4 and L5 concerns, and VirtualService handles the L7 concerns.
    Source: https://freecontent.manning.com/istio-gateway/

Istio has replaced the familiar Ingress resource with new Gateway and VirtualServices resources.

    Inside the cluster the request is routed to the Istio IngressGateway Service which is listening on the port the load balancer forwards to.
    The Service forwards the request (on the same or a new port) to an Istio IngressGateway Pod (managed by a Deployment).
    The IngressGateway Pod is configured by a Gateway (!) and a VirtualService.
    The Gateway configures the ports, protocol, and certificates.
    The VirtualService configures routing information to find the correct Service
    The Istio IngressGateway Pod routes the request to the application Service.
    [...]
    Now we have reached the most interesting part in this flow, the IngressGateway. This is a fancy wrapper around the Envoy proxy and it is configured in the same way as the sidecars used inside the service mesh (it is actually the same container). When we create or change a Gateway or VirtualService, the changes are detected by the Istio Pilot controller which converts this information to an Envoy configuration and sends it to the relevant proxies, including the Envoy inside the IngressGateway.
    [...]
    VirtualService, it works in concert with the Gateway to configure Envoy. If you only add a Gateway nothing will show up in the Envoy configuration, and the same is true if you only add a VirtualService.
    VirtualServices are really powerful and they enable the intelligent routing that is one of the very reasons we want to use Istio in the first place.
    Source: https://blog.jayway.com/2018/10/22/understanding-istio-ingress-gateway-in-kubernetes/

Valid ports are, HTTP|HTTPS|GRPC|HTTP2|MONGO|TCP|TLS and more.

    # Port forward to the first istio-ingressgateway pod
    alias igpf='kubectl -n istio-system port-forward $(kubectl -n istio-system
    get pods -listio=ingressgateway -o=jsonpath="{.items[0].metadata.name}") 15000'

    # Get the http routes from the port-forwarded ingressgateway pod (requires jq)
    alias iroutes='curl --silent http://localhost:15000/config_dump |
    jq '\''.configs.routes.dynamic_route_configs[].route_config.virtual_hosts[]|
    {name: .name, domains: .domains, route: .routes[].match.prefix}'\'''

    # Get the logs of the first istio-ingressgateway pod
    # Shows what happens with incoming requests and possible errors
    alias igl='kubectl -n istio-system logs $(kubectl -n istio-system get pods
    -listio=ingressgateway -o=jsonpath="{.items[0].metadata.name}") --tail=300'

    # Get the logs of the first istio-pilot pod
    # Shows issues with configurations or connecting to the Envoy proxies
    alias ipl='kubectl -n istio-system logs $(kubectl -n istio-system get pods
    -listio=pilot -o=jsonpath="{.items[0].metadata.name}") discovery --tail=300'
    Source: https://blog.jayway.com/2018/10/22/understanding-istio-ingress-gateway-in-kubernetes/

https://istio.io/docs/tasks/traffic-management/

Ingress controller is replaced with two components named, Gateway and VirtualService. Gateways are used to configure the istio-proxies (envoys) while the VirtualServices are used to route the traffic.


    gateways	string[]
    The names of gateways and sidecars that should apply these routes. A single VirtualService is used for sidecars inside the mesh as well as for one or more gateways. The selection condition imposed by this field can be overridden using the source field in the match conditions of protocol-specific routes. The reserved word mesh is used to imply all the sidecars in the mesh. When this field is omitted, the default gateway (mesh) will be used, which would apply the rule to all sidecars in the mesh. If a list of gateway names is provided, the rules will apply only to the gateways. To apply the rules to both gateways and sidecars, specify mesh as one of the gateway names.
    Source: https://istio.io/docs/reference/config/networking/v1alpha3/virtual-service/#VirtualService

** How not to use the ingress in istio

The settings in VirtualService called _gateways_ allows you to do exactly that:

    The names of gateways and sidecars that should apply these routes. A single
    VirtualService is used for sidecars inside the mesh as well as for one or
    more gateways. The selection condition imposed by this field can be overridden
    using the source field in the match conditions of protocol-specific routes.
    The reserved word mesh is used to imply all the sidecars in the mesh. When
    this field is omitted, the default gateway (mesh) will be used, which would
    apply the rule to all sidecars in the mesh. If a list of gateway names is
    provided, the rules will apply only to the gateways. To apply the rules to
    both gateways and sidecars, specify mesh as one of the gateway names.
    Source: https://istio.io/docs/reference/config/networking/v1alpha3/virtual-service/#VirtualService

You can use *mesh* to use your normal ingress system.

    The reserved word mesh is used to imply all the sidecars in the mesh.
    Source: https://istio.io/docs/reference/config/networking/v1alpha3/virtual-service/#VirtualService

** configuration

.image images/isti-configuration.png
.caption Source: https://rinormaloku.com/istio-practice-routing-virtualservices/

        There should be a note as to what host is referring to. Why is host used in this manner? I am guessing because the virtual service could also be used for accessing resources outside a kb cluster? Since sa-frontend is really a service why is it selected via the term host? Also why port is mentioned at all since the service already defines the port?
        From istio docs on virtual service: Host - The address used by a client when attempting to connect to a service. Could you clarify more about this. Thanks.

        Update: Here are some more notes on this:Destination indicates the network addressable service to which the request/connection will be sent after processing a routing rule. The destination.host should unambiguously refer to a service in the service registry. Istio’s service registry is composed of all the services found in the platform’s service registry (e.g., Kubernetes services, Consul services), as well as services declared through the ServiceEntry resource.

        Note for Kubernetes users: When short names are used (e.g. “reviews” instead of “reviews.default.svc.cluster.local”), Istio will interpret the short name based on the namespace of the rule, not the service. A rule in the “default” namespace containing a host “reviews will be interpreted as “reviews.default.svc.cluster.local”, irrespective of the actual namespace associated with the reviews service. To avoid potential misconfigurations, it is recommended to always use fully qualified domain names over short names.
        Source: https://rinormaloku.com/istio-practice-routing-virtualservices/

* Resources

- [[https://istio.io/blog/2020/tradewinds-2020/]] - March 2020
- [[https://blog.christianposta.com/microservices/istio-as-an-example-of-when-not-to-do-microservices/]] - March 2020


- Video: [[https://www.youtube.com/watch?v=wq1nlJNvy1w][Setting Sail with Istio [B] - Lachlan Evenson, Microsoft]]
- Video: [[https://www.youtube.com/watch?v=Bh7sWtrkijY][Istio 0.2 automatic sidecar injection using Kubernetes Initializers]] - 6 Oct 2017
- [[https://www.youtube.com/watch?v=s4qasWn_mFc][Youtube: Istio and Kubernetes - Kelsey Hightower]] Jul 2017
- [[https://www.safaribooksonline.com/library/view/velocity-conference-2017/9781491976265/video311439.html]] (safari book)
- [[https://www.youtube.com/watch?v=IfPt3z6UAGw][JavaDay UA 2017: 8 Steps to Become Awesome with Kubernetes (Burr Sutter)]]
- [[https://www.youtube.com/watch?v=wq1nlJNvy1w][Setting Sail with Istio [B] - Lachlan Evenson, Microsoft]]
- [[https://medium.com/@mattklein123/service-mesh-data-plane-vs-control-plane-2774e720f7fc]]
- [[https://tech.bigbasket.com/bigbaskets-experience-with-istio/]] October 10, 2018
