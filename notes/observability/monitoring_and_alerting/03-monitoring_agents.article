Monitoring agents
|| Last update: 20 Feb 2019

* Intro

We will run two global services (scheduled on all our nodes):

- The Prometheus node exporter to get node metrics
- Google's cAdvisor to get container metrics

The Prometheus server to scrape these exporters and will be configured to use
DNS service discovery.

* cAdvisor

.image images/cAdvisor.png 100 _

- [[https://github.com/google/cadvisor]]

cAdvisor is an open source collector to gather container metrics.
It auto-discovers all containers and collects CPU, memory, filesystem, and
network from the node it is running on.

    cAdvisor (short for container Advisor) analyzes and exposes resource usage
    and performance data from running containers.
    Source: https://prometheus.io/docs/guides/cadvisor/

It exposes this data in its own UI or via Prometheus metrics. It has other
exporters.

** Storage Plugins (exporters)

- [[https://github.com/google/cadvisor/blob/master/docs/storage/README.md][Github Exporters]]

cAdvisor [[https://github.com/google/cadvisor/blob/master/docs/runtime_options.md][stores]]
the latest historical data in memory.
We could also [[https://github.com/google/cadvisor/blob/master/docs/storage/influxdb.md][save the data]]
in InfluxDB and pull that from grafana directly, but we would lose Prometheus
capabilities.
Can push to influxdb, elasticsearch (only v2) or prometheus.

- [[https://github.com/google/cadvisor/issues/1812][GA Elasticsearch and Kibana 6 is out but cadvisor does not work with it. ]]

NOTE: Don't use it with Elasticsearch since there is not support for new
versions.

Use [[https://github.com/google/cadvisor/blob/master/docs/storage/prometheus.md][Prometheus]].

- [[https://prometheus.io/docs/guides/cadvisor/]]

When you use prometheus you can see the metrics at _/metrics_:

    # If you are running it in localhost
    curl localhost:8080/metrics

** Usage

Run cAdvisor as a daemon on your cluster.

Beware the mountpoint. Without _/sys_ won't work and without it you can't get
the metrics of the system. So you need to mount the right / instead.

Instead of:

    -v /sys:/sys:ro

On a normal ubuntu system you should mount /sys while on ECS IAM optimised you
need to mount a different endpoint. Use:

    -v /cgroup:/sys/fs/cgroup:ro

Explained in this two tickets:

- [[https://github.com/google/cadvisor/issues/1515]] - Unable to start cadvisor when running on AWS Linux Optimized AMI, rootfs/sys/fs/cgroup/cpuset: no such file or directory
- [[https://github.com/google/cadvisor/issues/1843]] - Fails startup on RHEL: Failed to start container manager: inotify_add_watch /sys/fs/cgroup/cpuacct,cpu: no such file or directory

There are two sub-parent probably connected to this:

- [[https://github.com/google/cadvisor/issues/1900]] - Unable to find cpuset on AWS ECS

Some metrics start by /ecs dome by /docker.

*** Run via bash locally

Quick glance about the usage of containers in your system.

    docker run \
      --rm \
      --volume=/:/rootfs:ro \
      --volume=/var/run:/var/run:rw \
      --volume=/sys:/sys:ro \
      --volume=/var/lib/docker/:/var/lib/docker:ro \
      --publish=8080:8080 \
      --detach=true \
      --name=cadvisor \
      google/cadvisor:latest

cAdvisor exposes its raw and processed stats via a remote REST API.
This is what Prometheus uses to pull the data.

*** Run it via Puppet

    # You need to have the Puppet docker module
    docker::run { 'google-cadvisor':
      image            => 'google/cadvisor:<version>',
      ports            => ['8080'],
      expose           => ['8080'],
      net              => 'host',
      volumes          => [
        '/:/rootfs:ro',
        '/var/run:/var/run:rw',
        '/sys:/sys:ro',
        '/var/lib/docker/:/var/lib/docker:ro',
      ],
      restart_service  => true,
      privileged       => false,
      pull_on_start    => true,
      extra_parameters => [
        '--sig-proxy=false',
      ],
    }

** Memory usage

A not of interest is the memory usage.

.image images/memory.png _ 800

The legend shows "hot" and "total".

*hot* refers to the _working_set_
([[https://en.wikipedia.org/wiki/Resident_set_size][RSS]] + cache),
which is held in RAM and are pages that has been recently touched as calculated
by the kernel. No swap.
*Working*Set* is the bytes of memory that the kernel deems to be necessary for
continuing to run the processes in a container. Kernel cannot tolerate
overcommitment of the sum total of working set.

*total* includes _hot_ + _cold_ memory and refers to all the memory used by a
process. Cold are the pages that have not been touched in a while and the
kernel can reclaim safely under pressure.

    Total (`memory.usage_in_bytes`) = `rss` + `cache`
    Working set = Total - inactive (not recently accessed memory = `inactive_anon` + `inactive_file`)
    Source: https://github.com/google/cadvisor/issues/638#issuecomment-160151279

Total memory usage reported by Linux or cgroups is generally misleading because
it includes cache. [[http://www.linuxatemyram.com/]]

They delegate to libcontainer to retrieve stats.

- [[https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=34e431b0ae398fc54ea69ff85ec700722c9da773]]

NOTE: Don't run with swap enabled

RSS is somewhat lower than Usage, since Usage counts pages that have not been
accessed in a while. So if your program is using and freeing pages, those are
not reclaimed until the machine gets close to running out of memory, and starts
to reclaim inactive pages.

Maybe transparent huge pages?? ECS [[https://github.com/aws/amazon-ecs-agent/issues/280]]

    With ecs agent version 1.14.0, released this past January, we made a change to
    the memory utilization metrics reporting to make it more accurately reflect the
    amount of memory in use by your application. Specifically, we changed the metric
    to exclude memory used by the linux page cache, since this is memory that can be
    reclaimed transparently by the kernel.
    Source: https://github.com/aws/amazon-ecs-agent/issues/863#issuecomment-311715298

** Performance

cAdvisor may take a lot of resources in your system. To lower this usage there
are some adviced settings: --disable-metrics='disk,tcp,udp' and
'increasing housekeeping even higher to 10s'.

    "Entrypoint": [
        "/usr/bin/cadvisor",
        "--logtostderr",
        "--housekeeping_interval",
        "10s",
        "--disable_metrics",
        "disk,tcp,udp"
    ],

To quote:

    Try using `--ignore-metrics` to disable metrics you are not using.
    Particularly expensive metrics are disk, diskIO, UDP, and tcp metrics.
    Source: https://github.com/google/cadvisor/issues/2049#issuecomment-424038803

Other options are:

- --docker_only
-

* Phrometeus

.image images/prometheus.png 100 _

- [[https://prometheus.io/]]
- [[https://github.com/prometheus/prometheus]]

** Install and run

To [[https://prometheus.io/docs/introduction/install/][install]] it with docker:

    docker run \
    --rm \
    --publish=9090:9090 \
    --name=prometheus \
    --detach=true \
    --volume=/opt/data/prometheus/metrics:/prometheus \
    --volume=/opt/data/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml \
    prom/prometheus:latest

** Configurations

To check what flags you can pass to Prometheus check the [[https://github.com/prometheus/prometheus/blob/master/Dockerfile][Dockerfile]].

prometheus.yml holds the server configurations. For now I will use the [[https://github.com/prometheus/prometheus/blob/master/documentation/examples/prometheus.yml][default one]].

    docker exec -i -t prometheus /bin/bash

** Notes

The Prometheus image uses a volume to store the actual metrics. For production deployments it is highly recommended to use the [[https://docs.docker.com/engine/userguide/containers/dockervolumes/#creating-and-mounting-a-data-volume-container][Data Volume Container]] pattern to ease managing the data on Prometheus upgrades.

** Testing

To check the actual config:

    http://ip:9090/config

To add a new site to scrape you need to change the [[https://prometheus.io/docs/operating/configuration/][config file]].

  - job_name: 'cAdvisor'
    scrape_interval: 5s
    # metrics_path defaults to '/metrics'
    # scheme defaults to 'http'.
    static_configs:
      - targets: ['ip:8080']

And then you need to [[https://www.robustperception.io/reloading-prometheus-configuration/][reload]] the configurations:

    # From the server
    curl -X POST http://localhost:9090/-/reload

* Node exporter for OS/node metrics

Node-exporter is a Prometheus exporter that expose the metrics for the instance
in prom syntax. You can run it locally:

    docker run \
    --publish=9100:9100 \
    --name=nodeexporter \
    --detach=true \
    prom/node-exporter:latest

    # CHECK THIS
    docker service create --name node --mode global --network prom \
    --mount type=bind,source=/proc,target=/host/proc \
    --mount type=bind,source=/sys,target=/host/sys \
    --mount type=bind,source=/,target=/rootfs \
    prom/node-exporter \
    -collector.procfs /host/proc \
    -collector.sysfs /host/proc \
    -collector.filesystem.ignored-mount-points "^/(sys|proc|dev|host|etc)($|/)"

See the metrics at:

    # If you are running it in localhost
    curl localhost:9100/metrics

Add the job:

  - job_name: 'nodeexporter'
    scrape_interval: 5s
    # metrics_path defaults to '/metrics'
    # scheme defaults to 'http'.
    static_configs:
      - targets: ['x.x.x.x:9100']

Now add grafana dashboards:

- [[https://github.com/stefanprodan/dockprom/blob/master/docker-compose.yml]]

You can use these as a base:

- [[https://stefanprodan.com/2016/a-monitoring-solution-for-docker-hosts-containers-and-containerized-services/]]

* NOTES TO REVIEW

    Test?

- [[https://github.com/kbudde/rabbitmq_exporter]]

      docker network create --driver overlay prom

      docker service create --name node --mode global --network prom \
      --mount type=bind,source=/proc,target=/host/proc \
      --mount type=bind,source=/sys,target=/host/sys \
      --mount type=bind,source=/,target=/rootfs \
      prom/node-exporter \
      -collector.procfs /host/proc \
      -collector.sysfs /host/proc \
      -collector.filesystem.ignored-mount-points "^/(sys|proc|dev|host|etc)($|/)"

      docker service create --name cadvisor --network prom --mode global \
      --mount type=bind,source=/,target=/rootfs \
      --mount type=bind,source=/var/run,target=/var/run \
      --mount type=bind,source=/sys,target=/sys \
      --mount type=bind,source=/var/lib/docker,target=/var/lib/docker \
      google/cadvisor:latest

      - job_name: 'node'
      dns_sd_configs:
      - names: ['tasks.node']
      type: 'A'
      port: 9100
      - job_name: 'cadvisor'
      dns_sd_configs:
      - names: ['tasks.cadvisor']
      type: 'A'
      port: 8080
