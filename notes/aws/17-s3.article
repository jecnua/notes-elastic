S3
|| Last update: 19 Sep 2019

* Intro

Amazon S3 will no longer support path-style API requests starting September 30th, 2020

- [[https://forums.aws.amazon.com/ann.jspa?annID=6776]]

So use:

    example: //<bucketname>.s3.amazonaws.com/key

NOT:

    example: //s3.amazonaws.com/<bucketname>/key

* Storage classes

- [[https://aws.amazon.com/s3/storage-classes/]]

- Standard - Designed for frequently accessed data.
- Standard-IA (min duration 30d) - Designed for long-lived, infrequently accessed data.
- One Zone-IA (min duration 30d) - Designed for long-lived, infrequently accessed, non-critical data.
- Intelligent-Tiering (min duration 30d)
- Glacier (min duration 90d) - Designed for long-lived, infrequent accessed, archived critical data.
- Glacier Deep Archive (min duration 180d) - Long-term retention of data that is accessed rarely in a year (maintaining data in on-premises magnetic tape libraries or archiving data offsite)
- Reduced Redundancy (deprecated/not recommended): Use One Zone-IA instead

** Data protection

S3 allow you to lock your bucket content to all public access. Bast way to do it
in CloudFormation is:

    Bucket:
      Type: AWS::S3::Bucket
      Properties:
        BucketName: MyBucket
        BucketEncryption:
          ServerSideEncryptionConfiguration:
            -
              ServerSideEncryptionByDefault:
                SSEAlgorithm: AES256
        LifecycleConfiguration:
          Rules:
            -
              ExpirationInDays: 30
              Id: ExpirationInDaysRule
              NoncurrentVersionExpirationInDays: 30
              Status: Enabled
        PublicAccessBlockConfiguration:
          BlockPublicAcls: true
          BlockPublicPolicy: true
          IgnorePublicAcls: true
          RestrictPublicBuckets: true

** Glacier

- Retrieval options in minutes (using expedited retrieval)

** Glacier Deep Archive

- [[https://aws.amazon.com/blogs/aws/new-amazon-s3-storage-class-glacier-deep-archive/]]

- Data is stored across 3 or more AWS Availability Zones
- Retrieved within 12 hours
- Bulk retrieval option, where you can retrieve petabytes of data within 48 hours

    Choose S3 Glacier Deep Archive when you want the lowest storage cost for
    archived data that is accessed at most once or twice a year, and you are
    willing to accept 12 or 48 hour latencies to first byte.
    [...]
    Glacier Deep Archive is integrated with Tape Gateway, a cloud-based virtual
    tape library feature of AWS Storage Gateway.
    Source: https://aws.amazon.com/about-aws/whats-new/2019/03/S3-glacier-deep-archive/

** Intelligent-Tiering

- [[https://aws.amazon.com/blogs/aws/new-automatic-cost-optimization-for-amazon-s3-via-intelligent-tiering/]]

    This storage class incorporates two access tiers: frequent access and
    infrequent access. Both access tiers offer the same low latency as the
    Standard storage class.
    [...]
    moves objects that have not been accessed for 30 consecutive days to the
    infrequent access tier. If the data is accessed later, it is automatically
    moved back to the frequent access tier.

* Replication

- S3 Same-Region Replication (SRR)
- S3 Cross-Region Replication (CRR): replicates data across different AWS Regions

    With SRR, new objects uploaded to an Amazon S3 bucket are configured for
    replication at the bucket, prefix, or object tag levels. Replicated objects
    can be owned by the same AWS account as the original copy or by different
    accounts, to protect from accidental deletion.
    [...]
    SRR makes another copy of S3 objects within the same AWS Region, with the
    same redundancy as the destination storage class.
    Source: https://aws.amazon.com/about-aws/whats-new/2019/09/amazon-s3-introduces-same-region-replication/

* s3 Access Points

- [[https://aws.amazon.com/blogs/aws/easily-manage-shared-data-sets-with-amazon-s3-access-points/]]

You can have multiple access point for bucket.

    3 Access Points are unique hostnames that you can create to enforce distinct
    permissions and network controls for any request made through the access point.
    S3 Access Points policies allow enforcing permissions by prefixes and object
    tags, allowing limits on the object data that can be accessed. Any S3 Access
    Points can be restricted to a Virtual Private Cloud (VPC) to firewall S3 data
    access within your private networks, and AWS Service Control Policies can be
    used to ensure all access points in an organization are VPC restricted.
    Source: https://aws.amazon.com/about-aws/whats-new/2019/12/amazon-s3-access-points-manage-data-access-at-scale-shared-data-sets/
