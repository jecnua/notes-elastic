Networking for Containers
|| Last update: 28 Dec 2016

* The technology

When you run a container on a server there are two ways to run it:

- In the _host_network_ namespace
- In it's own network

A network packed is formed of many layers. Here are some:

- Mac address (layer 2)
- Source and dest IP (layer 3)
- Port and other tcp/ip info (layer 4)
- The content of the http package (layer 7)

** Vxlan

Vxlan is an udp based [[https://tools.ietf.org/html/rfc7348][standard protocol]].

Open switch datapath module (ODP) allows an applucation to tell the kernel how
to directly route the package to an application (or container). This way is the
kernel that takes care of adding the vxlan headers.

vxlan tunnels to create layer 2 overlat networks. Existing routers
see the vxlan traffic as regula ip/udp packets.
Each end of the vxlan tunnel is terminated by a vxlan tunnel endpoint (vtep).
The vtep manages encapsulation and decapsultaion of packages.

A network namespace run as an isolated network stack.
ODP and vxlan are part of the linux kernel since version 3.12.

** Overlay network

An *overlay*network* obscures the real network architecture using
traffic encapsulation. It basically wraps all cluster packages at node level
with additional tcp/udp headers). They need an address range to work with.
This allows multiple IPs on a single machine.

One way to do it is to create a virtual network interface to do the
encapsulation automatically for you.

** Alternatives

Calico non-IPIP and flannel both have an non overlay option. This is fast and
lightweight however they both span only 1 AZ!
VPC Route Table have only 50 entries.

* Docker networking

Docker uses a *host*private* networking approach. It creates a _virtual_bridge_
(docker0) and allocates a subnet to it. For each container you spawn, it
allocates a _virtual_ethernet_device_ (veth) attached to this bridge.

The _veth_ appears as *eth0* in the container. This interface is given an ip
from the subnet range. This however makes the container visible only on the
same machine.

** Swarm

.image images/swarm_vxlan.png 300 _
.caption Source: [[http://blog.nigelpoulton.com/demystifying-docker-overlay-networking/][Demystifying Docker overlay networking]]

Swarm uses vxlan tunnels. A virtual switch (also
called virtual switch) called br0 is created inside the network namespace.
A vtep is then created and connected to the switch and the host network.

Each container gets is own virtual ethernet (veth) adapter connected to br0
virtual switch.

* K8S networking

- Kubernetes networking [[http://kubernetes.io/docs/admin/networking/][docs]]
- [[https://github.com/kubernetes/kubernetes/blob/master/docs/design/networking.md][Design choices]]

K8s wanted to take a different approach to allow a less painful migration from
applications in VMs to containers. In k8s, as per design choices, containers in
a pod shares the same _network_namespace_ (including ip address) and as such
can communicate via _locahost_.

** The IP-per-POD model

In kubernetes every *POD* has an unique IP (IPv4) address and containers
running inside the POD gets assigned ports on this ip. This avoids the need to
create _links_ (now deprecated in docker) and no need to map container ports
to host ports (unless using LoadBalancer on AWS for _limitations_).

k8s reccomends to allocated blocks of IPs to each nodes in the cluster depending
on the workload expected.

The POD is unaware if it's being bounder to an host port or not.

*** Low level implementation in docker

This is implemented in Docker via a _pod_container_ which holds the network
namespace open while _app_containers_ join the container and so the namespace.

** Implementations

K8s networking requirements are few:

- Pods are routable on a network
- Pods should see their own routable IP address
- Nodes can communicate with all containers

and what do you choose to implement them is up to you.

*** Software defined network (SDN)

Some of the options are for (but not only) overlay network:

- Project [[https://www.projectcalico.org/][Calico]] (IPIP mode) ip-in-ip/host-gw
- CoreOS [[https://coreos.com/flannel/docs/latest/][Flannel]] vxlan/host-gw
- Weaveworks [[https://www.weave.works/][Weave-net]]

Usually build on the following _underlay_network_:

- MACvlan
- IPvlan

Or you can do it manually: [[https://github.com/kelseyhightower/kubernetes-the-hard-way/blob/master/docs/07-network.md][kubernetes-the-hard-way]]

*** Network plugins (CNI)

    # From the docs:
    Disclaimer: Network plugins are in alpha. Its contents will change rapidly.

- [[http://kubernetes.io/docs/admin/network-plugins/#cni][docs]]
- [[https://github.com/containernetworking/cni][spec]]

K8S assumes each POD can be given an IP and it relies on plugins to do so.
CNI plugins needs to adhere to the appc/CNI specification to be interchangeable.

They use a k/v store like ETCD to store mappings between the virtual IP and the
real node IP. A deamon on every node will manage the forwarding of the traffic
to the right POD.

*** Weaveworks

.image images/weave-net-fdp.png 300 _
.caption Source: [[https://www.weave.works/weave-docker-networking-performance-fast-data-path/][Weave Networking Performance with the New Fast Data Path]]

- https://www.weave.works/docs/net/latest/kube-addon/

In weave each packet is encapsulated in a tunnel (wrapped in new headers).
The weave router is the component that add/remove them. But beware router is
in userspace.

When you work with this type of technology (encapsulate/decapsulate) in userspace
the package runs twice through the kernel. Copies of the packages and context
switching costs up to 3x the latency you would have going thought a host
network.

Waeve routers use port _6784_UDP_.
By default, waeve MTU is set to 1410 (pretty conservative).
On amazon EC2 you can securely push it to *8950*.

* Sources

- [[https://medium.com/@anne_e_currie/kubernetes-aws-networking-for-dummies-like-me-b6dedeeb95f3][Kubernetes/AWS Networking for Dummies (like me)]] (23 Dec 2016)
- [[https://jvns.ca/blog/2016/12/22/container-networking/][Julia Evans: container networking]] (22 Dec 2016)
- [[http://blog.nigelpoulton.com/demystifying-docker-overlay-networking/][Demystifying Docker overlay networking]] (12 Oct 2016)
- [[https://www.weave.works/weave-docker-networking-performance-fast-data-path/][Weave Networking Performance with the New Fast Data Path]] (13 Nov 2015)
- [[https://github.com/kelseyhightower/kubernetes-the-hard-way/blob/master/docs/07-network.md][kubernetes-the-hard-way]]
