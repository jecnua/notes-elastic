Lambda
|| Last update: 12 Mar 2018

* Intro



* Logging

    Whenever you write to the stdout from your Lambda function, it ends up in the function’s Log Group in CloudWatch Logs.
    To ship your logs from CloudWatch Logs to ELK you can subscribe the Log Group to a cloudwatch-logs-to-elk function that is responsible for shipping the logs.
    it’s better to setup a rule in CloudWatch Events to invoke a subscribe-log-group Lambda function to set up the subscription for new Log Groups.
    2 things to keep in mind:
    lots services create logs in CloudWatch Logs, so you’d want to filter Log Groups by name, Lambda function logs have the prefix /aws/lambda/
    don’t subscribe the Log Group for the cloudwatch-logs-to-elk function (or whatever you decide to call it), otherwise you create an infinite loop for the cloudwatch-logs-to-elk function where its own logs will trigger itself and produce more logs and so on
    - https://theburningmonk.com/2017/03/yubls-road-to-serverless-architecture-part-3/


    Another approach is to take a leaf from Datadog’s book and use special log messages and process them after the fact. For instance, if you write logs in the format MONITORING|epoch_timestamp|metric_value|metric_type|metric_name like below..

    console.log(“MONITORING|1489795335|27.4|latency|user-api-latency”);

    console.log(“MONITORING|1489795335|8|count|yubls-served”);

    then you can process these log messages (see Logging section above) and publish them as metrics instead. With this approach you’ll be trading off liveness of metrics for less API latency overhead.
    - https://theburningmonk.com/2017/03/yubls-road-to-serverless-architecture-part-3/

* Metrics

- https://www.datadoghq.com/blog/monitoring-lambda-functions-datadog/

Of course, you can employ both approaches in your architecture and use the appropriate one for each situation:

for functions on the critical path (that will directly impact the latency your users experience), choose the approach of publishing metrics as special log messages;
for other functions (cron jobs, kinesis processors, etc.) where invocation duration doesn’t significantly impact a user’s experience, publish metrics as part of the invocation

* Tracing

- https://github.com/serverless/serverless/issues/3495

Additionally, you might wish to pass along other useful contextual information (such as payment ID, tournament ID, etc.) about the request to downstream services. Otherwise they would only appear in log messages from services where they originated from.

- x-ray
- tracing for telling startup time

turns it on by default
and you can wrap certain calls

* External advices

** CPU is throttled via Cgroups

You can verify that AWS Lambda uses Cgroups for CPU throttling by reading the contents of the special file /proc/self/cgroup and see your function is assigned to specific control groups such as "2:cpu:/sandbox-root-UXXIp3/sandbox-023d01", but you cannot directly see the imposed limits.

- [[https://www.kernel.org/doc/Documentation/cgroup-v1/cgroups.txt][cgroup-v1]]

** Memory

We allocate more memory than we need for our functions. Spikes can cause out of memory exceptions. We prefer to allocate more memory instead of experiencing these errors in production. Second reason is, although you only specify the RAM, AWS allocates proportional CPU to your functions.

** Startup costs

Python has not startup cost but JVM has a startup costs.

** Dependencies

You can't use python wheel and egg to package python program. You need to use their own.

** Very noisy neighbours

Lambda share the same infra with a LOT of other functions.

** Failures

In lambda you don't know it failed you need to know when you fail - wait timeout.

* Sources

- https://theburningmonk.com/2017/03/yubls-road-to-serverless-architecture-part-3/
