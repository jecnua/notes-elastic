Jaeger
|| Last update: 27 Feb 2019

* Intro

.image images/Jaeger_Logo_Final_BLACK.png 400 _

My related code project [[https://github.com/jecnua/tracing-nodejs]].

* Jaeger

- [[http://www.jaegertracing.io/]]
- [[https://github.com/uber/jaeger]]
- [[https://github.com/cncf/toc/blob/master/proposals/jaeger.adoc][CNCF proposal]]

The backend is written in GO and the UI is in react.
Being pretty new, it's build to be scalable and it's cloud friendy. The client
emits the traces to the _agent_ which listen to spans (inbound) and route them
to the collector.

*Sponsorship*: Uber has 7 full time maintainers and Red Hat has 4 full time
maintainers

* Architecture

- [[https://www.jaegertracing.io/docs/architecture/]]

.image images/jaeger_arch.png 500 _
.caption Souce: [[https://www.jaegertracing.io/docs/architecture/]]

Jaeger was an uber project and has been build with scalability and parallelism in
mind.

** Workflow

.image images/jaeger_work.png 700 _
.caption Souce: [[https://www.jaegertracing.io/docs/architecture/]]


    The client emits traces to the agent which listens for inbound spans and routes
    them to the collector. The responsibility of the collector is to validate,
    transform and store the spans to the persistent storage. To access tracing data
    from the storage, the query service exposes a REST API endpoints and the React
    based UI.
    Source: https://sematext.com/blog/opentracing-jaeger-as-distributed-tracer/

** Agents

    $ [...] --collector.zipkin.http-port

The agents receive spans over UDP on port 5775. The spans are batched, encoded
as [[https://thrift.apache.org/][Thrift structures]] and submitted to the
collector.

*NOTE*: The app is using Thrift because at Uber was what they are used to.
However, it's in the planning a move to _gRPC_.
Source: https://groups.google.com/forum/#!topic/jaeger-tracing/toBNkL_zEDE

The Routing and Discovery phase of the collectors from the client library is also
the responsibility of the agent.

Jaeger can accept Zipkin span transparently :D

Performance can be tuned:

- [[https://medium.com/jaegertracing/tuning-jaegers-performance-7a60864cf3b1]]

** Span

Fields are:

- traceID
- spanID
- parentSpanID
- flags
- operationName
- references
- startTime (unix epoch)
- duration (millis)
- tags
- logs
- processID
- process
- serviceName
- warnings

** Storage

    $ jaeger-collector --span-storage.type elasticsearch [...]

It supports for _span_stores_:

- In-memory
- *Cassandra*
- *ElasticSearch*
- ScyllaDB (in progress)

In Elasticsearch two indices will be created: one for storing the services and
the other one for the spans of given services.

* Query API

Expose a rest interface.

* Sampling

- [[https://www.jaegertracing.io/docs/1.10/sampling/]]

    Jaeger libraries implement consistent upfront (or head-based) sampling.
    [...]
    When service A receives a request that contains no tracing information,
    Jaeger tracer will start a new trace, assign it a random trace ID, and make
    a sampling decision based on the currently installed sampling strategy. The
    sampling decision will be propagated with the requests to B and to C, so
    those services will not be making the sampling decision again but instead
    will respect the decision made by the top service A. This approach
    guarantees that if a trace is sampled, all its spans will be recorded in the
    backend. If each service was making its own sampling decision we would
    rarely get complete traces in the backend.
    Source: https://www.jaegertracing.io/docs/1.10/sampling/

The agent *POLL* for a _sampling_strategy_ from the tracing backend and propagate
the sampling rate to all tracer clients. This is useful in dynamic environments.

For now Jaeger *does*not* support tail-based sampling.

- [[https://github.com/jaegertracing/jaeger/issues/425][GITHUB ISSEUS: Discuss post-trace (tail-based) sampling]]

They however allow:

- constant sampling
- probabilist, head based sampling
- rate limiting
- remote

Remote is probably the most interesting one:

    [remote] This allows controlling the sampling strategies in the services
    from a central configuration in Jaeger backend, or even dynamically (see
    Adaptive Sampling).
    Source: https://www.jaegertracing.io/docs/1.10/sampling/#client-sampling-configuration

Another idea on a possible approach:

- [[https://groups.google.com/forum/?#!msg/distributed-tracing/fybf1cW04ZU/KhcF5NxTBwAJ][GOOGLE GROUP: Keeping "most interesting" traces]]

Upcoming Adaptive Sampling functionality:

- [[https://github.com/jaegertracing/jaeger/issues/365][GITHUB ISSUE: Adaptive Sampling ]]

* Test it

- [[https://github.com/jaegertracing/jaeger/tree/master/examples/hotrod]]

    docker run -d --name jaeger --rm -p 6831:6831/udp -p 16686:16686 jaegertracing/all-in-one:latest
    docker run --rm -it --link jaeger -p8080-8083:8080-8083 jaegertracing/example-hotrod:latest --jaeger-agent.host-port=jaeger:6831

If you want to sping the component separately:

- [[https://www.jaegertracing.io/docs/deployment/]]

** On k8s

- [[https://github.com/jaegertracing/jaeger-kubernetes]]

* Infrastructure

** Span context

The span context *is*NOT*defined* in the opentracing javascript

- [[https://github.com/opentracing/opentracing-javascript/blob/master/src/span_context.ts]]

* Monitoring

- [[https://www.jaegertracing.io/docs/monitoring/]]

Jeager export metrics via prometheus endpoint.

At Uber, they monitor:

- Node signals: CPU/memory usage, file descriptors, network usage, etc
- Go runtime signals: goroutine count, GC, etc
- Jaeger internals: dropped span count, mem queue size, storage write counts/latencies

At Uber they don't use auto-scaling for collectors, just a fixed size pool.
The bottleneck is typically the storage, which is hard to auto-scale.

* Jeager operator

Almost production ready:

- [[https://github.com/jaegertracing/jaeger-operator]]

* PRO and CON

Jaeger is newer in respect of Zipkin, but allow dynamic sampling and has a lower
footprint.

* Jaeger backend on ECS

** UDP load balancing in AWS (not)

Sorry for the link but I didn't find it on the new repo

- https://github.com/jpkrohling/jaeger/blob/5567f02b285329ea40caecdaa6a723eb626a24f1/docs/deployment.md

    The agents can connect point to point to a single collector address, which
    could be load balanced by another infrastructure component (e.g. DNS) across
    multiple collectors. The agent can also be configured with a static list of
    collector addresses.
    On Docker, a command like the following can be used:
    docker run \
      --rm \
      -p5775:5775/udp \
      -p6831:6831/udp \
      -p6832:6832/udp \
      -p5778:5778/tcp \
      jaegertracing/jaeger-agent \
      /go/bin/agent-linux --collector.host-port=jaeger-collector.jaeger-infra.svc:14267

- [[https://www.reddit.com/r/aws/comments/8tirpw/udp_load_balancing_in_aws/]]
- [[https://www.reddit.com/r/aws/comments/82pvuf/udp_load_balancing_in_aws/]]

The only solution I can find it to use dns.
Since I can use dns, I will invest in that direction, I don't want to work with
lambda and floating ip.

- [[https://docs.aws.amazon.com/AmazonECS/latest/developerguide/create-service-discovery.html]]

NLB doesn not support UDP

There is no support for discovery systems:

- [[https://github.com/jaegertracing/jaeger/issues/213]]

** Elasticsearch and disk limit

- https://github.com/jaegertracing/jaeger/blob/master/plugin/storage/es/README.md

I’ve been looking at similar ways of splits to allow you to manipulate DNS
outside of other CF templates

    * aws_elasticsearch_domain.es: LimitExceededException: Volume size must be
    between 10 and 35 for t2.small.elasticsearch instance type and
    elasticsearch version 6.3
    status code: 409, request id: x-x-x-x-x

** Don't use host port

also this:

    service jaeger-agent was unable to place a task because no container
    instance met all of its requirements. The closest matching
    container-instance x-x-x-x-x is already using a
    port required by your task. For more information, see the Troubleshooting
    section.

if you are exposing a port the old task is locked in an INACTIVE status and you
can't release

Unless you put 0% availability during releases

** Agent

for lack of UDP routing capabilities I can't run an agent behind a lb in aws
this means I will run the agent as deamon on each node and use a normal lb over
it.

however still you have no way to route the traffic there at all

** No load balancers

so you have to ignore load balancers if you are using udp and agent

** Collector

I decided to run it as daemon to get performance (is tcp) and avoid using
another ENI. this will allow to use lower sizes as boxes

** Agent connection to collector

Agent needs to run on docker ip

    "command": [
        "--collector.host-port=172.17.0.1:14267"
    ],

- [[https://stackoverflow.com/questions/51184501/aws-ecs-containers-are-not-connecting-but-works-perfectly-in-my-local-machine]]

* Re/Sources

- VIDEO [[https://www.youtube.com/watch?v=tFZAHWl8y_I][Jaeger Project Deep Dive - Juraci Kröhling, Red Hat]] - 4 May 2018
- [[https://sematext.com/blog/opentracing-jaeger-as-distributed-tracer/]] - 8 May 2018
