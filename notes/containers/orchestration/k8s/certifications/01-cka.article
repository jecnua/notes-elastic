CKA notes
|| Last update: 25 Oct 2018

* To clarify

- k8s ports
- cgroup drivers
- no need for cni package
- https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/
- CRI https://github.com/kubernetes/community/blob/master/contributors/devel/container-runtime-interface.md
- cri-o
- vx-lan

* Kubernetes course: CKA certified kubernetes administrator

* Intro

.image images/k8s_high_level.png _ 800
.caption Source: [[https://www.safaribooksonline.com/videos/oscon-2017/][Kubernetes hands-on - Kelsey Hightower (Google) - Part 1]]

* The physical cluster

.image images/arch.png _ 800
.caption Source: [[https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS158x+1T2018]]

At high level there are masters and nodes.

        For fault tolerance purposes, there can be more than one master node in
        the cluster. If we have more than one master node, they would be in a HA
        (High Availability) mode, and only one of them will be the leader,
        performing all the operations. The rest of the master nodes would be
        followers.
        Source: https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS158x+1T2018

All master nodes connect to etcd.

** Nodes memory

Kubernetes controller and node servers are expected to have *swap*disabled*.
This is the recommended deployment. If swap is not disabled, kubelet service
will not start on the masters and nodes.

Kubernetes requires the swap to be off! To disable it on a node you can do:

    # As root
    swapoff -a

and then remove it from the _/etc/fstab_ file.

You could disable it by passing --fail-swap-on=false but is not adviced.

    The kubelet now fails if swap is enabled on a node. To override the default
    and run with /proc/swaps on, set --fail-swap-on=false. The experimental
    flag --experimental-fail-swap-on is deprecated in this release, and will be
    removed in a future release.
    Source: https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.8.md#before-upgrading

- [[https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/]]
- [[https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.8.md#before-upgrading]]
- [[https://kubernetes.io/docs/setup/independent/]]
- [[https://github.com/kubernetes/kubernetes/issues/53533]]

** Nodes components

Nodes will require three services:

- a container runtime
- the kubelet
- kube-proxy
- (beta) cloud controller manager

*** Node health

*kube-controller-manager* assign CIDR blocks to a new node, keep track of the
nodes health and check with the cloud provider is the node is still there.

After 40s it will go in condition unknown if the healthbeat fails. After 5m it
will evict pods. It will check the status every few seconds. There is a flag
called:

    --node-monitor-period duration     Default: 5s
    The period for syncing NodeStatus in NodeController.
    Source: https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/

It will also check what percentage of nodes in that zone is unhealty.
There is an unhealthy zone threshold and there are rules for each AZ.

    --unhealthy-zone-threshold float32     Default: 0.55
      Fraction of Nodes in a zone which needs to be not Ready (minimum 3)
      for zone to be treated as unhealthy.

The eviction rate is parametric. 0.1 per second means that no more than one pod
per node every 10s.

    --node-eviction-rate float32     Default: 0.1
      Number of nodes per second on which pods are deleted in case of node
      failure when a zone is healthy (see --unhealthy-zone-threshold for
      definition of healthy/unhealthy). Zone refers to entire cluster in
      non-multizone clusters.

The eviction rate is *reduced* to the secondary node.

    --secondary-node-eviction-rate float32     Default: 0.01
      Number of nodes per second on which pods are deleted in case of node failure
      when a zone is unhealthy (see --unhealthy-zone-threshold for definition of
      healthy/unhealthy). Zone refers to entire cluster in non-multizone clusters.
      This value is implicitly overridden to 0 if the cluster size is smaller
      than --large-cluster-size-threshold.

If you want to go multi-az you can shift the workload to a new zone if the first
one gets unhealthy. If the system sees ALL nodes as unhealthy it will
_stop_all_evictions.

From 1.8+ the node controller can *taint* node to respect their status.

- [[https://kubernetes.io/docs/concepts/architecture/nodes/]]

*** (BETA) Cloud controller manager

NEEDS MORE RESOURCES

- [[https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/]]

Deamon that manages cloud related resources in a control loop.
It can implement:

- Node controller: Updated the nodes status by coordinating with the cloud API.
- Service controller: Listen to service create, udpate and delete and responsible for LoadBalancer types
- Route controller: Responsible for setting up network routes (only on gce)
- Persistent volume labels controller: Sets zone and region labels on PersistenVolumes

To run this beta functionality you will however need to:

- NOT specify the --cloud-provider flag in kube-apiserver and kube-controller-manaher
- Run kubelet with the flag --cloud-provider=external
- The kube-apiserver should not run the PersistentVolumeLabel admission controller
- Set the correct InitializerConfiguration to label persistent volumes

** Network

Kubernetes made some interesting choices on how networking should be implemented.
It defines the network model but leaves it to third party plugins a way to
implement it.

Each pod can communicate to each other pod on any host of the cluster, without
a NAT.

** CNI

- [[https://github.com/containernetworking/cni]]

    CNI (Container Network Interface), a Cloud Native Computing Foundation
    project, consists of a specification and libraries for writing plugins
    to configure network interfaces in Linux containers, along with a number
    of supported plugins. CNI concerns itself only with network connectivity of
    containers and removing allocated resources when the container is deleted.
    Because of this focus, CNI has a wide range of support and the specification
    is simple to implement.
    Source: https://github.com/containernetworking/cni

A CNI plugin is an *executable* invoked by the container management system. It's
responsible for:

- Inserting the network interface (one end of a vETH pair) in the container network namespace
- Making changes to the host: like attaching the other end of the vETH pair to the bridge network
- Assign the IP, put up the routes and apply IPAM rules depending on your topology

You can choose the right plugin for your workload.

** CNI plugins

- [[https://github.com/projectcalico/calico-cni]]
- [[https://github.com/weaveworks/weave]]

*** Flannel

When you configure flannel you will need to pass the CIDR block your k8s cluster
will need to use and assign it to the nodes.

You can define this CIDR with:

    --pod-network-cidr=10.244.0.0/16

Flannel makes use of an overlay network, usually Virtual Extensible LAN (VXLAN).

- [[https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/]]

* Kubernetes nuts and bolts

** Communication

When you use kubectl, the command converts the information from yaml to json
when making the API request.

** Objects

When you work with k8s you use the api objects to describe the desired
cluster's state.
You can interact with the api via kubectl or the api directly from
inside/outside k8s.

Once an object is set, the control plane works to make sure the actual state
will match the desired state.

objects are abstractions that represents the state of your system.

- [[https://kubernetes.io/docs/concepts/abstractions/overview/]]

There are some _basic_ k8s objects like:

- Pod [[https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/]]
- Service [[https://kubernetes.io/docs/concepts/services-networking/service/]]
- Volume [[https://kubernetes.io/docs/concepts/storage/volumes/]]
- Namespace [[https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/]]

And _high-level_abstraction_ called *controllers*.
Controllers are build upon the basic objects and provide additional
functionalities and convenient features. Among them:

- ReplicaSet [[https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/]]
- Deployment [[https://kubernetes.io/docs/concepts/workloads/controllers/deployment/]]
- StatefulSet [[https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/]]
- DaemonSet [[https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/]]
- Job [[https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/]]

Kubernetes objects are _persistent_entities_ in the k8s system. They represent
the state of the cluster. Any of them have at least the two following
nested object fields:

- object *spec*: This is your configuration defining the *desired* state. We provide it to k8s.
- object *status*: This section is added by kubernetes. It is supplied and updated by the control plane and represent the *actual* state.

k8s reacts to the _difference_ between state and status.

* Control plane

The kuberneyes control plane consists of a collection of processes running on
the cluster.

** Kubernetes controller (master node)

The kubernetes controller is a collection of 3 processes designed to run on a
single node in your cluster. They are:

- kube-apiserver: [[https://kubernetes.io/docs/admin/kube-apiserver/]]
- kube-controller-manager: [[https://kubernetes.io/docs/admin/kube-controller-manager/]]
- kube-scheduler: [[https://kubernetes.io/docs/admin/kube-scheduler/]]

Administrative tasks are made by communicating with the api-server. Commands are
send in *JSON* via *REST* and the api validates and process the requests. The result
is saved on etcd.

The scheduler schedule work to different nodes. It takes this decisions using a
whole lot of metrics it gets from the nodes plus the user requirements.
It schedules the work in terms of Pods and Services.

The controller-manager mages different non-terminating control loops. Each of this
manage a specific object and watch their current state though the API server.
If the state differs from the desired one it will take actions to change it.

The control plane maintains a record of all k8s objects and run a continous
control loop to manage the object state.

It will react to changes in the system and make sure the desired state match the
expected one.

** Kubernetes worker node

.image images/worker.png  _ 800
.caption Source: [[https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS158x+1T2018]]

Each individual _non-master_ node runs two processes:

- kubelet [[https://kubernetes.io/docs/admin/kubelet/]] : Communicates with the controller node
- kube-proxy [[https://kubernetes.io/docs/admin/kube-proxy/]] : A network proxy which reflects the kubernetes networking services on each node 

It will also need a container runtime like:

- [[http://cri-o.io/][cri-o]].
- [[https://containerd.io/][containerd]] (more notes on another page)
- [[https://coreos.com/rkt/][rkt]]
- [[https://linuxcontainers.org/lxd/][lxd]]

** Metadata

The _metadata_ is what uniquely identify a resource. All objects have unique:

- name: client, max 253 char, alphanumeric lower case, you can use - and .
- UID: generated by the control plane, spatially and temporary unique

- [[https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/]]

You can use either labels or annotation to attach metadata to k8s objects.

- *annotations* are not used to indentify or select objects
- *labels* can be used to select objects and to find collections of object that satisfy certain condition

They are both key value maps.

*** Annotations

- [[https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/]]

Annotations are used to attach *non-identifying*metadata* to object.
Tools can retrieve this metadata to do anything they need/want.

The metadata in an annotation can be small or large, structured or unstructured,
and can include characters not permitted by labels.

*** Labels and selectors

Labels are key -value pairs attached to objects.
They are used to specify identifying attributed that can be used to select a
workload.

* kubelet

The kubelet is an agent and runs as a deamon on every node (master and nodes).
It will communicate with the master node and runs containers as stated on the
saved state. It will also make sure all containers are healthy at all times.

The kubelet connects to the containers via the Container Runtime Interface (CRI)

** Container Runtime Interface (CRI)

- [[https://github.com/kubernetes/community/blob/master/contributors/devel/container-runtime-interface.md]]

.image images/cri.png  _ 800
.caption Source: [[https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/]]

Kubernetes 1.5 introduced an internal plugin API named Container Runtime
Interface (CRI) to provide easy access to different container runtimes. CRI
enables Kubernetes to use a variety of container runtimes without the need to
recompile. In theory, Kubernetes could use any container runtime that implements
CRI to manage pods, containers and container images.

    Abstracting the runtime interface
    One of the innovations taking advantage of this standardization
    is in the area of Kubernetes orchestration. As a big supporter
    of the Kubernetes effort, CoreOS submitted a bunch
    of patches to Kubernetes to add support for communicating
    and running containers via rkt in addition to the upstream
    docker engine. Google and upstream Kubernetes saw that
    adding these patches and possibly adding new container
    runtime interfaces in the future was going to complicate the
    Kubernetes code too much. The upstream Kubernetes team
    decided to implement an API protocol specification called the
    Container Runtime Interface (CRI). Then they would rework
    Kubernetes to call into CRI rather than to the Docker engine,
    so anyone who wants to build a container runtime interface
    could just implement the server side of the CRI and they
    could support Kubernetes. Upstream Kubernetes created a
    large test suite for CRI developers to test against to prove
    they could service Kubernetes. There is an ongoing effort to
    remove all of Docker-engine calls from Kubernetes and put
    them behind a shim called the docker-shim.
    Source: open-source-yearbook-2017

    kubelet (grpc client) connects to the CRI shim (grpc server) to perform
    container and image operations.
    Source: https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS158x+1T2018/

CRI implements two services:

- ImageService: responsible for image related operations
- RuntimeService: responsible for pod and container-related operations

Some of this implementations are:

- OBSOLETE - [[https://github.com/kubernetes/kubernetes/tree/master/pkg/kubelet/dockershim][dockershim]]: it interfaces with docker and docker will use containerd in the backend
- OBSOLETE - [[https://github.com/containerd/cri][cri-containerd]]: k8s interfaces directly with containerd (obsolete)
- contained 1.2
- cri-o

For more information about containerd look at the note page about it.

*** cri-containerd (obsolete)

- [[https://github.com/containerd/cri]]

.image images/cri-containerd.png _ 800
.caption Source: [[https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS158x+1T2018/]]

Since by default containerd (< 1.2) do not support CRI, engineers worked on
creating a CRI interface for containerd.
This project allowed a kubernetes cluster to use containerd as an underlying
runtime without using docker.

.image images/cri-containerd-deep.png _ 800
.caption Source: [[https://kubernetes.io/blog/2017/11/containerd-container-runtime-options-kubernetes/]]

cri-containerd is an implementation f CRI for containerd. It operates on the same
node as the kubelet and the container runtime. This process handles all the
requests from the kubelet and uses contained to manage the container lifecycle
in the backend.

Compared with the old docker CRI implementation it eliminates an extra hop in the
stack.

*** ContainerD plugin

- [[https://kubernetes.io/blog/2018/05/24/kubernetes-containerd-integration-goes-ga/]]

While cri-containerd as a step forward from dockershim, now that containerd
implements the CNI interface it's even better.

*** CRI-O

- [[http://cri-o.io/]]
- [[https://github.com/kubernetes-sigs/cri-o]]

.image images/cri-o-arch.png _ 800
.caption Source: [[http://cri-o.io/]]

CRI-O works with any Open Container Initiative (OCI) compatible runtime (any one
that is compliant can be plugged in). It supports:

- runC
- ClearContainers

* Workloads and Objects

** Namespace

Namespaces can be seen as virtual clusters. Provides a scope for names (names of
resources need to be unique within a namespace but not across) and allow
to divide the cluster in subsets of resources (*resource*quotas*). All objects
in the same namespace shares the access control policies.

By default you have three namespaces:

- default: Is a catch-all namespace for all objects that do not specify one.
- kube-system: Reserved to objects created by the kubernetes system.
- kube-public: Readable by all users even the non-authenticated.

The namespace is also part of the DNS name that will be created with a service.
When you try to do a dns resolution of a service with just the name, it will
resolve in the _local_ namespace.

    <service>

If you want to cross namespace you will need
to specify it.

    <service>.<namespace>

- [[https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/]]

** Services

Services points to deployments and exposes particular ports to other/external
services. This all happens via *kube-proxy*.

** Config map

Decouples the configuration from the app

* Re/Sources (not already referenced)

- [[https://linuxacademy.com/linux/training/course/name/certified-kubernetes-administrator-preparation-course]]
- [[https://www.safaribooksonline.com/videos/oscon-2017/][Kubernetes hands-on - Kelsey Hightower (Google) - Part 1]]
