CloudWatch Logs
|| Last update: 11 Jul 2018

* Limitations

There can only be one subscription filter associated with a log group. If you are updating an existing filter, you must specify the correct name in filterName . Otherwise, the call fails because you cannot associate a second filter with a log group.

and that's why I said amazon out of the box is limited
https://docs.aws.amazon.com/cli/latest/reference/logs/put-subscription-filter.html
this anticipate the time to think about a buffer

https://stackoverflow.com/questions/46459341/stream-aws-cloudwatch-log-group-to-multiple-aws-elasticsearch-services

> What I ended up using is to create one Lambda that subscribes to the Log Group and then proxy the events into an SNS topic.

    PATTERN="[timestamp=*Z,request_id=*-*,event]"
    ARN="arn:aws:lambda:$REGION:$AWS_ID:function:$NAME"
    PRINCIPAL="logs.$REGION.amazonaws.com"

    while read -r LOG_GROUP_NAME; do
        if [[ "$LOG_GROUP_NAME" == 'EOF' ]]
        then
            echo "LAST LINE"
            exit
        fi
        echo "Adding subscription filter for $LOG_GROUP_NAME"
        FILTERED=$(echo "$LOG_GROUP_NAME" | sed 's/\/aws\/.*\/\(.*\)-\w+/\1/' | sed 's/-/_/g')
        echo "Filter name will be: ${FILTERED}_filter"
        # aws logs put-subscription-filter \
        #     --log-group-name "$LOG_GROUP_NAME" \
        #     --filter-name "${FILTERED}_filter" \
        #     --filter-pattern "$PATTERN" \
        #     --destination-arn "$ARN" \
        #     --profile "$PROF" \
        #     --region "$REGION"
        # You will need to add permissions to read that log group
        # or the subscription won't work
        echo "Adding permission to read from $LOG_GROUP_NAME"
        # aws lambda add-permission \
        #     --function-name $NAME \
        #     --action "lambda:*" \
        #     --statement-id "${FILTERED}_log_permission" \
        #     --principal $PRINCIPAL --source-arn "arn:aws:logs:eu-west-1:430105358689:log-group:${LOG_GROUP_NAME}:*" \
        #     --profile "$PROF" \
        #     --region "$REGION"
        sleep 1
        echo ""
    done <whitelist.txt
    echo "DONE"
    exit 0

But unfortunately:

    An error occurred (LimitExceededException) when calling the
    PutSubscriptionFilter operation: Resource limit exceeded.
    Adding permission to read from /aws/lambda/test

* Possible approach

If you just ingest in one single point with the data growth it will get too much
and it will be overloaded.

Plus, do you want real time?

Instead of processing serially aim to have a fan out.

Kinesis streams:
a Service make it easy to capture, deliver and process streams on AWS.
custom processing, sub second response time, complete control over streams and
processing workflow

Kinesis firehose:

- load massive volumes of streaming data into s3, redishift, es and splunk
- 0 administration
- data latency under 60s
- direct to data store integration
- seamless elasticity

** Kinesis

We could use a buffer in the middle.
What about kinesis streams or kinesis firehose?

Kinesis Streams used to storage data (Domain 2.0: Storage) and Kinesis Firehose
use to collect data (Domain 1.0: Collection)

Firehose is fully managed (i.e. scales automatically) whereas Streams is
manually managed. Second, Firehose only goes to S3 or RedShift, whereas Streams
can go to other services.

With Kinesis Streams, you build applications using the Kinesis Producer Library
put the data into a stream and then process it with an application that uses
the Kinesis Client Library and with Kinesis Connector Library send the processed
data to S3, Redshift, DynamoDB etc.

With Kinesis Firehose it’s a bit simpler where you create the delivery stream
and send the data to S3, Redshift or ElasticSearch (using the Kinesis Agent or
API) directly and storing it in those services.

Kinesis Streams on the other hand can store the data for up to 7 days. Which is
 why is leans towards to Domain 2.0: Storage.

You may use Kinesis Streams if you want to do some custom processing with
streaming data. With Kinesis Firehose you are simply ingesting it into S3,
Redshift or ElasticSearch.

Firehose you are creating a delivery stream to send the data to Redshift, S3 or
ElasticSearch

Using firehose:

- [[https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html]]
- [[https://aws.amazon.com/blogs/database/send-apache-web-logs-to-amazon-elasticsearch-service-with-kinesis-firehose/]]

Using:
- [[https://github.com/logstash-plugins/logstash-input-kinesis]]

*** About firehose

- [[https://aws.amazon.com/blogs/database/send-apache-web-logs-to-amazon-elasticsearch-service-with-kinesis-firehose/]]

At the time of this article these are the allowed output:

[[https://docs.aws.amazon.com/firehose/latest/dev/create-destination.html#create-destination-splunk]]

- Amazon S3
- Amazon Redshift
- Amazon ES
- Splunk

There is also a problem with the document id if you have your own:

- [[https://forums.aws.amazon.com/thread.jspa?threadID=237104]]

** Plus lambda

- [[https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html]]

lambda after the kinesis streams

Log Data -> Kinesis streams -> lambda -> es

CloudWatch, Kinesis Stream, Lambda and ELK to visualize these events.

* Solution?

Kinesis Data stream + lambda

kinesis firehose only works with their es service.
plus you can't guarante the document id

- [[https://forums.aws.amazon.com/thread.jspa?threadID=237104]]

lambda + kinesis

be aware of

        All our lambdas were triggered by Kinesis streams and in 1–2%
        of cases we observed the same KinesisRecord being processed
        twice. What was weird, however, that time passing between first
        and second invocation of a lambda with the same entry data was
        exactly 10 minutes. We performed an investigation and ruled out
        a bug in our business logic and couldn’t find anything that would
        explain this behavior. It was so startling for us that it took
        us some time to even attempt to google it — and there it was:
        https://stackoverflow.com/questions/43786225/s3-lambda-trigger-double-invocation-after-exactly-10-minutes.

Source: [[https://medium.com/@martatatiana/aws-lambdas-triggered-by-kinesis-streams-in-java-a-few-lessons-d1cffe5ea683]]

Lambda real-time event sources follows different patterns:

.image images/lambda_event_sources.png
.caption Source: [[https://www.slideshare.net/AmazonWebServices/real-time-data-processing-using-aws-lambda-devday-austin-2017]]

- s3/sns have asynch invocation (asynch push model)
- iot/alexa have synch invocation (sych push model)
- DynamoDB/kinesis have synch invocation (stream pull model)

If you use lambda, best practise is to write the function to be
stateless and initiate aws clients and database clients OUTSIDE
the scope of the function handler to take advantage of connection reuse.

IMP: No control over the number of processors per queue.
[[https://docs.aws.amazon.com/lambda/latest/dg/scaling.html][You get one Lambda processor per Kinesis shard]].


        Poll-based event sources that are stream-based:
        For Lambda functions that process Kinesis or DynamoDB streams
        the number of shards is the unit of concurrency. If your stream
        has 100 active shards, there will be at most 100 Lambda function
        invocations running concurrently. This is because Lambda processes
        each shard’s events in sequence.
        Source: https://docs.aws.amazon.com/lambda/latest/dg/scaling.html

Kinesis and lambda throttling:

- [[https://docs.aws.amazon.com/lambda/latest/dg/concurrent-executions.html]]

KCL is the same:

- [[https://stackoverflow.com/questions/34503226/multiple-consumers-per-kinesis-shard/34509567#34509567]]

Lambda retries until the data expires with kinesis.

Amazon kineis is mapped as event source in lambda.
Use batch size to have less calls (but they will be longer and take more memory)

.image images/streams.png
.caption Source: [[https://www.slideshare.net/AmazonWebServices/real-time-data-processing-using-aws-lambda-devday-austin-2017]]

For each shard you will have one lambda. So we need to be careful not to follow behind.

.image images/throughput.png
.caption Source: [[https://www.slideshare.net/AmazonWebServices/real-time-data-processing-using-aws-lambda-devday-austin-2017]]

A wrong package blocks the lambda:

.image images/retries.png
.caption Source: [[https://www.slideshare.net/AmazonWebServices/real-time-data-processing-using-aws-lambda-devday-austin-2017]]

watch to monitor:

Kinesis:

- getrecords (throughput)
- putrecord (bytes, latency, records etc)
- getrecords,iteratorgaemilliseconds: how old your last processed records where

Lambda:

- Invocation count
- duration
- errorcount
- throttlecount
- iteratorage: time elapsed for batch

** Attach cloudwatch logs to kinesis

- [[https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Subscriptions.html]]

** Execute lambda

- [[https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html]]

Manual not lambda:

- [[https://gist.github.com/doi-t/58d5a25c11a798e999b5cd4487287b28]]

** Implement a DLQ

- [[https://docs.aws.amazon.com/lambda/latest/dg/dlq.html]]

* Beware of limits

- [[https://docs.aws.amazon.com/streams/latest/dev/service-sizes-and-limits.html]]

* Unrelated

Ho to fix multiline in Java stack:

- [[http://docs.aws.amazon.com/lambda/latest/dg/java-logging.html]]
- [[https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html]]

* Re/Sources

- [[https://www.slideshare.net/AmazonWebServices/real-time-data-processing-using-aws-lambda-devday-austin-2017]]
