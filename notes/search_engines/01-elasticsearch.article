ElasticSearch
|| Last update: 11 Apr 2019

* Intro

UPDATE: Elastic reply to AWS creating their own distro [[https://www.elastic.co/blog/on-open-distros-open-source-and-building-a-company]]

AWS articles:

- [[https://aws.amazon.com/blogs/aws/new-open-distro-for-elasticsearch/]]
- [[https://aws.amazon.com/blogs/opensource/keeping-open-source-open-open-distro-for-elasticsearch/]]

* Node types

A node is a running instance of elasticsearch.
Nodes can store data locally or not.
If you have set the app to store data, shards can and will be allocated locally.

** Data nodes

Holds the data in shards (which are lucene indexes).
Performs indexing and search queries.

    node.data = true
    node.masters = false

** Coordinating nodes (known before as client nodes)

Smart load balancers that expose a rest interface.
It is responsible for routing queries to nodes with relevant shards and aggregating results.
It is never rerouted to another client node.

The reason behind the creation of dedicated clients is (no data on them) is to dedicate all
their computational power to the *scatter/gather* phase of the search.

    node.data = false
    node.masters = false

You can actually _shut_down_ the HTTP transport on the other nodes to make sure they don't
reply HTTP at all (I didn't thought, I call them sometimes to check stuff).

    ## You can still communicate through the transport module (TCP)
    http.enabled = false

*NOTE*: It is recommended to set a long lived http req to the client nodes.

** Master nodes

Lightweight operational (cluster management) responsibility.
A master holds the _cluster_state_, and handles the shard distribution.
The master node *isn't* involved in any search or document change of any type.

    node.data = false
    node.masters = true

Is good practice to have a set of _dedicated_ master nodes.
Since they work through an election system you need an odd number of them (min 3).

** Nuts and bolts

By default, all nodes are data nodes. What you want is divide the
responsibilities. Separating concerns is good and let us optimise each type of
node for it's particular workload.

Client only nodes:

- https req parsing
- network overload (avoid)
- perform gather process

Data nodes only:

- search
- index

* Concepts

** Cluster

A cluster consists in one or more nodes with the same cluster names.

** Index

An index is just a *logical*namespace* that point to physical shards.
Read requests can be handled by a _primary_ or _replica_ shard. More copies
of the data you have, the more search throughput you can handle.

NOTE: Once all replica shards report success, success is reported to the user.

*** Indexing

New documents to ES pass to primary shards, add to storage and add to inverted
index.

LIMITATIONS

- [[https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping.html]]

If you are trying to add metadata into the log document beware of the
limitations:

  RequestError(400, 'illegal_argument_exception', 'Limit of mapping depth [20] in
  index [xxx-2018.09.05] has been exceeded due to object field [...]'):

** Search

We don't know which document will match the query... it could be in any shard.
A search request consults *a*copy*of*every*shard*. There are two phases:

- query (scatter and gather): in this phase replicas increase throughput
- fetch: the _coordinating_node_ multi-get all that does it need

The coordinating node perform a _merge_sort_ on the combined result from other
shards. Is CPU intensive and memory intensive.

* Performance and operations

You can increase performances at the cost of data security:

- Replication sync (default) or async: Wait or not for hack from the replicas.
- Consistency: Is the quorum. How many shards need to report to the primary
before any write request is accepted (protects again split brain problems).

** Search: Adaptive Replica Selection (default in v7.0+)

- [[https://www.elastic.co/guide/en/elasticsearch/reference/7.0/search.html#search-adaptive-replica]]

    In Elasticsearch 6.x and prior, a series of search requests to the same
    shard would be forwarded to the primary and each replica in round robin
    fashion.
    [...]
    In 6.1, we added an experimental feature called Adaptive Replica Selection.
    Each node tracks and compares how long search requests to other nodes take,
    and uses this information to adjust how frequently to send requests to
    shards on particular nodes.
    Source: https://www.elastic.co/blog/elasticsearch-7-0-0-released

** Multi-cluster

- [[https://www.elastic.co/blog/tribe-nodes-and-cross-cluster-search-the-future-of-federated-search-in-elasticsearch]]

Cross cluster search:

    In Elasticsearch 7.0, we’re adding a new execution mode for cross-cluster
    search: one which has fewer round-trips when they aren’t necessary. This
    mode (ccs_minimize_roundtrips) can result in faster searches when the
    cross-cluster search spans high-latencies, e.g. across a WAN.
    Source: https://www.elastic.co/blog/elasticsearch-7-0-0-released

* To know (could be obsolete - need verification)

In AWS Use a small number of data nodes not on spot, to have safety of the data.
In AWS Use spot instances to boost performances.

Currently ingest node only executes ingest preprocessing on the index thread pool.
For both index requests and bulk requests. This should be changed, if pipelines
are specified on bulk requests the bulk TP should be used for ingest preprocessing.

The ingest stats include the following statistics:

    ingest.ingest_total- The total number of document ingested during the lifetime of this node
    ingest.ingest_time_in_millis - The total time spent on ingest preprocessing documents during the lifetime of this node
    ingest.ingest_current - The total number of documents currently being ingested.
    ingest.ingest_failed - The total number ingest preprocessing operations failed during the lifetime of this node
    Also the ingest stats contain a break down of the above stats on a per pipeline basis. These stats are automatically updated if pipelines are added or removed. This information is useful to give insight how much nodes spent on ingest related activities.

- Add ingest_took to bulk response
- Add ingest info to node info API, which contains a list of available processors
- Ingest: use bulk thread pool for bulk request processing (was index before)
- Enable acked indexing
- Add points to SegmentStats
- new jvm option files (jvm.options)
- Add GC overhead logging
- STATS: Add I/O statistics on Linux (WAO)

New: /_cluster/allocation/explain

- [[https://github.com/elastic/elasticsearch/pull/17305]]

* Re/Sources

- [[https://www.npmjs.com/package/elasticdump2]]
- [[http://blog.kiyanpro.com/2016/03/11/elasticsearch/Export-Index-Using-Elasticsearch-Dump]]
- [[http://blog.tamasboros.com/export-elasticsearch-database-to-json-dumps/]]
- [[https://github.com/taskrabbit/elasticsearch-dump]]
- [[https://hub.docker.com/r/khanhicetea/elasticsearch-dump/~/dockerfile/]]
