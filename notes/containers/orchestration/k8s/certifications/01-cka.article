CKA/CKAD notes
|| Last update: 12 Sept 2020

* Kubernetes course: CKA certified kubernetes administrator

.image images/k8s_high_level.png _ 700
.caption Source: [[https://www.safaribooksonline.com/videos/oscon-2017/][Kubernetes hands-on - Kelsey Hightower (Google) - Part 1]]

Kubernetes is build on the concept of transient, decoupled objects connected
together. It is based on 15 years of experience at google building Borg.

- [[https://ai.google/research/pubs/pub43438][Borg paper]]

Communication from outside or internal services is API driven.
All configuration is saved in JSON format but often users write it in
YAML. k8s will transform it in json before sending it to the API.
Orchestration is managed by controllers (a series of watch-loops). All of them
interrogates the API server for a particular object state and act accordingly.

* The physical cluster

At high level there are two types of roles: _masters_ and _nodes_. In HA there
can be more than one master node but only one of them can be a _leader_.

        For fault tolerance purposes, there can be more than one master node in
        the cluster. If we have more than one master node, they would be in a HA
        (High Availability) mode, and only one of them will be the leader,
        performing all the operations. The rest of the master nodes would be
        followers.
        Source: https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS158x+1T2018

The master node hosts the *control*plane*.
All master nodes connect to *etcd*.

** Nodes memory

Kubernetes master and node servers are expected to have *swap*disabled*.
This is the recommended deployment. If swap is not disabled, kubelet service
will not start unless a specific flag is passed at startup time.

To disable swap on a node:

    # As root
    swapoff -a

and then remove it from the _/etc/fstab_ file.

You could disable this check by passing _--fail-swap-on=false_ but it's not
advised.

    The kubelet now fails if swap is enabled on a node. To override the default
    and run with /proc/swaps on, set --fail-swap-on=false.
    Source: https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.8.md#before-upgrading

- [[https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/]]
- [[https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.8.md#before-upgrading]]
- [[https://kubernetes.io/docs/setup/independent/]]
- [[https://github.com/kubernetes/kubernetes/issues/53533]]

*** Node health

*kube-controller-manager* assigns CIDR blocks to a new node, keep track of the
nodes health and check with the cloud provider is the node is still there.

After _40s_ it will go in condition *unknown* if the heartbeat fails. After _5m_
it will start to evict pods. It will check the status every few seconds. There
is a flag called:

    --node-monitor-period duration     Default: 5s
    The period for syncing NodeStatus in NodeController.
    Source: https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/

It will also check what percentage of nodes in that zone is un-healty.
There is an unhealthy zone threshold and there are rules for each AZ.

    --unhealthy-zone-threshold float32     Default: 0.55
      Fraction of Nodes in a zone which needs to be not Ready (minimum 3)
      for zone to be treated as unhealthy.

The eviction rate is parametric. 0.1 per second means that no more than one pod
per node every 10s.

    --node-eviction-rate float32     Default: 0.1
      Number of nodes per second on which pods are deleted in case of node
      failure when a zone is healthy (see --unhealthy-zone-threshold for
      definition of healthy/unhealthy). Zone refers to entire cluster in
      non-multizone clusters.

The eviction rate is *reduced* to the secondary node.

    --secondary-node-eviction-rate float32     Default: 0.01
      Number of nodes per second on which pods are deleted in case of node failure
      when a zone is unhealthy (see --unhealthy-zone-threshold for definition of
      healthy/unhealthy). Zone refers to entire cluster in non-multizone clusters.
      This value is implicitly overridden to 0 if the cluster size is smaller
      than --large-cluster-size-threshold.

If you want to go multi-az you can shift the workload to a new zone if the first
one gets unhealthy. If the system sees ALL nodes as unhealthy it will
_stop_all_evictions_.

From 1.8+ the node controller can *taint* node to respect their status.

- [[https://kubernetes.io/docs/concepts/architecture/nodes/]]

** Network and CNI

- [[https://kubernetes.io/docs/concepts/cluster-administration/networking/]]

Kubernetes made some interesting choices on how networking should be implemented.
It defines the network model but leaves it to third party plugins a way to
implement it. There are three networking problem to think about:

- Container to container (in a Pod) communication (solved by sharing network namespace)
- Pod to Pod communication (left to the CNI plugin to implement)
- External world to Pod communication (solved by services)

k8s specify that there shouldn't be any Network Address Translation (NAT) while
doing the Pod-to-Pod communication across hosts. To do so or you make routable pods
and nodes like on google and amazon or you use a _Software_Defined_Networking_.
A CNI plugin is an *executable* invoked by the container management system.

- [[https://github.com/containernetworking/cni]]

    CNI (Container Network Interface), a Cloud Native Computing Foundation
    project, consists of a specification and libraries for writing plugins
    to configure network interfaces in Linux containers, along with a number
    of supported plugins. CNI concerns itself only with network connectivity of
    containers and removing allocated resources when the container is deleted.
    Because of this focus, CNI has a wide range of support and the specification
    is simple to implement.
    Source: https://github.com/containernetworking/cni

It's responsible for:

- Inserting the network interface (one end of a [[http://man7.org/linux/man-pages/man4/veth.4.html][vETH pair]]) in the container network namespace
- Making changes to the host: like attaching the other end of the vETH pair to the bridge network
- Assign the IP, put up the routes and apply IPAM rules depending on your topology

Some of the plugins that implement this functionalities are:

- [[https://coreos.com/flannel/docs/latest/]]
- [[https://github.com/projectcalico/calico-cni]]
- [[https://github.com/weaveworks/weave]]

    The container runtime offloads the IP assignment to CNI, which connects to
    the underlying configured plugin, like Bridge or MACvlan, to get the IP
    address. Once the IP address is given by the respective plugin, CNI forwards
    it back to the requested container runtime.
    Source: https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS158x+1T2018/

There can only be one pod network per cluster, but the
[[https://github.com/Huawei-PaaS/CNI-Genie][CNI Genie]] project is trying to
remove this limitation.

*** Flannel

- [[https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/]]

When you configure flannel you will need to pass the CIDR block your k8s cluster
will need to use and assign it to the nodes.

You can define this CIDR with:

    --pod-network-cidr=10.244.0.0/16

Flannel makes use of an overlay network, usually
[[https://en.wikipedia.org/wiki/Virtual_Extensible_LAN][Virtual Extensible LAN (VXLAN)]].

Flannel however is not able to enforce network policies like Calico and Weave Net.

* Kubernetes master node control plane components

.image images/pre-ccm-arch.png _ 800
.caption Source: [[https://kubernetes.io/docs/concepts/architecture/cloud-controller/]]

The *kubernetes*control*plane* consists of a collection of processes running on
the cluster masters as daemon or docker containers (kubeadm). It is a collection
of *three*processes* designed to run on a single node in your cluster. They are:

- [[https://kubernetes.io/docs/admin/kube-apiserver/][kube-apiserver]]
- [[https://kubernetes.io/docs/admin/kube-scheduler/][kube-scheduler]]
- [[https://kubernetes.io/docs/admin/kube-controller-manager/][kube-controller-manager]]/cloud-controller-manager

Also add-ons like the DNS services are usually present.

Administrative tasks are made by communicating with the *apiserver*. Commands
are send in *JSON* via *REST* and the api validates and process the requests.
The result is saved on *ETCD*. The apiserver scale horizontally depending on the
traffic. It is the only component speaking with ETCD.

The *scheduler* schedule a Pod to nodes. It takes these decisions using
a whole lot of metrics it gets from the nodes plus the user requirements (like
nodeSelector). It will also check restrictions like if Volumes needs to be
mounted and where. Examples:

- Quota restrictions
- Taints and tolerations
- Labels
- Volume needs

It schedules the work in terms of Pods and Services. It listens to the apiserver
for the creation of new pods.

The *controller-manager* manages many non-terminating control loops.
Can be seen as a set of different master processes with the intent of matching
the desired state and actual state.
Each of them manages a specific object and watch their current state though the
API server. If the state differs from the desired one it will take actions to
change it. It can be present in one of this two flavours:

- kube-controller-manager (phisical)
- cloud-controller-manager (cloud env)

There are several controller in use like:

- endpoints
- namespace
- replication

and more.

** Controllers (specifics)

- [[https://github.com/kubernetes/sample-controller/blob/master/docs/controller-client-go.md]]

** (BETA in v1.13) Cloud controller manager

It is optional and only for cloud environments:

- [[https://kubernetes.io/docs/concepts/architecture/cloud-controller/]]
- [[https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/]]

.image images/post-ccm-arch.png _ 800
.caption Source: [[https://kubernetes.io/docs/concepts/architecture/cloud-controller/]]

It is a daemon that manages cloud related resources in a control loop. It can
implement:

- Node controller: Updated the nodes status by coordinating with the cloud API.
- Service controller: Listen to service create, udpate and delete and responsible for LoadBalancer types
- Route controller: Responsible for setting up network routes (only on gce)
- Persistent volume labels controller: Sets zone and region labels on PersistentVolumes

To run this beta functionality you will need to:

- *NOT* specify the --cloud-provider flag in kube-apiserver and kube-controller-manager
- Run kubelet with the flag *--cloud-provider=external*
- The kube-apiserver *should*not* run the PersistentVolumeLabel admission controller
- Set the correct *InitializerConfiguration* to label persistent volumes

TODO: check --cloud-provider-external in the kubelet

** kube-apiserver

- [[https://kubernetes.io/docs/admin/kube-apiserver/]]

Kubernetes exposes an API via the kube-apiserver (entrypoint to all k8s
interactions). As long as you have
access to the node and the right credentials, you can query the api. The
HTTP API space of k8s can be divided into three independent groups:

- Core group (/api/v1): basic objects. This is mostly due to historical reasons
- Named group: Objects in the /apis/$NAME/$VERSION format
- System wide: Like /healthz, /logs, /metrics, etz

The apiserver if asked _/apis_ will return the API groups. API groups can have
multiple versions and dollow a domina name format. Some names are reserved like:

- single-word domains
- empty group
- any name ending in .k8s.io

The apiVersion field in the object's configuration file define the API
endpoint to connect the API server.

The v1 API group is a collection of groups (there are eight at the moment).
For example:

- v1
- storage.k8s.io/v1
- rbac.authorization.k8s.io/v1

Kubernetes uses the _resourceVersion_ value to find if there are updates to
implement. It uses what is called _optimistic_concurrency_. When a object needs
to be modified, if the _resourceVersion_ is changed then a 409 confict is returned.
This field is implemented by the _modifiedIndex_ parameter in etcd, and is unique
to namespace, kind and server.

The "kubectl proxy" command creates a local service to access a ClusterIP.
Will allow to send requests to localhost on the proxy port. Useful for
troubleshooting or development work.

    Without kubectl proxy configured, we can get the Bearer Token using kubectl,
    and then send it with the API request. A Bearer Token is an access token
    which is generated by the authentication server (the API server on the
    master node) and given back to the client. Using that token, the client can
    connect back to the Kubernetes API server without providing further
    authentication details, and then, access resources.
    Source: https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS158x+1T2018/

* Kubernetes worker node components

It has three major components. It runs two processes:

- [[https://kubernetes.io/docs/admin/kubelet/][kubelet]]
- [[https://kubernetes.io/docs/admin/kube-proxy/][kube-proxy]]

The kubelet communicates with the api, run containers, manage resources and
watches over status.

It will also need a container runtime like:

- [[http://cri-o.io/][cri-o]]
- [[https://containerd.io/][containerd]] (more notes on another page)
- [[https://coreos.com/rkt/][rkt]]
- [[https://linuxcontainers.org/lxd/][lxd]]

** Kube-proxy

- [[https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/]]
- [[https://kubernetes.io/docs/concepts/services-networking/service/#the-gory-details-of-virtual-ips]]

*kube-proxy* is a network proxy in charge of managing the network connectivity.
Monitor the _apiserver_ for changes in Service and Endpoint objects.
It reflects these networking changes on each node by managing network rules.
It will update the rules when new objects are created or removed.

    This reflects services as defined in the Kubernetes API on each node and can
    do simple TCP, UDP, and SCTP stream forwarding or round robin TCP, UDP, and
    SCTP forwarding across a set of backends.
    Source: https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/

By default for each service and endpoint created. It opens a random port and
listens for traffic to the *ClusterIP:Port* and redirect the traffic to one
of the service _endpoints_.
It support three proxy modes that can be sent via flag at initialization:

- userspace (v1.0): TCP/UDP over IP or Layer 4
- iptables (v1.1+) (*default*)
- [[http://www.linuxvirtualserver.org/software/ipvs.html][ipvs]] (beta) (v1.9+): managed by the Huawei team

    --proxy-mode ProxyMode
    Which proxy mode to use: 'userspace' (older) or 'iptables' (faster) or 'ipvs'
    (experimental). If blank, use the best-available proxy (currently iptables).
    If the iptables proxy is selected, regardless of how, but the system's kernel
    or iptables versions are insufficient, this always falls back to the userspace
    proxy.
    Source: https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/

In *iptables* mode, it will set up *iptable* rules to capture the traffic.
It uses a _Readiness_Probe_ to ensure all pods are functional priori to
connection. It supports up to 5k nodes.

.image images/iptables.png _ 500
.caption kube-proxy with iptables. Source: [[https://kubernetes.io/docs/concepts/services-networking/service/]]

On very big clusters the iptable cluttering can become an issue not only on
performance. The *IPVS* mode allow faster traggic and refresh while keeping
a cleaner system (almost no iptables rule).
The *IPVS* mode requires the IPVS modules installed in the kernel. It works in
kernel space and is faster than the other options. Allows for more complex and
smarter load balancing:

- the typical round robin
- shortest expected delay
- least connection

and more. It can manage more than 5k nodes.

    IPVS (IP Virtual Server) implements transport-layer load balancing inside
    the Linux kernel, so called Layer-4 switching. IPVS running on a host acts
    as a load balancer at the front of a cluster of real servers, it can direct
    requests for TCP/UDP based services to the real servers, and makes services
    of the real servers to appear as a virtual service on a single IP address.
    Source: http://www.linuxvirtualserver.org/software/ipvs.html

Interesting tidbit, Datadog used IPVS in their cluster and they met a lot of bugs
with it.

- Accessing services from the host doesn't work
- no localhost:nodeport
- no graceful termination

All should be fixed in version 1.12 but if you want to know more look at this
video: [[https://www.slideshare.net/lbernail/kubernetes-at-datadog-the-very-hard-way]]

* Metadata

- [[https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/]]
- [[https://kubernetes.io/docs/concepts/overview/working-with-objects/names/]]
- [[https://github.com/kubernetes/community/blob/master/contributors/design-proposals/architecture/identifiers.md]]

The *metadata* is what _uniquely_ identify a resource. All objects have unique:

- UID: System generated
- Name: Client-given

    UID: A non-empty, opaque, system-generated value guaranteed to be unique in
    time and space; intended to distinguish between historical occurrences of
    similar entities.
    Source: https://github.com/kubernetes/community/blob/master/contributors/design-proposals/architecture/identifiers.md

    Name: A non-empty string guaranteed to be unique within a given scope at a
    particular time; used in resource URLs; provided by clients at creation time
    and encouraged to be human friendly; intended to facilitate creation
    idempotence and space-uniqueness of singleton objects, distinguish distinct
    entities, and reference particular entities across operations.
    Source: https://github.com/kubernetes/community/blob/master/contributors/design-proposals/architecture/identifiers.md

    Every object created over the whole lifetime of a Kubernetes cluster has a
    distinct UID. It is intended to distinguish between historical occurrences of
    similar entities.
    Source: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/

You can use either labels or annotation to attach metadata to k8s objects.

- *annotations* are *not* used to indentify or select objects
- *labels* can be used to select objects and to find collections of object that satisfy certain condition

They are both key value maps.

To show all the pods with a specific label run:

    kubectl get pods -L labelnameone,labelnametwo,etc

You can also use it as a selector:

    kubectl get pods -l labelnameone==thisvalue

** Annotations

- [[https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/]]

Annotations are used to attach arbitrary *non-identifying*metadata* to object
in a key-value format. Tools can retrieve this metadata to implement additional
functionality. They cannot be used by kubernetes commands.

The metadata in an annotation can be small or large, structured or unstructured,
and can include characters not permitted by labels. They can be larger than
labels.

An example of an annotation is a 'description' or git commit. Contents of the
annotation field is displayed when running the _describe_ command.

It can be implemented with kubectl:

    kubectl annotate pods --all env="dev" -n dev
    kubectl annotate --overwrite pods env="dev" -n dev
    kubectl annotate pods test env- -n dev # remove it

** Labels

- [[https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/]]
- [[https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/]]

Labels are key-value pairs, defined
under the metadata field, that can be attached to any _Object_.

    Labels are intended to be used to specify identifying attributes of objects
    that are meaningful and relevant to users.
    [...]
    We don’t want to pollute labels with non-identifying, especially large
    and/or structured, data.
    Source: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/

They *do*not*provide*uniqueness* to objects. Each key needs to be unique for any
given object! Non-identifying information should be recorded using _annotations_.

Labels are generally used to map organizational/logic structures onto the system
objects.
They can be used to organize and select a subset of objects (for example to do
bulk operations) and allow cross-cutting operations (more flexible than the
hierarchical structure).

They can have a prefix structured as a DNS subdomain:

    Valid label keys have two segments: an optional prefix and name, separated by a
    slash (/). The name segment is required and must be 63 characters or less.
    The prefix is optional. If specified, the prefix must be a DNS subdomain: a
    series of DNS labels separated by dots (.), not longer than 253 characters in
    total, followed by a slash (/). If the prefix is omitted, the label Key is
    presumed to be private to the user. The kubernetes.io/ prefix is reserved for
    Kubernetes core components.
    Valid label values must be 63 characters or less.
    Source: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/

Internally ks8 *index* and *reverse-index* labels for efficient queries and
_watches_.

You can add a label to any object via the cli by using the *kubectl*label*
command:

    kubectl label <object> <object_name> <key>=<value>
    # example
    kubectl label deployment nginx app=test

If a label is already present with that name you can use the *--override* flag
to change it.
You can see in the cli all the labels of an _Object_ by passing *--show-labels*:

    kubectl get deployments,rs,pod --all-namespaces --show-labels

You can show the of a label in the kubectl response by passing the *-L* flag.

    $ kubectl get pods -L app
    NAME                     READY   STATUS    RESTARTS   AGE   APP
    nginx-7f6cdb7fd7-bqvfv   1/1     Running   0          25m   nginx
    nginx-7f6cdb7fd7-c8696   1/1     Running   0          28m   test

Finally you can select a subset of element by using it as a selector with *-l*.

    kubectl get deploy,rs,po -l app=nginx

Selecting via labels allow you to use two type of selections:

- Equality based: The supported operators are =, ==, !=
- Set bases: The supported operators here are in , notin and exists

There is also a limited support for selection via field called *field*selector*:

    kubectl get pod --field-selector metadata.name=a-pod
    kubectl get pod --field-selector spec.name=an-example
    kubectl get pod --field-selector metadata.namespace=default

There are a set of recommended labels that k8s advices at
[[https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/]].

Exercises: [[https://github.com/jecnua/certified-kubernetes-administrator-cka-kata/tree/master/01-labels]]

** Label selectors

- [[https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors]]

Label selectors allow to indentify a set of objects. It is considered the core
grouping primitive of k8s.

There are two types of _selectors_:

- Equality-based: Allow filtering of objects based on labels keys and values. Three kinds of operators are supported: =, ==, or !=
- Set-based: Allow filtering of objects based on a set of values. Three kinds of operators are supported: in, notin, and exist(only the key identifier)

    In the case of multiple requirements, all must be satisfied so the comma
    separator acts as a logical AND (&&) operator.
    Source: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors

Example Equality-based:

    environment=production,tier!=frontend

Example Set-based:

    environment in (production, qa)
    tier notin (frontend, backend)
    partition
    !partition
    partition in (customerA, customerB),environment!=qa

Very important:

    Note: the label selectors of two controllers must not overlap within a
    namespace, otherwise they will fight with each other.
    Source: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors

*Set*operators*are*more*expressive* and allow the *OR* operator.

    $ kubectl get pods -l environment=production,tier=frontend
    $ kubectl get pods -l 'environment in (production),tier in (frontend)'

Set operators are supported by the newer resources but not old ones like
ReplicationController.

* Cluster to/from Master communication

- [[https://kubernetes.io/docs/concepts/architecture/master-node-communication/]]

Communication from the cluster to the master is SSL encrypted.
The only service _exposed_ to external traffic is the api-server.
The API server listens on port 443 for HTTPS traffic. A form of AUTH should be
enabled.

    Nodes should be provisioned with the public root certificate for the cluster
    such that they can connect securely to the apiserver along with valid client
    credentials. For example, on a default GKE deployment, the client credentials
    provided to the kubelet are in the form of a client certificate.
    [...]
    Pods that wish to connect to the apiserver can do so securely by leveraging
    a service account so that Kubernetes will automatically inject the public
    root certificate and a valid bearer token into the pod when it is
    instantiated. The kubernetes service (in all namespaces) is configured with
    a virtual IP address that is redirected (via kube-proxy) to the HTTPS
    endpoint on the apiserver.
    Source: https://kubernetes.io/docs/concepts/architecture/master-node-communication/

Master to nodes is much less secure. There are two main types: apiserver to kubelet
and apiserver to nodes/pods/services.
For the cases when the apiserver needs to communicate with the cluster (kubelet)
like:

- Getting Logs
- Attaching to running pods
- Providing the port-forwarding

It is not secure. The certificate is not verified by default and is vulnerable to
man in the middle attach. Don't run them on public network. Possible solutions are:
use the flag --kubelet-certificate-authority or do an ssh tunnel.

Communications to  nodes/pods/services are totally unsafe because they are plain
http (neither authenticated or encrypted). Possible solution ssh tunneling.

** Certificates in kubernetes

- [[https://www.youtube.com/watch?v=gXz4cq3PKdg][Certifik8s: All You Need to Know About Certificates in Kubernetes [I] - Alexander Brand, Apprenda]]

Certificates allow party in a conversation to authenticate each other.
For example use a third party both can trust (CA). CA issue cert for all these
party.

All interaction with it needs to be secure.

1) We need to create a cluster certificate authority. It needs to be the
*trusted*root* throughout the cluster. All the cert in the cluster need to be
signed by this CA. This way went the receive a cert than can check it's signed
by the CA and trust it.

NOTE: For HA (multiple API server) you want to be sure the IPs of the LB and the
DNS is part of the certificate Subject Alternative Name (SAN) field. Otherwise
They are going to complain. Each master has it's own cert.

2) First thing to secure is the API server. We need to protect the https. Flag
are --tsl-cert-file and --tls-private-key-file.

3) Kubelet api (internal private api) needs to be secured. Kubelet exposes an
HTTPs api. Consumed by API server to get logs, metrics, exec, etc.
Is also protected by authentication and authorization. Also client (the api
server) needs to have a client cert.

X.509 Client Cert Authentication

Is the strategy for authenticating requests that present a client certificate.
Used by k8s components but also end users. Any request that present a clietn
certificate signed by the Cluster CA is authenticated. User is obtained from
*Common*Name*(CN)* field. Groups are obtained from *Organization*field*.
User information and group memebership!

Each component will have a different cert because they will have a different
access to the cluster. So they have their own identify. Only kubelet have an
organization and the hostname is part of the certificate.
You want to make sure that each node (kubelet) in the cluster have it's own
identity. This different treatment will allow you to user specific things like
*Node*authorizationzer* and *Node*Restriction*Admission*Plugin*. Limit access to
the cluster from the kubelet. Without, the kubelet would be able to change any
resource of the cluster via API. So basically limits the read/write access to the
resources that are related to the node itself and pods bound to the node.

- Controller manager, common name: system:kube-controller-manager
- Scheduler, common name: system:kube-scheduler
- Kube Proxy, common name: system:kube-proxy
- Kubelet, common name: system:node:${hostname}, Organization: system:nodes

Options:

- Manually create it
- There is a certificate generation API

kubelet need a client cert to speak with the API and a cert for it's own api.
The kubelet CAN request a certificate as it starts up. This is build on top of
the Certficate API and bootstrap token authenticator.

- [[https://github.com/apprenda/kismatic]] To look at

*** certificates.k8s.io

- [[https://db-blog.web.cern.ch/blog/lukas-gedvilas/2018-02-creating-tls-certificates-using-kubernetes-api]]

The API is certificates.k8s.io/v1beta1. Clients create a certificate signing
request and send it to the API. The requesting user is stored in the CSR.
They remain pending until approved by the admin. Cert is issued once the CSR
is approved.

The controller manager signs the cert so it needs access to the CA private key.

* The kubelet

The kubelet is a *node*agent* and runs as a *daemon* on every server (master and
nodes).
It communicates with the master node and runs containers as stated on the
_desired_state_. It makes sure all containers are healthy at all times and is in
charge of killing and restarting unhealthy ones.
It's _extremely_ configurable via flags. Allows you to pass _feature_flags_
to allow ALPHA and BETA functionalities.
It thinks and works in term of pods (_PodSpec_) and not higher level constructs.

NOTE: The kubelet can also access PodSpec via File (path), HTTP endpoint and
HTTP server. However they are rarely used.

If a Pod needs access to storage, configmaps or secrets, is the kubelet that makes
it happen.

The kubelet interfaces with the container runtime via the Container Runtime
Interface (CRI).

** Kubelet ports

- [[https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/]]

The kubelet by default serve on port 10250.

    The read-only port for the Kubelet to serve on with no
    authentication/authorization (set to 0 to disable) (default 10255)
    [...]
    --healthz-port int32
    The port of the localhost healthz endpoint (set to 0 to disable) (default 10248)
    Source: https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/

** Kubelet bootstrap

    *TO*CHECK*
    When the kubelet starts, it will seek out the master node and try to listen (???).
    Reports success or failure to the master (???)
    *TO*CHECK*

    *TO*CHECK*
    It accepts API calls for specifications? is it called or polls?
    *TO*CHECK*

** Kubelet TLS bootstrap (GA 1.12+)

- [[https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/]]

When the kubelet first start, it generate a self-singed certificate/key pair
(that is used to accept requests) and create a Certificate Signing Request (CSR)
for submission to a cluster-level certificate signing process.
This process query the cluster API server to get an associated certificate
signed by the cluster’s root certificate authority.

    Before, when a kubelet ran for the first time, it had to be given client
    credentials in an out-of-band process during cluster startup. The burden was
    on the operator to provision these credentials. Because this task was so
    onerous to manually execute and complex to automate, many operators deployed
    clusters with a single credential and single identity for all kubelets.
    These setups prevented deployment of node lockdown features like the Node
    authorizer and the NodeRestriction admission controller.
    Source: https://kubernetes.io/blog/2018/09/27/kubernetes-1.12-kubelet-tls-bootstrap-and-azure-virtual-machine-scale-sets-vmss-move-to-general-availability/

This is based on the API that allows to request certificates from
a cluster-level Certificate Authority (CA).
Allows the provision of TSL client certificate for the kubelets and allow the
creation of a TSL-secured cluster.

It automates the provision and distribution of signed certificates.

In *BETA* there is a functionality that as certificates approach expiration,
the same mechanism will be used to request an updated certificate.

- [[https://github.com/kubernetes/enhancements/issues/267][GITHUB ISSUE: Kubelet Server TLS Certificate Rotation]]

** Container Runtime Interface (CRI)

- [[https://github.com/kubernetes/community/blob/master/contributors/devel/container-runtime-interface.md]]

.image images/cri.png  _ 800
.caption Source: [[https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/]]

.image images/kubelet_cri.png _ 800
.caption Source: [[https://www.youtube.com/watch?v=ZuXQyAj96FA]]

*NOTE*: This image is not totally correct. You need a CRI shim only if your container
runtime is not CRI compatible.

Kubernetes 1.5 introduced an internal plugin API named Container Runtime
Interface (CRI) to provide easy access to different container runtimes. CRI
enables Kubernetes to use a variety of container runtimes without the need to
recompile. In theory, Kubernetes could use any container runtime that implements
CRI to manage pods, containers and container images.

    Abstracting the runtime interface
    One of the innovations taking advantage of this standardization
    is in the area of Kubernetes orchestration. As a big supporter
    of the Kubernetes effort, CoreOS submitted a bunch
    of patches to Kubernetes to add support for communicating
    and running containers via rkt in addition to the upstream
    docker engine. Google and upstream Kubernetes saw that
    adding these patches and possibly adding new container
    runtime interfaces in the future was going to complicate the
    Kubernetes code too much. The upstream Kubernetes team
    decided to implement an API protocol specification called the
    Container Runtime Interface (CRI). Then they would rework
    Kubernetes to call into CRI rather than to the Docker engine,
    so anyone who wants to build a container runtime interface
    could just implement the server side of the CRI and they
    could support Kubernetes. Upstream Kubernetes created a
    large test suite for CRI developers to test against to prove
    they could service Kubernetes.
    Source: open-source-yearbook-2017

    kubelet (grpc client) connects to the CRI shim (grpc server) to perform
    container and image operations.
    Source: https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS158x+1T2018/

CRI implements two services:

- ImageService: responsible for image related operations
- RuntimeService: responsible for pod and container-related operations

Some of this implementations are:

- OBSOLETE - [[https://github.com/kubernetes/kubernetes/tree/master/pkg/kubelet/dockershim][dockershim]]: it interfaces with docker and docker will use containerd in the backend
- OBSOLETE - [[https://github.com/containerd/cri][cri-containerd]]: k8s interfaces directly with containerd (obsolete)
- contained 1.1
- cri-o

.image images/multi-cri.png  _ 800
.caption Source: [[http://blog.realjf.com/archives/210]]

For more information about containerd look at the note page about it.

*** crictl

- [[https://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.md]]

Crictl is a CLI for all CRI container runtimes helpful for application and system
troubleshooting. The objective is to make it easy to use for debug and
development scenarios. It replaces the docker cli for debugging purposes.

For any CRI-compatible container runtime crictl is the reccomended tool.
crictl works consistently across all CRI-compatible container runtimes!

The scope of crictl is limited to troubleshooting.

A containerd namespace mechanism is employed to guarantee that Kubelet and
Docker Engine won’t see or have access to containers and images created
by each other. This makes sure they won’t interfere with each other.
Doker ps can't see k8s pods and k8s can't see docker pods.

*** cri-containerd (end of life)

- [[https://github.com/containerd/cri]]

Since by default containerd (< 1.2) did not support CRI, engineers worked on
creating a CRI interface for containerd. This project allowed a kubernetes
cluster to use containerd as an underlying runtime without using docker.

.image images/cri-containerd-deep.png _ 800
.caption Source: [[https://kubernetes.io/blog/2017/11/containerd-container-runtime-options-kubernetes/]]

cri-containerd is an implementation of CRI for containerd. It operates on the same
node as the kubelet and the container runtime. This process (Daemon) handles all the
requests from the kubelet and uses contained to manage the container lifecycle
in the backend.

Compared with the old docker CRI implementation it eliminates an extra hop in the
stack. cri-containerd manages the pod networking via CNI.

    Let’s use an example to demonstrate how cri-containerd works for the case
    when Kubelet creates a single-container pod:
    1.Kubelet calls cri-containerd, via the CRI runtime service API, to create
    a pod;
    2.cri-containerd uses containerd to create and start a special pause container
    (the sandbox container) and put that container inside the pod’s cgroups and
    namespace (steps omitted for brevity);
    3.cri-containerd configures the pod’s network namespace using CNI;
    4.Kubelet subsequently calls cri-containerd, via the CRI image service API,
    to pull the application container image;
    5.cri-containerd further uses containerd to pull the image if the image is
    not present on the node;
    6.Kubelet then calls cri-containerd, via the CRI runtime service API, to
    create and start the application container inside the pod using the pulled
    container image;
    7.cri-containerd finally calls containerd to create the application container,
    put it inside the pod’s cgroups and namespace, then to start the pod’s new
    application container. After these steps, a pod and its corresponding
    application container is created and running.
    Source: https://kubernetes.io/blog/2017/11/containerd-container-runtime-options-kubernetes/

However, cri-containerd and containerd 1.0 were still 2 different daemons which
interacted via grpc. The extra daemon in the loop made it more complex for users
to understand and deploy, and introduced unnecessary communication overhead.

Works with containerd 1.0.

*** ContainerD plugin (containerd 1.1+)

- [[https://github.com/containerd/cri/blob/release/1.11/docs/config.md]]

While cri-containerd as a step forward from dockershim, now that containerd
implements the CNI interface it's even better.

.image images/containerd-plug.png _ 800

    In containerd 1.1, the cri-containerd daemon is now refactored to be a containerd
    CRI plugin. The CRI plugin is built into containerd 1.1, and enabled by default.
    Unlike cri-containerd, the CRI plugin interacts with containerd through direct
    function calls. This new architecture makes the integration more stable and
    efficient, and eliminates another grpc hop in the stack. Users can now use
    Kubernetes with containerd 1.1 directly
    Source: https://kubernetes.io/blog/2018/05/24/kubernetes-containerd-integration-goes-ga/

The CRI GRPC interface listens on the same socket as the containerd GRPC
interface and runs in the same process.

You can configure to use [[https://katacontainers.io/][kata containers]].

Works with containerd 1.1 and kubernetes 1.10+.

*** CRI-O

- [[http://cri-o.io/]]
- [[https://github.com/kubernetes-sigs/cri-o]]

.image images/cri-o-arch.png _ 800
.caption Source: [[http://cri-o.io/]]

CRI-O works with any Open Container Initiative (OCI) compatible runtime (any one
that is compliant can be plugged in). It supports:

- runC
- ClearContainers

* Kubernetes objects

Kubernetes has a rich _object_model_. These objects represents different
_persistent_entities_ in the cluster.
When you work with k8s you use these api objects to describe the desired
cluster's state.
You can interact with the api via kubectl or the api directly from
inside/outside k8s.

Objects are abstractions that represents the state of your system.
Once an object is set, the control plane works to make sure the actual state
will match the desired state.

- [[https://kubernetes.io/docs/concepts/abstractions/overview/]]

There are some _basic_ k8s objects like:

- Pod [[https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/]]
- Service [[https://kubernetes.io/docs/concepts/services-networking/service/]]
- Volume [[https://kubernetes.io/docs/concepts/storage/volumes/]]
- Namespace [[https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/]]

And _high-level_abstraction_ called *controllers*.
Controllers are build upon the basic objects and provide additional
functionalities and convenient features. Among them:

- ReplicaSet [[https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/]]
- Deployment [[https://kubernetes.io/docs/concepts/workloads/controllers/deployment/]]
- StatefulSet [[https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/]]
- DaemonSet [[https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/]]
- Job [[https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/]]

ReplicaSet add to the Pod the _scaling_ and _healing_ concept.
Deployments add _versioning_ and _rollbacks_.
Services adds a static (non-ephemereal) IP and networking.

** Spec and Status

Inside an object we declare our intent in the spec field. The actual state is
instead recorded by k8s control plane in the status field.
The two nested object fields are:

- object *spec*: This is your configuration defining the *desired* state. We provide it to k8s.
- object *status*: Supplied and updated by the control plane and represent the *actual* state.

k8s reacts to the _difference_ between state and status and tries to match them.

To create an object we provide the spec field to k8s API server together with
the apiVersion, kind and metadata.

The apiVersion specify the API endpoint we want to connect to. With the kind we
specify the object type. With the metadata we attach basic information to objects
like the name.

Once the object is created the control plane adds the status.

** Communication

All communications with the api-servers are in json. When you use kubectl, the
command-line converts the information from yaml to json when making the API
request.

Kubectl calls curl on your behalf. You can see what the kubectl cli transform
the call into by passing a verbosity to the call.

    kubectl --v=10 get pods

The verbosity goes from 0 (default) to 9.

If you want to use curl directly without kubectl, you will need to pass:

- ca (cluster)

and:

- client key (user)/client certificate (user)
- username/password [or] token (mutually exclusive)

otherwise only insecure calls can be made.

All of these can be set with:

    kubectl config set-credentials

** Namespaces

- [[https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/]]
- [[https://kubernetes.io/docs/tasks/administer-cluster/namespaces/]]

Namespaces can be seen as _virtual_clusters_. They provide a scope for names:
names of resources need to be unique within a namespace but not across and allow
to divide the cluster in subsets of resources (*resource*quotas*) (within the namespace).
All objects in the same namespace shares the access control policies.

By default you have three initial namespaces:

- default: Is a catch-all namespace for all objects that do not specify one.
- kube-system: Reserved to objects created by the kubernetes system.
- kube-public: Readable by all users even the non-authenticated. Usually used for special purposes like bootstrapping a cluster or general information.

The namespace is also part of the DNS name that will be created with a service.
When you try to do a dns resolution of a service with just the name, it will
resolve in the _local_ namespace.

    <service>

If you want to cross namespace you will need
to specify it.

    <service>.<namespace>

Few things like nodes and persistenVolumes do not live in namespaces.

*** Resource quotas

- [[https://kubernetes.io/docs/concepts/policy/resource-quotas/]]

    A resource quota, defined by a ResourceQuota object, provides constraints
    that limit aggregate resource consumption per namespace. It can limit the
    quantity of objects that can be created in a namespace by type, as well as
    the total amount of compute resources that may be consumed by resources in
    that project.
    Source: https://kubernetes.io/docs/concepts/policy/resource-quotas/

To limit the amount of resources an user can use in a namespace, Administrators
can create ResourceQuota objects. They allow you to manage:

- Compute Resource Quota: Limit the total sum of resources (CPU and memory)
- Storage Resource Quota: Limit the total sum of storage (total storage, pvc, ephemeral-storage) resources that can be requested
- Object Count Quota: Restrict number of object of a specific type

It allows you to set soft and hard limits.

There is a beta feature (v1.12) field in the quota spec of a pod that allow
a pod to run in a specific priority class by setting the appropriate
priorityClassName ([[https://kubernetes.io/docs/concepts/policy/resource-quotas/#resource-quota-per-priorityclass]]).

    If quota is enabled in a namespace for compute resources like cpu and memory,
    users must specify requests or limits for those values; otherwise, the quota
    system may reject pod creation. Hint: Use the LimitRanger admission controller
    to force defaults for pods that make no compute resource requirements.
    Source: https://kubernetes.io/docs/concepts/policy/resource-quotas/

** Pod

Unit of deployment and logical collection of one or more containers. All
containers in a pod are deployed in parallel on the same node and share:

- Network namespace (IP address and ports)
- Mounts (they mount the same external storage - volumes)
- IPC namespace (inter process communication)
- PID namespace (only if set and not by default) [BETA in 1.13-1.15]

Pods are *ephemeral* as they are not able to self-heal. It's up to _controllers_
to handle pod replication and fault tolerance. You can attach the POD
specification to another object using the Pod Template. Containers inside a Pod
are started in *parallel* and there is no way to know which one will start first.

When a Pod is created, a special _pause_ container (the sandbox container) is
created first and all the containers inside the pod are attached to his network
namespace. The pause container is used to get an IP (and network namespace) and
is also useful not to lose all this if the container needs to be restarted.

For containers to communicate inside a pod they can use the loopback interface,
IPC or a shared filesystem.

NOTE: The number of the *restarts* value is the number of container restarts not
pod restarts.

The possible status phase of a pod are:

- Pending
- Running
- Succeeded
- Failed
- Unknown

Restart policies are:

- Always
- On-failure
- Never

It's valid for all containers in a pod.

*** Sharing PID namespace (BETA in 1.13-1.15)

- [[https://kubernetes.io/docs/tasks/configure-pod-container/share-process-namespace/]]
- PROPOSAL: [[https://github.com/verb/community/blob/master/contributors/design-proposals/node/pod-pid-namespace.md]]

The PID namespace sharing was added later in respect of the others.
However the github issue is very old: [[https://github.com/kubernetes/kubernetes/issues/1615][Shared PID and UTS namespaces ]]

- [[https://github.com/kubernetes/enhancements/issues/495]]
- [[https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/pod-pid-namespace.md]]

*** Sharing UTS namespace

Still not possible to share UTS.

    one is working on configurable UTS namespaces.
    Source: https://github.com/kubernetes/kubernetes/issues/1615

- [[https://github.com/kubernetes/kubernetes/issues/1615]]

*** Resource management for containers in a Pod

- [[https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/]]
- [[https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/resource-qos.md]]

The framework allows you to define the _requirements_ and the _limits_ for each
container inside a Pod. There are two options you can set inside the Pod spec:

- resources.requests: How much does the container need to be allocated
- resources.limits: The upper limit on how much you are allowed to consume

Each of them have three _resource_type_:

- cpu
- memory
- ephemeral-storage (beta in v1.13)

CPU is specified in units of cores while memory is specified in units of bytes.
For CPU the setting 1000m is a vCPU, 250m is 1/4 of CPU and so on.
Memory/ephemeral-storage is measured in _bytes_. 64Mi means 64 megabytes.

    You can express memory as a plain integer or as a fixed-point integer using
    one of these suffixes: E, P, T, G, M, K. You can also use the power-of-two
    equivalents: Ei, Pi, Ti, Gi, Mi, Ki.
    Source: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/

These resources settings are only at container level and not Pod level. The POD
resource requirements are the sum of all the resources of its containers.

*NOTE*: A container *will*not* be killed for *excessive*CPU*usage*.

    If a Container exceeds its memory limit, it might be terminated. If it is
    restartable, the kubelet will restart it, as with any other type of runtime
    failure.
    If a Container exceeds its memory request, it is likely that its Pod will
    be evicted whenever the node runs out of memory.
    A Container might or might not be allowed to exceed its CPU limit for
    extended periods of time. However, it will not be killed for excessive CPU
    usage.
    [...]
    For container-level isolation, if a Container’s writable layer and logs
    usage exceeds its storage limit, the Pod will be evicted. For pod-level
    isolation, if the sum of the local ephemeral storage usage from all
    containers and also the Pod’s emptyDir volumes exceeds the limit, the Pod
    will be evicted.
    Source: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/

If something happens and the container needs to be stopped, info will be written
here: [CHECK THIS]

    terminationMessagePath       <string>
      Optional: Path at which the file to which the container's termination
      message will be written is mounted into the container's filesystem. Message
      written is intended to be brief final status, such as an assertion failure
      message. Will be truncated by the node if greater than 4096 bytes. The
      total message length across all containers will be limited to 12kb.
      Defaults to /dev/termination-log. Cannot be updated.

    terminationMessagePolicy     <string>
      Indicate how the termination message should be populated. File will use the
      contents of terminationMessagePath to populate the container status message
      on both success and failure. FallbackToLogsOnError will use the last chunk
      of container log output if the termination message file is empty and the
      container exited with an error. The log output is limited to 2048 bytes or
      80 lines, whichever is smaller. Defaults to File. Cannot be updated.

*** LimitRange

- [[https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/]]

You can set a default for this values at namespace level with a type of object
called *LimitRange*.

It will allow you to define the default _requests_ and _limits_ for each
container running in a defined namespace.

    apiVersion: v1
    kind: LimitRange
    metadata:
      name: mem-limit-range
    spec:
      limits:
      - default:
          memory: 512Mi
        defaultRequest:
          memory: 256Mi
        type: Container

You can interact with it normally via kubectl.

        kubectl get limitranges -n <your_namespace>

NOTE: These settings will not appear in the Deployment or ReplicaSet definition,
only in the Pod spec.

*** Priority class

- [[https://kubernetes.io/docs/concepts/policy/resource-quotas/#resource-quota-per-priorityclass]]

The PriorityClass will impact the:

- Scheduling order
- Preemption: allows graceful termination period
- Pod disruption budget

You can also set Quotas and assign it to a priority class via a ScopeSelector.

    apiVersion: v1
    kind: List
    items:
    - apiVersion: v1
      kind: ResourceQuota
      metadata:
        name: pods-high
      spec:
        hard:
          cpu: "1000"
          memory: 200Gi
          pods: "10"
        scopeSelector:
          matchExpressions:
          - operator : In
            scopeName: PriorityClass
            values: ["high"]

You need to create a priority object and add the class in the pod definition.

A new option in stable (1.15) added the capability to enable or disable pod
preemption.

    Add a Preempting field to both PodSpec and PriorityClass. Setting the Preempting field in
    PriorityClass provides a straightforward interface, and allows ResourceQuotas to restrict
    preemption.
    Source: https://www.cncf.io/wp-content/uploads/2019/07/CNCF-Webinar_-Kubernetes-1.15.pdf

*** Node selector

nodeSelector work by attaching labels to nodes and then using selectors in the
pod definition. However this choice is binding. If no nodes is available with
that specific flag, the workload won't be allocated.
Plus the nodes will need to have ALL the labels in the pod selectors.

*** Taints and Tolerance (WRITE BETTER)

- [[https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/]]

Nodes can have taints to discourage Pods assignment on that specific node.
They only way for a pod to the scheduled there is with tolerations.

    $ kubectl explain pods.spec.tolerations

if the key is correct and exists but different taint, no effect
if you use exists you don't need a value

    DESCRIPTION: If specified, the pod's tolerations. The pod this Toleration is
    attached to tolerates any taint that matches the triple <key,value,effect>
    using the matching operator <operator>.

With taints you mark a node and only pods that have that tolerance can be
assigned there.

NOTE: The node controller will add specific taint when some conditions occur
on nodes like:

- memory-pressure
- disk-pressure
- out-of-disk

When a node is tainted it won't remove the ones already running on that node.

*** Affinity and Anti-Affinity (BETA)

- [[https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity]]

Node Affinity/Anti-affinity: Steer the affinity towards a node. Is *soft* so it
will if it can. For anti-affinity for nodes you need to use *taints*.
To be surer a certain pod don't go to certain nodes you need to use taint.

Pod Affinity/Anti-affinity: Steer towards/away from another pod.

*** Probes

- [[https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes]]

There are two types of probes:

- Liveness
- Readiness

The *liveness*probe* checks on the application health. If the health check fails
k8s will restart the container. It is defined by:

- Liveness command: a command which return value will define the health
- Liveness HTTP request: and http request (you can define custom headers)
- TCP liveness probe

If it returns failure, the control plane will kill the container and the restart
policy will be applied. If this probe does not exists, the default value will be
success.

The usecase if kill and restart a container if the check fails.

Readiness probe is a check to define when a pod can start accepting requests.
A Pod that do not report ready status will not receive traffic from Kubernetes
Services.

If the readiness probe fails, the pod will be removed from the services to receive
traffic. Default is failure unless is not provided (if it's not provided it's
success).

Readiness is used to create containers that taken themselves down. Send traffic
only if this succeed.

Google in their example have two endpoint in the container:

    /started
    /healthz

** Downward API

- [[https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/]]

Is a type of volume.

This allows you to have all the labels and annotations (?? more ??) accessible
inside the pod. To do so you need to define them and each path you define in the
volume will be mapped in /etc/podinfo/<parh> inside the container.

all the element and annotation of the pod will have to me mounted in a file
called "path"

There are two files mounted on the pod:

- /etc/podinfo/labels
- /etc/podinfo/annotations

There will be lines inside each file and each line will be a label/annotation.

This is implemented via symlink. if you ls /etc/podinfo you can see them. the
files are actually symlink to some files with a generated name. k8s update this
file and change the symlink to update the file informations. this allow to
refresh on the fly.

** ReplicationControllers (OBSOLETE)

- [[https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/]]

Is a controller part of the controller manager and it makes sure that the
specified number of replicas for a Pod is running at any given point in time.
It also had some functionality to do rolling upgrades.

    NOTE: A Deployment that configures a ReplicaSet is now the recommended way
    to set up replication.
    Source: https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/

- RC only supports equality-based selectors.
- RC manages the updates client-side, while the deployments do that server side (CHECK)

Commands like kubectl rolling-update are deprecated and not present on the cli
anymore: [[https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/]]

** ReplicaSet

- [[https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/]]

Is the next generation ReplicationController, which differ only in selector
support (supports both Equality-based and Set-based selectors).
While they could be used independently, they are mostly used by Deployments to
orchestrate the Pod management.

NOTE: If you change the pod definition of a ReplicaSet nothing will happen. Only
when one of the Pod dies the new one (and only this new one) will be the new
version.

The ReplicaSet add the concept of _number_of_replicas_ and _self_healing_ to pods.
Encapsulate a pod template definition inside their definition.
You can also think it gives you scaling but you need to change the number manually.

The ReplicaSet selectors allows to match pods that the ReplicaSet will work it. It allows loose
coupling between the two.

You can delete a ReplicaSet without affecting the pod by using the command:

    kubectl delete --cascade=false

When you adopt pre-existing pods with a new ReplicaSet, this won't be updated to the
new spec! They need to be terminated first.

Changing the label used as a selector from a ReplicaSet of a Pod will remove the Pod
from the control of the controller. Let's say for example that a Pod is not working
and you want to keep it alive for debugging but still restore service, you can
change the label used by the ReplicaSet so that a new one will be create while keeping
the original alive.

    kubectl label pod <pod_id> <selector_key>=<random_value> --overwrite
    # example
    kubectl label pod nginx-7f6cdb7fd7-c8696 app=debug-mode --overwrite

* Autoscalers

- [[https://www.youtube.com/watch?v=Dtr3rR04ekE][Autoscaling and Cost Optimization on Kubernetes: From 0 to 100 - Guy Templeton & Jiaxin Shan]] 4 Sep 2020

** Horizontal Pod Autoscaler (HPA)

- [[https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/]]

HPA can scale Deployments, ReplicSets or ReplicationControllers by default at
50% CPU usage.

- Kubelet checks every 30s
- HPA checks every 30s (?)
- After any action HPA waits 180s before any further action

It uses metrics on the MetricServer.
The autoscaler do not collect the metrics, just act on them. It can scale on:

- Resources (metrics.k8s.io): CPU and Memory (what you see in kube-top)
- Custom metrics (custom.metrics.k8s.io): one is a pod metric (one for each pod of and average it), and object (will expect to find a single value like ingress qps)
- External (external.metrics.k8s.io)

metric server have a way to delegate how to get some metrics to another API
servers. There is a metric server that is the default. There is another server
that is the custom or external metric server adapter.
You can use a prometheus or another server. Need to install the adapter which
will translate the call for your datasource.

.image images/HPA_custom_metrics.png _ 800
.caption Source: [[https://www.youtube.com/watch?v=UZ9NYQ-dpdw][Deep Dive into Autoscaling ]] 4 Sep 2020

Calculating the desired scale is:

    (utilization / target) * scale = desired scale

It then keeps a 5 minutes windows to stabilize so it doesn't flap.

To prevent trashing you can set a downscale delay and a upscale delay.
Define intervals between successive operations of the same kind.

When using HPA make sure to:

- Handle SIGTERM correctly
- Define a readiness and liveness probe
- Enable Cluster autoscaler

You can scale safely on multiple metrics (from 1.15+). It will always take the safest
(highest) choice.
There is an alpha functionality to be able to scale to 0 (in 1.18).

You can tune since 1.18+ how fast to scale it.

.image images/HPA_speed.png
.caption Source: [[https://www.youtube.com/watch?v=Dtr3rR04ekE]]

HPA have two versions: v1 and v2.
HPA v2 (beta in 1.18+) is different than v1.
in v2 is not only 1 top level target in v2 you can provide more than one metric.
it will write the current metric it observer in the status so you can see in HPA
what does it see.

in v2 inside the behaviour fields two structure:

- sacle up
- scale down

relative and absolute policiy to limit the rate.
there is a voice called

    stabilizationWindowSeconds: 300

in v1 that is a flag called --horizontal-pod-autoscaler-downscale-stabolization=5m
but can't change for every HPA.
also you will be able in v2 to change other defaults
in v2 there are new scale-up scale-down controls.

in v2 status there are also new info called _conditions_ which will tell you
what actions where taken and why. For examples:

- ScalingActive
- AbleToScale
- ScalingLimited

all this new fields are backward compatible but is not the best.
in v1 most of this info is translated in annotations.

the Pods value you can decide how much scale to. so you can scale faster or slower.
up or down.

** Vertical Pod Autoscaler (VPA)

- [[https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler]]

Is not part of core k8s like HPA. You need to install it.
It is use to benchmark pods size.
It *recommends* pod sizes and can *apply* them for you.
Monitor real CPU and memory usage based on resource data:

- CPU usage
- Memory usage
- OOM events

If requests are too small you get:

- OOM
- Workload eviction
- CPU starvation

It has three components:

- Recommender: Responsible for calculation of reccomantations based on historical data
- Updater: responsible for evictions of pods which are to have their resource modified
- Admission plugin: A Mutating admission Webhook parsing all pod creationg requests and modifying

It can run in 4 modes:

- Off: Recommendation only: VPA publishes only recommended pod sizes, and doesn't change anything.
- Initial: Initialisation only: VPA resizes pod only when they are created
- Recreate/Auto: VPA resizes pods that already exists

It tracks long-term usage pattern and for applications with relatively flat
usage over lifetime.

You need to create a VPA object and you need to point it to the object you are
monitoring.

Best practices:

- Start with recommendation-only mode
- Use pod disruption budget
- Set minimum and maximum container sizes in VPA object
- From time to time update recommended container sizes and put it in the deployment spec.

Enable cluster autoscaler, keep metrics server healthy.

Don't use both VPA and HPA togethers. Because you don't have to scale them.
Alternatively:

- Use custom metrics for HPA
- Use static number for scaling

Using for singleton or things like prometheus or internal services.

** Cluster Autoscaler (CA)

- [[https://github.com/kubernetes/autoscaler/]]
- [[https://www.youtube.com/watch?v=UZ9NYQ-dpdw][Deep Dive into Autoscaling ]] 4 Sep 2020

.image images/CA_schema.png
.caption Source: [[https://www.youtube.com/watch?v=UZ9NYQ-dpdw][Deep Dive into Autoscaling ]] 4 Sep 2020

Provides nodes so that every pod in cluster can schedule.
Compacts and removed underutilized nodes.
Based on scheduling simulations and declared pod requests, NOT metrics.
CA operates on NodeGroups - resizable sets of identical nodes
NodeGroup is implemented differently by each provider.

Do to the simulation, the CA uses the same scheduler code and try to see
for example "will adding a node of this allow me to schedule something now is
not schedulable"? That is the reason why is always important to use the right
version of the CA for the cluster! they need to be in synch.
It also containes a cloud provider to actuate it's decisions.

When the label called mark them as as unschedulable status condition for pods
as unschedulable that is when the autoscaler activates.

expansion option is what they call a single actions that would solve the problem.
if one is successful it will still evaluate other options while remember the
first.

cluster autoscaler does not consider mixups (multispe asg).

This component add or removes nodes from the cluster. The decisions are based
on:

- Inability to deploy a Pod (scale up)
- Having nodes with low utilisation for 10m (scale down)

Commands are in the form of _cluster-autoscaler-_.

It is based on simulation on scheduling and declared pod requests and not on
real data.
Scales up are triggers by pending pods.
Scale down is evaluated from node utilising resource below a certain threshold.
It then evaluate whether the pods currently running on the node can be rescheduled
on other nodes in the cluster. If so, it threat the node as scale down candidate
and waits _scale-down-unneeded-time_ and then drain and remove the node from
the cluster.

There are addon resizer, with which CA support different way to decide how to
scale up/the expanders is what decides which option to choose.
there are many:

- Random (default)
- Priority
- Price (only GKE/GCP)
- Least waste: picks the candidate node group with the least wasted CPU after scale up

or you can implement your own in go.

Best practices:

- Pick the version that matches the cluster version (as stated above)
- Define Pod Disruption budgets
- Container lifecycle PreStop can be used to manage the termination
- Setting pod priorities

By default it aims at having cluster utilisation above scale down threshold of
50%.

Some cost optimisation advices:

- You can use the flag --expendable-pods-priority-cutoff to avoid to scale for low priority jobs
- You can scale on demand only if Spot is not available using "expansion priority" (it will wait max-node-provision-time)

.image images/CA_cost.png
.caption Source: https://www.youtube.com/watch?v=Dtr3rR04ekE

In AWS you can use the Mixed Instance Policy to have diversification in the spot
instance types you use. The instance types should have the same amount of
ram and cpu. To help the prevision (simulation) of the CA.

You can protect a node from being terminated by the CA with the flag:

cluter-autoscaler.kubernetes.io/safe-to-evict=false

You can use these on nodes with critical jobs.
If you want to overscale create dummy pods with low priority so that they can
be evicted to make space for the high priority ones.

Use resource quotas in each namespace to avoid it to scale forever.
Also you can use the maximum size on the ASG definition.
PodDisruptionBudgets helps to avoid outages.

** Other options

- Addon Resizer: less sophisticated VPA
- Cluster Proportional Autoscaler: Usually used to scale DNS replica set
- KEDA

KEDA Kubernetes event driven autoscaling makes use of HPA under the hood to allow
event driven autoscaling of workloads from metrics from a variety of resources
(CNCF Sandbox)

* Objects

** DaemonSets

- [[https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/]]
- [[https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/]]

Will run a pod (and only one) at all times on each node of the cluster.
From v1.12 the scheduler handles DaemonSets deployments so you can use label
selector to restrict which nodes it will run on.
When a node needs to be remove from the cluster the controller will cleanup
after the DeamonSet.

Like Deployments they have a concept of history and deployment strategy.
Rollout works in the same way.

The only difference is the type of strategy you can choose. Valid values of
daemonset.spec.updateStrategy are:

- OnDelete (default): Will wait for the original pod to die before applying the change
- RollingUpdate: Same as Deployments

Another difference is that the only option of
_.spec.updateStrategy.rollingUpdate_ available is maxUnavailable (default 1).
They won't allow you to run a new one before deleting the old one.

** StatefulSets

The StatefulSet controller provides identity and guaranteed ordering of
deployment and scaling to Pods.
Is used to manage applications that requires an identity like name, network,
strict ordering, etc.

To track pods as unique, they get an identity via an ordinal (number), stable
storage and stable network identity. The identity remains with the node no matter
where the pod is running. These pods are not interchangeable.

They have an _identifier_ that maintains across any rescheduling.
The scheme is sequential starting with 0.
Numbers from 0 to n-1 so they require a headless service.
They will be deleted from n-1 to 0.

- foo-0
- foo-1
- foo-2

Ordered deployment, scaling, deletion, upgrade and termination. They are useful
with apps that needs persistent storage. A following Pod will not launch until
the current one is in ready state. They are NOT deployed in parallel.

Storage MUST be persistent.
Deleting a statefuset won't delete the volume.

** Deployments

- [[https://kubernetes.io/docs/concepts/workloads/controllers/deployment/]]

Deployments provide declarative updates to Pods via ReplicaSets (it manages Pods
via ReplicaSets). The DeploymentController is part of the master node controller
manager.

Modifying the "Pod template" of a Deployment object will trigger a
*Deployment*rollout*, while operations like scaling the deployment do not
trigger this type of event.

Changes like these will create a new revision of the deployment and they will
allow you to revert it via a *rollback* operation. However when you roll back
is the pod template part that is rolled back, not other options like
the number of pods.

    kubectl rollout undo deployment <deployment_name>
    # or
    kubectl rollout undo deployment <deployment_name> --to-revision=2

Other advanced functionality of the deployments are:

- Autoscaling (via HPA)
- Pausing/resuming (for example to fix bugs)
- Rolling deployments with proportional scaling (via maxSurge/maxUnavailable)
- Versioning
- Rollback

NOTE: If you tell a new deployment to point with labels to an existing rs and
pods it will take control of the rs and will be able to manage it (scaling).
When you change the pod definition a new rs will be created with the correct
naming convention this time.

In API version apps/v1, a Deployment’s label selector is immutable after
it gets created.

NOTE: BEWARE. You don't want the same pod to be managed by two deployments and
there is no k8s check to tell you that you are doing it.

At the moment (v1.13) there are only two _strategy_ for updates set via the
spec.strategy field:

    # For more info
    kubectl explain deployments.spec.strategy

- RollingUpdate (default): You can set _maxUnavailable_ and _maxSurge_ under .spec.strategy.rollingUpdate
- Recreate: All pods will be killed before creating the new ones

Other options:

- minReadySeconds: Minimum number of seconds for which a newly created pod should be ready without any of its container crashing, for it to be considered available.
- progressDeadlineSeconds: The maximum time in seconds for a deployment to make progress before it is considered to be failed.
- replicas (default to 1)
- revisionHistoryLimit (default to 10)

When interacting directly with kubectl, you can use _--scale_ to scale the num of
Pods in a Deployment.

*** Pause/Resume

The field spec.paused "Indicates that the deployment is paused and will not be
processed by the deployment controller.". This means that any change applied to
the Pod spec do not cause a new deployment change.
This can be managed directly by changing the field or using:

    kubectl rollout pause deployment <deployment_name>
    # and
    kubectl rollout resume deployment <deployment_name>

*** Change cause history

You could (BUT DO NOT) use _--record_ to record the reason for change of a deployment.
However, the flag is not supported everywhere (in run but not in create) and
is generally a bad idea. [[https://github.com/kubernetes/kubernetes/issues/40422]]
If you want to show a message in rollout history, you can change the metadata.annotation:

    kubernetes.io/change-cause: <your comment>

That is what --record does.

    kubectl rollout history deployment <deployment_name>

You can always do the change and then:

    kubectl annotate deployments <deployment_name> kubernetes.io/change-cause='<Your description'

** Job & CronJob

- [[https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/]]

Part of the batch API group.
A job creates one or more Pods to perform a task. If a pod fails it will be
restarted. Once the task is over all the pods are terminated. Is a one-job
functionality.

There are three types of jobs:

- Non-parallel jobs: force one pod to run successfully
- Parallel jobs with fixed competition count: job completes when x amount of pods complete successsfully.
- Parallel jobs with work queue: required coordination

After completion the state is set to terminated the pods are not delete
(but not shown in get pods either). To show them you need to do:

    kubectl show pods -a
    # or
    kubectl get pods --show-all

You can set a deadline with the flag activeDeadlineSeconds field.

- [[https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/]]
- [[https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/]]

A cron job is similar with the difference that the Job will run always at a
specific time. Since there could be reason why a pod could not be run when needed
or run twice, Jobs should be idempotent (at least once).

There is one option:

    .spec.concurrencyPolicy

that defines how to handle existing jobs. There are three options:

- Allow (default): another concurrent job can be run
- Forbid: the current job continues and the new one will be skipped
- Replace: cancels current job and start a new one in it's place

** Service

- [[https://kubernetes.io/docs/concepts/services-networking/service/]]

Pods are ephemeral and resources like IP addresses allocated to them cannot be
static. To solve this problem, instead of accessing the pods directly a
construct called *service* is used.

    $ kubectl get svc --all-namespaces
    NAMESPACE     NAME                   TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE
    default       kubernetes             ClusterIP   10.96.0.1      <none>        443/TCP         112s
    kube-system   default-http-backend   NodePort    10.108.60.44   <none>        80:30001/TCP    100s
    kube-system   kube-dns               ClusterIP   10.96.0.10     <none>        53/UDP,53/TCP   108s

targetPort defaults to port, but it could be set to any value, including a string
referring to a port on the backend pod. The port number can change in the pod
definition during a release and this won't cause downtime if the name is the same.

    TIP: Use named port

A service groups pods using Labels and Selectors and load-balance traffic access
to them. The service lifespan is decoupled from the pods (loose coupling).
You can define the port the service is going to listen to and the _targetPort_
(the port the container is listening to). A tuple of Pods IP addresses along
with the _targetPort_ is referred to a service *endpoint*.

    $ kubectl get endpoints --all-namespaces
    NAMESPACE     NAME                      ENDPOINTS             AGE
    default       kubernetes                192.168.39.107:8443   26s
    kube-system   default-http-backend      172.17.0.4:8080       16s
    kube-system   kube-controller-manager   <none>                24s
    kube-system   kube-dns                  <none>                18s
    kube-system   kube-scheduler            <none>                24s
    kube-system   kubernetes-dashboard      172.17.0.3:9090       16s

Each service is associated with an *endpoint*object*. It is a dynamic list of all
the pods that are selected by a service label selector at any point in time.
An IP is attached to the service in a different CIDR of the Pods and it's called
ClusterIP.
The CIDR for services can be set at API server _startup_option_.
In kubeadm you can select the pods IP range with *--pod-network-cidr* and the
services ip range with *--service-cidr*.

_NOTE_: Google has been working on Extensible Service Proxy (ESP) based on nginx.
This is more flexible than endpoint but is not used outside
[[https://cloud.google.com/endpoints/docs/frameworks/frameworks-extensible-service-proxy][Google App Engine]]
and GKE.

Services can be exposed internally or externally. They ca also connect internal
resources to external ones. Services can be of the following type:

- *ClusterIP* (default): Static lifetime IP only accessible from within the cluster
- *NodePort*: The service is exposed on each node on a static port.
- *LoadBalancer* (only cloud): Only works in a cloud env and will create a lb on the cloud provider, open a random port on every node and accept traffic
- *ExternalName*: Map service to an external endpoint residing outside the cluster.

*NodePort* is in the range 30000-32767 (defined at cluster configuration). Used
for debugging or when a static ip address is necessary (like firewalling
whitelist).

*ClusterIP* is a way to associate a *Virtual*IP* (VIP) to a service. The kube-proxy
will manage the traffic. They represent a standard stable network overview of
the system.

- [[https://kubernetes.io/docs/tutorials/stateless-application/expose-external-ip-address/]]

You can define externalIPs in the spec of any type of service. If incoming network
traffic has this IP has the destination and arrives to one of the service ports
then it will be routed to one of the service endpoint.

*ExternalName* has no selector and define no port or endpoint. It returns an
alias (CNAME) of an external service. redirection happens at DNS level not via
proxy or forward. This can be useful to connect internal services to apps
still not moved into kubernetes.

NodePort created a ClusterIP anyway.
LoadBalancer will create a ClusterIP AND open a random node port to attach to the LB.

NOTE: Giving the _port_ a name is very useful because it decouples pod and service
definition. You will be able to change the port you are listening to in the pod
without changing the service if you keep the same name!

    While there is no configuration for this option, there is the possibility
    of session affinity via IP.
    Source: Linux foundation course

A service for which Kubernetes does not provide a ClusterIP is called
[[https://kubernetes.io/docs/concepts/services-networking/service/#headless-services][headless]].

A service can expose multiple ports, but in this case they need to be named.

*** Endpoints object limits and EndpointSlices () - BETA 1.17+

- [[https://kubernetes.io/blog/2020/09/02/scaling-kubernetes-networking-with-endpointslices/]]
- [[https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/]]

    With the Endpoints API, there was only one Endpoints resource for a Service.
    That meant that it needed to be able to store IP addresses and ports (network
    endpoints) for every Pod that was backing the corresponding Service. This
    resulted in huge API resources. To compound this problem, kube-proxy was
    running on every node and watching for any updates to Endpoints resources.
    If even a single network endpoint changed in an Endpoints resource, the whole
    object would have to be sent to each of those instances of kube-proxy.
    A further limitation of the Endpoints API is that it limits the number of
    network endpoints that can be tracked for a Service. The default size limit
    for an object stored in ETCD is 1.5MB. In some cases that can limit an
    Endpoints resource to 5,000 Pod IPs. This is not an issue for most users,
    but it becomes a significant problem for users with Services approaching
    this size.
    Source: https://kubernetes.io/blog/2020/09/02/scaling-kubernetes-networking-with-endpointslices/

The solution to the above problem is to slice/shard the endpoint in subsets
so that you can update this subsets individually and have better performances.
You can also build on this functionality to have smarter routing.

- [[https://kubernetes.io/docs/concepts/services-networking/service-topology/]]

*** Service LoadBalancer finalizer - ALPHA 1.15

- [[https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#garbage-collecting-load-balancers]]

Is a protection to ensure that the service resource is not fully deleted until
the load balancer resource is deleted. Any service of type load balancer will be
attached a finalize, which should be removed by service controller upon cleanup.

*** Service discovery (DNS)

- [[https://github.com/coredns/coredns]]
- [[https://coredns.io/plugins/]]

k8s supports two methods of discovering a service:

- DNS (recommended): The DNS add-on will create a DNS record for each Service
- Environment variable: As soon as the pods start, env variable with all the existing services are added. They are *not*updated* after pod creation.

DNS is provided by [[https://github.com/coredns/coredns][CoreDNS]] from version
v1.13. It is much more flexyble than is predecessor and allow extensibility
via [[https://coredns.io/plugins/][plugins]] (~30 only by default).

The DNS name is in the form of:

    <servicename>.<namespace>.svc.cluster.local

In the same namespace services can be referred by the service name only. If you
are on a different namespace you need to define the namespace too.
For headless services, a DNS resolution will resolve all the pods ip (endpoint).

You can define the hostname and subdomain of your service in the Pod
definition. [[https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/]]

    apiVersion: v1
    kind: Pod
    metadata:
      name: busybox2
      labels:
        name: busybox
    spec:
      hostname: busybox-2
      subdomain: default-subdomain
      containers:
      - image: busybox
        command:
          - sleep
          - "3600"
        name: busybox

The pod DNS policy (how the pod access the world) can be:

- Default: Inherit the node settings
- ClusterFirst: If the dns does not conform to the cluster domain suffix, it will be resolved with the upstream name server inherited from the node
- ClusterFirstWithHostNet: (only with host network = true) App can see the NIC of the host machine
- None: Ignore DNS setting of the kubernetes env

The DNS add-on also support SRV queries. A DNS SRV will be created for all
named ports of a service.

    <portname>.<portprotocol>.<servicename>.<namespace>.svc.cluster.local

Given port name, the port number is returned.

** Ingress

- [[https://kubernetes.io/docs/concepts/services-networking/ingress/]]

    "An Ingress is a collection of rules that allow inbound connections to reach
    the cluster Services."
    Source: https://kubernetes.io

While _services_ manage the routing to access the pods, the *Ingress* allows you
to decouple routing rules to access the service from the application.
It allows *inbound* connection to reach the cluster _service_ and is implemented
via a Layer 7 HTTP load balancer called *ingress*controller*.
With ingress you don't connect directly to a Service. When you reach it, the
request is forwarded to the respective service.
You could manage the external access with services (NodePort, LoadBalancer, etc)
but the Ingress is an abstraction/alternative for this process.

Some of the functionality the Ingress add there are:

- TLS
- Name-based virtual hosting: Uses domain name to route the traffic
- Path-based routing: Fan out ingress rule
- Custom rules

An *ingress*controller* is an application that watches the API for changes in the
Ingress resources and updates the layer 7 load balancer. An example of
implementation is the
[[https://github.com/kubernetes/ingress-nginx/blob/master/README.md][nginx ingress controller]]

** Volume

A volume can be seen as a directory backed by a storage medium defined by the
_Volume_type_. A volume is attached to a pod and is shared by all the
containers in the pod. It has the same lifespan of the pod and outlives the
containers (survives containers restart).

They address the problem of:

- Permanence: Storage that lasts longer than lifetime
- Shared state: Multiple pod share state/files

Volumes in general survive single container restarts but if you want also to
survive a pod restart, you need to use a persistenVolumes that is decoupled
for the pod lifecycle.

Some of the volume types are:

- emptyDir (default): non-persistant: local to the pod. if the pod dies it's lost. survive container restart.
- hostPath: mount a dir on the node. is uncommon because pods should be independent from the node: usecase cAdvisor or similar daemon
- nfs
- iscsi
- persistentVolumeClaim
- configmap
- gitRepo
- secret

Some cloud version:

- gcePersistentDisk (gcp)
- awsElasticBlockStore (aws)
- azureDisk (azure)
- azureFIle (azure)

Each volume needs to be explicitly mounted and defined in the pod. It will
appear as a directory or file on the filesystem.

*** PersistentVolume & PersistentVolumeClaim

PV api provides a way for user or administrator to manage and consume storage.
To manage the volume it uses the PersistenVolume API.
To consume it uses the PersistentVolumeClaim API.

There are two types of provisioning:

- Static: preallocated by the admin
- Dynamic: via storage class

A PV is a network attached storage provisioned beforehand by an admin or user
and defined as a pool to k8s.
PV _can_ be dynamically provisioned based on the *storage*class* resource. A
storage class contains pre-defined provisioner and parameters to create a
persistent volume. Using PersistentVolumeClaims, a user sends the request for
dynamic PV creation, which gets wired to the StorageClass resource.

    A PersistentVolumeClaim (PVC) is a request for storage by a user. Users request
    for PersistentVolume resources based on size, access modes, etc. Once a suitable
    PersistentVolume is found, it is bound to a PersistentVolumeClaim.
    Source: https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS158x+1T2018/

While creating a PersistentVolumeClaim, a user can specify the size and access
mode.

** Storage class

You can't change the reclaimPolicy of a storage class after it's creation:

    $ kubectl patch storageclass gp2 -p '{"reclaimPolicy":"Retain"}'
    The StorageClass "gp2" is invalid: reclaimPolicy: Forbidden: updates to
    reclaimPolicy are forbidden.

Create instead a new gp2 class and make it the new default.

- [[https://kubernetes.io/docs/tasks/administer-cluster/change-default-storage-class/]]

You can change the reclaimPolicy of an existing volume however:

- [[https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims]]
- [[https://kubernetes.io/docs/tasks/administer-cluster/change-pv-reclaim-policy/]]

You can set a storage class to create a PV only after the pod has been scheduled
with the option *volumeBindingMode* set to *WaitForFirstConsumer* instead of
_immediate_.

- [[https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode]]

*** CSI

CSI was created to standardise the Volume interface; a volume plugin built using
a standardized CSI would work on different container orchestrator.

Kubernetes 1.9 added alpha support for CSI
With CSI, third-party storage providers can develop solutions without the need
to add them into the core Kubernetes codebase.

** Config map

-[[https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/]]

Decouples the configuration details from the container image. It can be consumed
by a Pod or a controller.

Can be created from:

- literal values
- files

    kubectl create configmap my-config --from-literal=key1=value1 --from-literal=key2=value2

with 'file' here we are referring to the object yaml.

This information can be accessed in two ways.
From:

- environment
- volume: the config map can be mounted as volume and for each  key we will have a file

** Secrets

- [[https://kubernetes.io/docs/concepts/configuration/secret/]]

    With Secrets, we can share sensitive information like passwords, tokens, or
    keys in the form of key-value pairs, similar to ConfigMaps; thus, we can
    control how the information in a Secret is used, reducing the risk for
    accidental exposures. In Deployments or other system components, the Secret
    object is referenced, without exposing its content.
    It is important to keep in mind that the Secret data is stored as plain
    text inside etcd. Administrators must limit the access to the API server
    and etcd.
    Source: https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS158x+1T2018/

From literal:

    kubectl create secret generic my-password --from-literal=password=mysqlpassword

By default it will be of type opaque (in base64) if you try to read it with
describe. To read it just run base64 --decode.

NOTE: base64 encoding does not do any encryption!

You can create a secret form file just base64 it before.

* Authentication, Authorization and Admission Control

Each request goes through the following three stages:

- Authentication: Logs in as a user
- Authorization: Authorizes the API request added by the logged in user
- Admission control: Some additional module that can modify or reject the request based on additional checks.

More on admission controllers here:

- [[http://go-talks.appspot.com/github.com/jecnua/notes-presentations/notes/containers/orchestration/k8s/03-admission_controller.article][notes/containers/orchestration/k8s/03-admission_controller]]

** Checking access

- [[https://kubernetes.io/docs/reference/access-authn-authz/authorization/#checking-api-access]]

There are way to check a user authorization.

    kubectl auth can-i create pod
    kubectl auth can-i create pod --as jecnua
    kubectl auth can-i create pod --as jecnua --namespace dev

This can be done through three apis:

- SelfSubjectAccessReview
- LocalSubjectAccessReview
- SelfSubjectRulesReview

** Lyfecycle hook

- [[https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/]]

Containers in a pod can be aware on which steps they are of they life-cycle.
There are two life-cycle hooks:

- PostStart
- PreStop

PostStart: Immediately after the container is created. No guaranteed it will be
run BEFORE the entrypoint. No parameters can be passed.

PreStop: Run immediately before the container terminates. Is *BLOCKING*. No
parameters can be passed.

They can be used by registering *hook*handlers*. They are:

- Exec
- HTTP

Hook delivery is *at*least*once* so you need to manage idempotency inside the
function.

** Init containers

- [[https://kubernetes.io/docs/concepts/workloads/pods/init-containers/]]

Run before the app containers starts.
Always run-to-completion
Run serially and only when the one before them returns successfully (if not it
will continuously restart the pod until it succeed)

It is defined in the pod spec.

They have a different namespace/isolation form the app containers. Among the usages:

- include utilities or setup
- block or delay the start of an app container

* Authentication

- [[https://kubernetes.io/docs/reference/access-authn-authz/authentication/#authentication-strategies]]

While there is no _user_ object, k8s have two kind of user:

- Normal User: Managed outside of kubernetes via independent services (like certificates)
- Service Accounts: In-cluster processes communicate with the API. They are usually created via API. They are tied to a Namespace and mount credentials as secrets.

In some cases also anonymous requests are allowed.

K8s have different authenticator modules. You can enable multiple and the first
one to successfully authenticate the request short circuit the others.

The most commonly used are probably:

- Client certificates
- Service account tokens: Uses signed bearer token to verify the requests.
- OpenID account tokens: Allow to connect to OAuth 2 providers

    Client Certificates
    To enable client certificate authentication, we need to reference a file
    containing one or more certificate authorities by passing the
    --client-ca-file=SOMEFILE option to the API server. The certificate
    authorities mentioned in the file would validate the client certificates
    presented to the API server.

Service account token get attached to Pods using the ServiceAccount Admission
Controller, which allows in-cluster processes to talk to the API server (with
restricted power). In every container they are mounted under
_ /var/run/secrets/kubernetes.io/serviceaccount/_:

    # ls -la /var/run/secrets/kubernetes.io/serviceaccount/
    total 4
    drwxrwxrwt    3 root     root           140 Dec 27 14:39 .
    drwxr-xr-x    3 root     root          4096 Dec 27 14:39 ..
    drwxr-xr-x    2 root     root           100 Dec 27 14:39 ..2018_12_27_14_39_28.154744863
    lrwxrwxrwx    1 root     root            31 Dec 27 14:39 ..data -> ..2018_12_27_14_39_28.154744863
    lrwxrwxrwx    1 root     root            13 Dec 27 14:39 ca.crt -> ..data/ca.crt
    lrwxrwxrwx    1 root     root            16 Dec 27 14:39 namespace -> ..data/namespace
    lrwxrwxrwx    1 root     root            12 Dec 27 14:39 token -> ..data/token

Admission controllers can be used to enforce resource quotas.

* Authorization

All API requests needs to be authorized. TO do so the request is validated by
multiple authorization modules. It the evaluation is successful the request
will be allowed. If not will be denied. All the authorizer will be run and as
soon as one fails the request return immediately as failed.
To authorized the call all need to pass.

Among the modules, the most common will be:

- Role-Based Access Control (RBAC) Authorizer

With RBAC access to resources can be regulated based on the role of users. When
a role is created, access to a resource can be restricted to operations (verbs).
There are two kinds of role:

- *Role*: Grant access to specific namespace
- *ClusterRole*: Grant permission cluster wide

Once the role is created it can be _bound_. There are two types of binding.

- *RoleBinding*: Bind user at the same namespace of the Role.
- *ClusterRoleBinding*: Grant access to resources in ALL namespaces

This authorizer is enabled by passing --authorization-mode=RBAC to the api
server.

You can list both type of roles in this way:

    kubectl get clusterrolebindings.rbac.authorization.k8s.io
    kubectl get roles.rbac.authorization.k8s.io -A

* ETCD

ETCD is a key-value store based on the Raft Consensus Algorithm. Nodes in the
cluster can have one of two roles: master or follower.
At any given time one of the nodes will be the master while the others are
followers.

ETCD is implemented as a b+tree key-value store. You don't find and replace an
entry, instead you always append new data to the end. Old data is marked for
removal and will be garbage collected (compaction process) later.

In k8s ETCD store the cluster state but also configuration details like
ConfigMaps and Secrets.

* Kubectl command-line quick tricks

    kubectl create deployment <name> --image <image_name> --dry-run
    #
    kubectl get deployment <name> --export -o yaml # export will remove the unique parameters

* Node maintenance

In order to work on a node to do maintenance, you need to move away all the
workload running at the moment on that node and make sure no new Pod is
allocated on it. This process is in two steps.

First you *cordon* the node. Cordoning a node will stop pods from being
scheduled on it. Before:

    $ kubectl describe nodes | grep -i taint
    Taints:             node-role.kubernetes.io/master:NoSchedule
    Taints:             <none>

Now cordon it:

    $ kubectl cordon ip-x-x-x-x.eu-west-1.compute.internal
    node/ip-x-x-x-x.eu-west-1.compute.internal cordoned

And you can see a new taint is now associated with the node:

    $ kubectl describe nodes | grep -i taint
    Taints:             node-role.kubernetes.io/master:NoSchedule
    Taints:             node.kubernetes.io/unschedulable:NoSchedule

Now you can *drain* it. Draining a node will first cordon it, but I put it in
two different steps so to be clear.

*NOTE*1*: As of v1.13 deamonset still can't be drained. If you run the simple
drain command you will receive an error and a new flag needs to be passed.

    $ kubectl drain --ignore-daemonsets ip-x-x-x-x.eu-west-1.compute.internal
    node/ip-x-x-x-x.eu-west-1.compute.internal already cordoned
    [...]

*NOTE*2*: Drain will also fail if you have pods with local storage. You need to
pass a new flag to force through.

    $ kubectl drain --ignore-daemonsets --delete-local-data ip-x-x-x-x.eu-west-1.compute.internal

Is very important to know that the _drain_ command will move all the Pods away
EVEN if there is no node able to receive it (they will stay pending). Make sure
you have enough capacity to host all the Pods on the node you want to work on
before draining it.

* Re/Sources (not already referenced directly)

- [[https://www.youtube.com/watch?v=UZ9NYQ-dpdw][Deep Dive into Autoscaling ]] 4 Sep 2020
- AMAZING VIDEO - [[https://www.youtube.com/watch?v=2dsCwp_j0yQ][YOUTUBE: Kubernetes the very hard way at Datadog]] - 24 Oct 2018
- [[https://kubernetes.io/blog/2018/05/24/kubernetes-containerd-integration-goes-ga/]] - May 2018
- [[https://kubernetes.io/blog/2017/11/containerd-container-runtime-options-kubernetes/]] - Nov 2017
- [[https://linuxacademy.com/linux/training/course/name/certified-kubernetes-administrator-preparation-course]]
- [[https://www.safaribooksonline.com/videos/oscon-2017/][Kubernetes hands-on - Kelsey Hightower (Google) - Part 1]]
