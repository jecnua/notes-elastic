A/ELB and 408 errors

* The problem

- [[https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/config-conn-drain.html][AMAZON FORUM: Basic EB environment suffers HTTP 408 pollution in httpd log]]
- [[https://forums.aws.amazon.com/thread.jspa?messageID=307846][ELB causing HTTP 408 errors]]
- [[https://forums.aws.amazon.com/message.jspa?messageID=687765][Basic EB environment suffers HTTP 408 pollution in httpd log]]
- [[https://serverfault.com/questions/485063/getting-408-errors-on-our-logs-with-no-request-or-user-agent][serverfault.com: Getting 408 errors on our logs with no request or user agent]]

You may find your apache log full of lines like this:

    x.x.x.x - - [07/Apr/2015:12:02:32 +0000] "-" 408 - "-" "-"

* What is error 408

- [[http://www.checkupdown.com/status/E408.html][http://www.checkupdown.com/status/E408.html]]

    The Web server (running the Web site) thinks that there has been too long
    an interval of time between 1) the establishment of an IP connection
    (socket) between the client (e.g. your Web browser or our CheckUpDown
    robot) and the server and 2) the receipt of any data on that socket, so the
    server has dropped the connection. The socket connection has actually been
    lost - the Web server has 'timed out' on that particular socket connection.
    The request from the client must be repeated - in a timely manner.

A 408 represents a connection is made but no request is sent in the appropriate
time scale, therefore the server drops the connection. Something is connecting
to the port and then never sending data!

** Is it an misconfiguration or an attack?

- [[https://www.trustwave.com/Resources/SpiderLabs-Blog/(Updated)-ModSecurity-Advanced-Topic-of-the-Week--Mitigating-Slow-HTTP-DoS-Attacks/][ModSecurity-Advanced-Topic-of-the-Week--Mitigating-Slow-HTTP-DoS-Attacks]]

408 errors can be generated in a variety of cases where your server is being
scanned for exploits.

** Other pages speaking about the topic

- [[http://georgebohnisch.com/fix-elastic-load-balancer-408-errors-elastic-beanstalk/][http://georgebohnisch.com/fix-elastic-load-balancer-408-errors-elastic-beanstalk/]]
- [[https://disloops.com/apache-logs-in-aws/][https://disloops.com/apache-logs-in-aws/]]
- [[https://forums.aws.amazon.com/message.jspa?messageID=445272][ELB Health Check causing 408 errors to be logged repeatedly]]

** On AWS: Is it the connection draining?

No. But here some resources worth reading.

- [[https://aws.amazon.com/blogs/aws/elb-connection-draining-remove-instances-from-service-with-care/][BLOG: ELB Connection Draining – Remove Instances From Service With Care]]

    When Connection Draining is enabled and configured, the process of
    deregistering an instance from an Elastic Load Balancer gains an additional
    step. For the duration of the configured timeout, the load balancer will
    allow existing, in-flight requests made to an instance to complete, but it
    will not send any new requests to the instance. During this time, the API
    will report the status of the instance as InService, along with a message
    stating that “Instance deregistration currently in progress.” Once the
    timeout is reached, any remaining connections will be forcibly closed.

- [[https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/config-conn-drain.html][DOCS: config-conn-drain.html]]

    When you enable connection draining, you can specify a maximum time for the
    load balancer to keep connections alive before reporting the instance as
    de-registered. The maximum timeout value can be set between 1 and 3,600
    seconds (the default is 300 seconds). When the maximum time limit is
    reached, the load balancer forcibly closes connections to the
    de-registering instance.

- [[http://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#deregistration-delay][http://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#deregistration-delay]]

From the docs:

    Elastic Load Balancing stops sending requests to targets that are
    deregistering. By default, Elastic
    Load Balancing waits 300 seconds before completing the deregistration
    process, which can help in-
    flight requests to the target to complete. To change the amount of time
    that Elastic Load Balancing
    waits, update the deregistration delay value. Note that you can specify a
    value of up to 1 hour, and that
    Elastic Load Balancing waits the full amount of time specified, regardless
    of whether there are in-flight
    requests.
    If a deregistering target terminates the connection before the
    deregistration delay elapses, the client
    receives a 500-level error response.
    The initial state of a deregistering target is
    draining. After the deregistration delay elapses, the
    deregistration process completes and the state of the target is
    unused. If the target is part of an Auto
    Scaling group, it can be terminated and replaced. However, connections
    between load balancer nodes
    and a deregistering target are kept for up to one hour if there are
        in-flight requests.

*Default*: 5 minutes

* Solutions

- [[https://serverfault.com/a/511958][https://serverfault.com/a/511958]]
- [[https://collectiveidea.com/blog/archives/2013/01/11/running-haproxy-behind-amazons-elb][https://collectiveidea.com/blog/archives/2013/01/11/running-haproxy-behind-amazons-elb]]

    It turns out that the monitoring side of ELB likes to open up
    a couple of sockets to the backend servers without sending
    any data down those sockets. These sockets have a timeout of
    60s and reconnect immediately upon closing.

The problem is actually connected to the elb *Idle*timeout* setting:

- [[https://stackoverflow.com/questions/28424808/is-elb-draining-tcp-based#comment45213462_28435501][https://stackoverflow.com/questions/28424808/is-elb-draining-tcp-based#comment45213462_28435501]]

    By default ELB keeps connections open for 60 secs to your clients and to
    the backend. You can change that value at anytime and specify any value
    between 1 and 3600 seconds. If you are specifying a keep alive on the
    backend, and you want the load balancer to be responsible for closing the
    connection, you have to configure the ELB idle timeout with a lower value
    than your keep alive

Or:

    It essentially hinges on ensuring that your httpd server's timeout settings
    are all greater than your ELB idle timeout setting (which defaults to 60
    seconds). Ensure that your apache Timeout directive is double than the idle
    timeout setting of your ELB.
    - Ensure that your apache Timeout directive is double than the idle timeout
    setting of your ELB.
    - Turn on the KeepAlive feature, ensure that MaxKeepAliveRequests is very
    large (either 0 for infinite or very high, like 2000), and that
    KeepAliveTimeout is greater than your ELB's idle timeout.

Good way to put it in this comment:

    [...] I believe what is going on is simply that ELB has a feature where it
    attempts to keep extra idle connections to the host: that way there is
    always a connection ready to return a response. If you seem to have 10
    active connections to your server at any moment, ELB makes certain to keep
    11: that way a new connection coming in will immediately be able to be
    served, and not have to wait for a new connection to spin up. (This
    behavior, for the record, is also why it is totally reasonable to run ELB
    with KeepAlive off on your backend server. ;P)

Another point:

    Honestly, my response to this is going to be to look into mod_reqtimeout:
    I am not even certain that having header read timeouts is required at all
    if you are using ELB. It might be that you are actually much better off
    never timing the extra spare connections from the ELB out at all.

And there are *three*ways* to solve it:

- Change the ELB timeout to be LOWER and apache2
- Change the apache2 timeout to be HIGHER than the ELB
- Disable apache2 timeout

** Solution 1: Change the ELB idle timeout to be LOWER and apache2

Pretty straightforward.

- [[https://aws.amazon.com/blogs/aws/elb-idle-timeout-control/][https://aws.amazon.com/blogs/aws/elb-idle-timeout-control/]]

** Solution 2: Change the apache2 timeout to be HIGHER than the ELB

- [[https://httpd.apache.org/docs/current/mod/core.html#timeout][https://httpd.apache.org/docs/current/mod/core.html#timeout]]

Enable the request timeout mod and set the timeouts for header and body to be
larger than the idle timeout setting on the ELB.

    # In the above configuration, replace XXXX with the value of your ELB
    # Connection Draining Timeout plus 2 seconds.
    # /etc/httpd/conf.d/mod_reqtimeout.conf
    <IfModule reqtimeout_module>
      RequestReadTimeout header=XXXX,MinRate=500 body=XXXX,MinRate=500
    </IfModule>

*** Resources

- [[http://georgebohnisch.com/fix-elastic-load-balancer-408-errors-elastic-beanstalk/][http://georgebohnisch.com/fix-elastic-load-balancer-408-errors-elastic-beanstalk/]]
- [[https://disloops.com/apache-logs-in-aws/][https://disloops.com/apache-logs-in-aws/]]

** Solution 3: Disable apache2 timeout

    RequestReadTimeout header=0 body=0

But this would disable request read timeout all together!

*** Resources

- [[https://medium.com/@gonfva/aws-elb-in-apache-logs-83883b5a90c5][https://medium.com/@gonfva/aws-elb-in-apache-logs-83883b5a90c5]]

** My experience

What failed:

- Reducing/removing *Connection*Draining* from the ELB of the AG
- Reducing/removing the *Deregistration*Delay* from the Target Group

Only changing the ELB idle timeout worked.
