Prometheus
|| Last update: 12 Mar 2021

* Intro

.image images/prometheus.png 100 _

Prometheus is a cloud aware monitoring and alerting tool.
Prometheus was the second project in [[https://www.cncf.io/][CNCF]] after k8s.
There is a lot of investments and backing behind it, being supported out of the
box by k8s, cAdvisor and other mainstream technologies.
It _focus_ on operation system monitoring.

NOTE: Don't use it for business metrics. Is not made to be precise, is made to be performant.
For business metrics use a database.

** What it is

Prometheus is a time based monitoring ecosystem based on a pulls system.
It aims *to* give:

- instrumentation
- storage (local disk)
- querying
- alerting (via alert manager)
- dashboard to query

It works only numeric time series.

** What it's not

Prometheus is not a time series database.

To quote someone (I don't remember who now):

    "Prometheus is not a time series database,
    it just happens to use one."

** Selling points

- key=value pair on any metrics (to identify: "came from", "pertaining to")
- PromQL query language
- Very efficient on a single node
- Operation simplicity (single static library and small config file)
- Cloud aware (getting the metadata from autodiscovery systems)

** What it doesn't do

- no collection of individual events (ip address, exact path and so on)
- no request tracing
- no magic anomaly detection
- no durable long-term storage
- no automatic horizontal scaling
- no user/auth management

* Architecture

Prometheus chose to use http as a transport.

It doesn't scale in the sense that it doesn't shard. Every instance is self
contained and autonomous. They scale vertically.
To have HA you need to run multiple (like using the prometheus operator) and
use something on top like Thanos.

** Query language

PromQL is not a sql style query language, but is very good to do arithmetics.

** Pull vs push

Prometeus implements a pull model. The selling point of a pull model are:

- automatic up-ness monitoring
- horizontal monitoring (multiple monitoring tools can scrape the same data)
- simple HA - two Prometheus server pulling in the same exporters (many of the same configured service)
- less configuration
- scales vertically

Prometheus implements something called remote write to push data forward to other
destinations.

** Storage

Storage being local, it means the longer retention you have, the bigger and more
performant is the disk you need.
For high retention better to use something like Thanos.

* Alerting

- [[https://prometheus.io/docs/alerting/overview/]]

Prometheus implements alerting via an application called alert manager.
Alert manager is a different service but part of the same binary.
To make alert manager to point to the correct Prometheus DNS (and not the pod
id/name) in Helm you need to set the option _baseURL_:

    server:
      baseURL: https://prometheus.mydomain.com

In the alerts definition, you can use labels to add contextual information
to send together with the alert (for example slack or pagerduty):

    - "name": "jaeger_alerts"
      "rules":
        - "alert": "JaegerHTTPServerErrs"
          "annotations":
            "message": |
              {{ $labels.job }} {{ $labels.instance }} is experiencing
              {{ printf "%.2f" $value }}% HTTP errors.
          "expr": "100 * sum(rate(jaeger_agent_http_server_errors_total[5m]))
          by (instance, job, namespace) / sum(rate(jaeger_agent_http_server_total[5m]))
          by (instance, job, namespace)> 1"
          "for": "15m"
          "labels":
            "severity": "warning"
            "runbook_url": "https://myrunbook.com"
            "confluence_page": "https://mydoc.com"

In this example you can see how runbook and confluence link are added to
the alert. When you alert you need a route and a receiver. A receiver looks like this:

    - name: 'a-channel-receiver'
      slack_configs:
        - channel: '#a-channel'
          send_resolved: true
          title: |
            *| {{ some gotmpl stuff }} |*
            *| Alert: {{ some gotmpl stuff }} |*
          text: {{ some gotmpl stuff }}

In the case above the target is slack. You can define a title and a text.
I would advice to look online for some templating example about it.

The you need a route. Something that capture alerts and send it to a receiver.

      - receiver: 'a-channel-receiver
        group_wait: 1m
        group_interval: 1m
        repeat_interval: 1h
        group_by: [ job, alertname ]
        match_re:
          alertname: '(Jaeger).*'

This one for example will "capture" all the alert that match the match_re and send
it to a receiver.

NOTE: BE CAREFUL ABOUT THE ORDERING.

The route will capture the alerts so you can't put it AFTER a catch all like the
following.

      - receiver: 'notify'
        group_wait: 10m
        group_interval: 30m
        repeat_interval: 1d
        group_by: [ name, alertname ]
        match_re:
          severity: 'notify'
      - receiver: 'warning'
        group_wait: 5m
        group_interval: 30m
        repeat_interval: 30m
        group_by: [ name, alertname ]
        match_re:
          severity: 'warning'
      - receiver: 'critical'
        group_wait: 30s
        group_interval: 1m
        repeat_interval: 10m
        group_by: [ name, alertname ]
        match_re:
          severity: 'critical'

This will match basically all your alerts so you need them AT THE END of
your config.

* Exposition formats

- [[https://prometheus.io/docs/instrumenting/exposition_formats/]]
- [[https://github.com/RichiH/OpenMetrics/blob/master/markdown/protobuf_vs_text.md]]

From version 2.0 Prometheus DOES NOT SUPPORT PROTOBUF anymore.

* Gathering metrics

When we speak about collecting metrics we consider two types:

- _whitebox_ instrumentation: applications/systems expose their metrics
- _blackbox_ instrumentation: use exporter (or batch jobs)

There is some limitations when working with a scraping system:

CON

- *ONLY* the latest metric of a type and *NO*HYSTORY*

PRO

- idempotent
- many Prometheus scraping the same nodes/apps have the same data

Prometheus can gather data by any source that exposes metrics in it's format.
When this is not available, _exporters_ can be written to do the heavy
lifting of _translating_ the data from another format (or gather the data
directly).

An examples of exporters is the [[https://github.com/prometheus/node_exporter][Node exporter]]
to gather node metrics.
All exporters can be found [[https://prometheus.io/docs/instrumenting/exporters/][here]].

* Labels and cardinality

- [[https://prometheus.io/docs/practices/naming/#labels]]
- [[https://prometheus.io/docs/practices/instrumentation/#use-labels]]

Prometheus embrace the _label_data_model_ and NOT the _hierarchical_data_model_.
It is considered:

- more flexible (allow cross cutting)
- more efficient
- explicit dimension

What you have to do with grafite is like this:

    api-server.*.*.post.*

What is cardinality:

    Cardinality in the context of monitoring systems is defined as the number of
    unique metric time series stored in your monitoring systemâ€™s time series
    database (TSDB). Generally, a metric time series (MTS) is the unique
    combination of a metric name and any number of key-value pairs known as
    dimensions or labels.
    Source: https://www.signalfx.com/blog/high-cardinality-monitoring-is-a-must-have-for-microservices-and-containers/

Having a high cardinality is dangerous:

    High cardinality is hard to manage because it increases both the number of
    time series that need to be stored by your time series database, and the
    size of queries that have to be made to it on a regular basis. Queries to
    the database become more computationally expensive, because there are now
    more time series, and any significant event (e.g. a code push, burst of user
    traffic) will result in a flood of simultaneous writes to the database as
    well.
    The documentation for many monitoring systems actually warns users not to
    send in dimensions with a high number of potential values, or to keep
    dimension values below a hard limit to avoid performance penalties.
    Source: https://www.signalfx.com/blog/high-cardinality-monitoring-is-a-must-have-for-microservices-and-containers/

To know which metrics are using the most resources it'd be good to count how
many time series each has:

    topk(10, count by (__name__)({__name__=~".+"}))
    # aggregate by jobs
    topk(10, count by (__name__, job)({__name__=~".+"}))
    # which job have more time series
    topk(10, count by (job)({__name__=~".+"}))
    Source: https://www.robustperception.io/which-are-my-biggest-metrics

A more complex query (and heavier is):

    sort_desc(label_replace( count by(__name__) ({__name__!="", __name__!="card_count"}), "metric_name", "$1", "__name__", "(.+)" ))

Another possible approach is to use two prom metrics:

- scrape_samples_scraped: It reports the numbers of samples that the target produced, before metric_relabel_configs is applied.
- scrape_samples_post_metric_relabeling

For example:

    topk(10, scrape_samples_post_metric_relabeling)
    Source: https://www.robustperception.io/which-targets-have-the-most-samples

For each *metric* any unique combinations of key-value creates new *time*series*
in Prometheus. Prometheus doesn't like is high-cardinality labels.

    Prometheus considers each unique combination of labels and label value as a
    different time series. As a result if a label has an unbounded set of
    possible values, Prometheus will have a very hard time storing all these
    time series. In order to avoid performance issues, labels should not be used
    for high cardinality data sets (e.g. Customer unique ids).
    Source: https://blog.pvincent.io/2019/05/prometheus-workshops-follow-up-frequently-asked-questions/

You should be careful with label and do not exaggerate with them:

    CAUTION: Remember that every unique combination of key-value label pairs
    represents a new time series, which can dramatically increase the amount of
    data stored. Do not use labels to store dimensions with high cardinality
    (many different label values), such as user IDs, email addresses, or other
    unbounded sets of values.
    Source: https://prometheus.io/docs/practices/naming/#labels

- [[https://prometheus.io/docs/practices/instrumentation/#do-not-overuse-labels]]

So the question is what is the right number for cardinality. It depends, but there
are some guidelines:

    As a general guideline, try to keep the cardinality of your metrics below 10,
    and for metrics that exceed that, aim to limit them to a handful across your
    whole system. The vast majority of your metrics should have no labels.
    If you have a metric that has a cardinality over 100 or the potential to
    grow that large, investigate alternate solutions such as reducing the number
    of dimensions or moving the analysis away from monitoring and to a
    general-purpose processing system.
    Source: https://prometheus.io/docs/practices/instrumentation/#do-not-overuse-labels

    For very simple applications with little logic that only do one thing, I'd
    expect on the order of 100 time series.
    [...]
    For complex applications with lots of moving parts, I'd expect on the order
    of 1000 time series.
    [...]
    When an application exposes more than that, getting up towards 10,000 time
    series that's an indication that you may have a cardinality issue and might
    want to cut back on labels a bit. This is however unavoidable sometimes for
    cases such as reverse proxies where there's many many backend services, or
    databases where there's many many tables and you need information for each.
    Be wary of going too much above 10,000, Prometheus is designed for many
    small scrapes, not a handful of massive ones.
    Source: https://www.robustperception.io/how-many-metrics-should-an-application-return

- [[https://www.robustperception.io/how-many-metrics-should-an-application-return]]

Some max cardinality from monitoring systems:

- Datadog : 1000 tag/dimension values per metric - specifically warning that exceeding this limit will incur performance penalties
- Datadog APM: Infinite cardinality (LOL)
- [[https://docs.newrelic.com/docs/agents/manage-apm-agents/agent-data/collect-custom-metrics][New relic]]: 2000
- SignalFx: 50000

Cardinality is the reason why
[[https://www.robustperception.io/why-are-prometheus-histograms-cumulative][histogram are cumulative]].

You can calculate how much memory they use for cardinality and ingestion here
[[https://www.robustperception.io/how-much-ram-does-prometheus-2-x-need-for-cardinality-and-ingestion]]

To be sure your application is not killed by an explosion of cardinality, you can use a tool like
[[https://blog.freshtracks.io/bomb-squad-automatic-detection-and-suppression-of-prometheus-cardinality-explosions-62ca8e02fa32][bomb squad]].

- [[https://github.com/open-fresh/bomb-squad]]

or use sample limit [[https://www.robustperception.io/using-sample_limit-to-avoid-overload]].

- [[https://www.robustperception.io/using-tsdb-analyze-to-investigate-churn-and-cardinality]]

** Best practices

- Labels should rarely have more than 100 possible values
- The set of label values should be bounded (think of them as enums, and not open ended sets of strings)

* Prometheus on k8s

** Helm charts

- [[https://github.com/prometheus-community/helm-charts/tree/main/charts]]

You can install prometheus simply by running:

    helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
    helm install prometheus-community/prometheus
