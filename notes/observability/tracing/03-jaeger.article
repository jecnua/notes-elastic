Jaeger
|| Last update: 28 Oct 2020

* Jaeger

- [[http://www.jaegertracing.io/]]
- [[https://github.com/jaegertracing/jaeger]]

    Jaeger, inspired by Dapper and OpenZipkin, is a distributed tracing platform
    created by Uber Technologies and donated to Cloud Native Computing Foundation.
    It can be used for monitoring microservices-based distributed systems.
    Source: https://github.com/jaegertracing/jaeger

The backend is written in GO and the UI is in react.

* High level architecture

- [[https://www.jaegertracing.io/docs/architecture/]]
- [[https://www.jaegertracing.io/docs/features/]]
- [[https://www.jaegertracing.io/docs/cli/]]

.image images/architecture-v2.png 500 _
.caption Souce: [[https://www.jaegertracing.io/docs/1.20/architecture/]]

Jaeger has been build with scalability and parallelism in mind and is designed
to have no single points of failure.

    An instrumented service creates spans when receiving new requests and
    attaches context information (trace id, span id, and baggage) to outgoing
    requests. Only the ids and baggage are propagated with requests; all other
    profiling data, like operation name, timing, tags and logs, is not
    propagated. Instead, it is transmitted out of process to the Jaeger backend
    asynchronously, in the background.
    Source: https://www.jaegertracing.io/docs/1.20/architecture/

When Jaeger was introduced in 2017 the collector directly linked to the DB. The
data process is done in batches and this type of architecture was suffering
from performance issue when too much data was ingested.
With too much data in the collector, the DB may suffer and data could be dropped.
All this was the PUSH model.

The new infra allows for kafka to be between the collector and the storage.
The ingestor read data from kafka and write them on the DB. Also Flink read from
Kafka if you use it.

* Components

- [[https://www.jaegertracing.io/docs/deployment/]]

The main components are:

- Agent - [[https://hub.docker.com/r/jaegertracing/jaeger-agent/]]
- Collector - [[https://hub.docker.com/r/jaegertracing/jaeger-collector/]]
- Query (retrieves traces from storage and hosts a UI to display them) - [[https://hub.docker.com/r/jaegertracing/jaeger-query/]]
- Ingester (optional: only if you use kafka as a buffer) - [[https://hub.docker.com/r/jaegertracing/jaeger-ingester/]]

** Agents

- [[https://www.jaegertracing.io/docs/deployment/#agent]]

The agent usually lives in a sidecar inside the pod and recieves spans via
UDP.

    The Jaeger agent is a network daemon that listens for spans sent over UDP,
    which it batches and sends to the collector. It is designed to be deployed
    to all hosts as an infrastructure component. The agent abstracts the routing
    and discovery of the collectors away from the client.
    Source: https://www.jaegertracing.io/docs/1.20/architecture/

The agents receive spans over UDP on port 5775. The spans are batched, encoded
as [[https://thrift.apache.org/][Thrift structures]] and submitted to the
collector.

*NOTE*: The app is using Thrift because at Uber was what they are used to.
However, it's in the planning a move to _gRPC_.
Source: [[https://groups.google.com/forum/#!topic/jaeger-tracing/toBNkL_zEDE]]

The Routing and Discovery phase of the collectors from the client library is also
the responsibility of the agent.

Jaeger can accept Zipkin span transparently (is Zipkin compatible).

Performance can be tuned:

- [[https://medium.com/jaegertracing/tuning-jaegers-performance-7a60864cf3b1]]

Agents have a service discovery system but it can be avoided if you are
running it in k8s load balancing via services.

    Jaeger supports agent level tags, that can be added to the process tags of
    all spans passing through the agent.
    Source: https://www.jaegertracing.io/docs/1.20/deployment/#agent

We can use agent level tags to give more filtering power at UI level for traces.

** Collector

- [[https://www.jaegertracing.io/docs/1.20/deployment/#collectors]]

    The Jaeger collector receives traces from Jaeger agents and runs them
    through a processing pipeline. Currently our pipeline validates traces,
    indexes them, performs any transformations, and finally stores them.
    Source: https://www.jaegertracing.io/docs/1.20/architecture/

Collectors are stateless.
Scale them with HPO.  If possible find the maximum that es can manage.

** Pluggable storage backends

- [[https://www.jaegertracing.io/docs/1.20/#technical-specs]]

Upstream supports:

- In-memory
- *Cassandra* 3.4+
- *ElasticSearch* 5.x/6.x/7.x
- Badger (demo and test purpose)

- [[https://www.jaegertracing.io/docs/1.20/deployment/#elasticsearch]]

In Elasticsearch two indices will be created: one for storing the services and
the other one for the spans of given services. (checks)

    $ jaeger-collector --span-storage.type elasticsearch [...]

ElasticSearch:

      Jaeger uses index-per-day pattern to store its data to Elasticsearch.

Since 1.10.1 Jaeger is able to maintain the number of indices and perform
their own cleanup.

- [[https://medium.com/jaegertracing/using-elasticsearch-rollover-to-manage-indices-8b3d0c77915d]]
- [[https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-rollover-index.html]]

    For example, indices that do not contain any data still allocate shards, and
    conversely, a single index might contain significantly more data than the
    others. Jaeger by default stores data in daily indices which might not
    optimally utilize resources. Rollover feature can be enabled by
    --es.use-aliases=true
    Source: https://www.jaegertracing.io/docs/1.20/deployment/#elasticsearch

It can also be managed by the operator (creation or run?)

ES rollover for indices requires initialisation. Worth checking if the operator
can do it for me.

    docker run -it --rm --net=host jaegertracing/jaeger-es-rollover:latest init http://localhost:9200

- [[https://www.jaegertracing.io/docs/1.20/frontend-ui/#archive-support]]

    archiveEnabled enables (true) or disables (false) the archive traces button.
    Default: false. It requires a configuration of an archive storage in Query
    service. Archived traces are only accessible directly by ID, they are not
    searchable.
    Source: https://www.jaegertracing.io/docs/1.20/frontend-ui/#archive-support

By default it creates spark dependencies:

    dependencies:
      enabled: true
      image: jaegertracing/spark-dependencies
      resources: {}
      schedule: 55 23 * * *

So disable them (if you don't need them).

** Buffer

Kafka can be used as a preliminary buffer.

** Ports

- [[https://www.jaegertracing.io/docs/1.20/getting-started/]]

    CLIENT 6831	  UDP	  agent	accept jaeger.thrift over compact thrift protocol
    CLIENT 6832	  UDP	  agent	accept jaeger.thrift over binary thrift protocol
    CLIENT 5778	  HTTP	serve configs, sampling strategies
    CLIENT 14271  HTTP	admin port: health check at / and metrics at /metrics
    QUERY  16686	HTTP	query	serve frontend
    QUERY  16687  HTTP  admin port: health check at / and metrics at /metrics
    COLLECTOR 14250	HTTP  collector	accept model.proto gRPC
    COLLECTOR 14269 HTTP  admin port: health check at / and metrics at /metrics

* Workflow

.image images/jaeger_work.png 700 _
.caption Souce: [[https://www.jaegertracing.io/docs/architecture/]]


    The client emits traces to the agent which listens for inbound spans and routes
    them to the collector. The responsibility of the collector is to validate,
    transform and store the spans to the persistent storage. To access tracing data
    from the storage, the query service exposes a REST API endpoints and the React
    based UI.
    Source: https://sematext.com/blog/opentracing-jaeger-as-distributed-tracer/

** Span and traces

    A span represents a logical unit of work in Jaeger that has an operation
    name, the start time of the operation, and the duration.
    [...]
    A trace is a data/execution path through the system, and can be thought of
    as a directed acyclic graph of spans.
    Source: https://www.jaegertracing.io/docs/1.20/architecture/

Fields are:

- traceID
- spanID
- parentSpanID
- flags
- operationName
- references
- startTime (unix epoch)
- duration (millis)
- tags
- logs
- processID
- process
- serviceName
- warnings

The span context *is*NOT*defined* in the opentracing javascript

- [[https://github.com/opentracing/opentracing-javascript/blob/master/src/span_context.ts]]

* Sampling

- [[https://www.jaegertracing.io/docs/1.20/sampling/]]

    To minimize the overhead, Jaeger clients employ various sampling strategies.
    When a trace is sampled, the profiling span data is captured and transmitted
    to the Jaeger backend. When a trace is not sampled, no profiling data is
    collected at all, and the calls to the OpenTracing APIs are short-circuited
    to incur the minimal amount of overhead. By default, Jaeger clients sample
    0.1% of traces (1 in 1000), and have the ability to retrieve sampling
    strategies from the Jaeger backend.
    Source: https://www.jaegertracing.io/docs/1.20/architecture/

    Uses consistent upfront sampling with individual per service/endpoint probabilities


There are two two approaches to do sampling:

- Coherent head-based sampling
- Tail based sampling


** Central control

Is possible to configure the settings centrally and then push these to the clients
(this allow runtime changes without redeploying). However it will also centralize
all in one file for all apps.

** Coherent head-based sampling

sampling decision when request first time is recieved and this is propagated to
all downstream services and needs to be respected). There are two ways to
implement this:

- Probabilitic (like 1%)
- Rate-limiting (like 10 req per second)

Sampling is configured per service. So if you have multiple endpoint, and some with
lower traffic with a single configuration
the endpoint with low traffic won't be sampled
Jaeger support also endpoint-level sampling



Jaeger libraries implement consistent upfront (or head-based) sampling.

CHECK Looks like the only option available is normal sampling?

    Jaeger libraries implement consistent upfront (or head-based) sampling.
    [...]
    When service A receives a request that contains no tracing information,
    Jaeger tracer will start a new trace, assign it a random trace ID, and make
    a sampling decision based on the currently installed sampling strategy. The
    sampling decision will be propagated with the requests to B and to C, so
    those services will not be making the sampling decision again but instead
    will respect the decision made by the top service A. This approach
    guarantees that if a trace is sampled, all its spans will be recorded in the
    backend. If each service was making its own sampling decision we would
    rarely get complete traces in the backend.
    Source: https://www.jaegertracing.io/docs/1.10/sampling/

The agent *POLL* for a _sampling_strategy_ from the tracing backend and propagate
the sampling rate to all tracer clients. This is useful in dynamic environments.

For now Jaeger *does*not* support tail-based sampling.

- [[https://github.com/jaegertracing/jaeger/issues/425][GITHUB ISSEUS: Discuss post-trace (tail-based) sampling]]

They however allow:

- constant sampling
- probabilist, head based sampling
- rate limiting
- remote

Remote is probably the most interesting one:

    [remote] This allows controlling the sampling strategies in the services
    from a central configuration in Jaeger backend, or even dynamically (see
    Adaptive Sampling).
    Source: https://www.jaegertracing.io/docs/1.10/sampling/#client-sampling-configuration

However this will mean that we will manage this centrally and it's not a good
idea. However it can change at runtime.

Another idea on a possible approach:

- [[https://groups.google.com/forum/?#!msg/distributed-tracing/fybf1cW04ZU/KhcF5NxTBwAJ][GOOGLE GROUP: Keeping "most interesting" traces]]

Upcoming Adaptive Sampling functionality:

- [[https://github.com/jaegertracing/jaeger/issues/365][GITHUB ISSUE: Adaptive Sampling ]]

Also post-collection data processing pipeline (coming soon)

** Tail-based sampling

Still not supported but in the works.

* Monitoring

- [[https://www.jaegertracing.io/docs/1.20/monitoring/]]

Jeager export metrics via prometheus endpoint by default.

At Uber, they monitor:

- Node signals: CPU/memory usage, file descriptors, network usage, etc
- Go runtime signals: goroutine count, GC, etc
- Jaeger internals: dropped span count, mem queue size, storage write counts/latencies

At Uber they don't use auto-scaling for collectors, just a fixed size pool.
The bottleneck is typically the storage, which is hard to auto-scale.

Tuning: [[https://www.jaegertracing.io/docs/1.20/performance-tuning/]]

* Infrastructure

- [[https://www.jaegertracing.io/docs/1.20/deployment/]]

There are three defined way to install Jaeger.

NOTE: The current recommended way of installing and managing Jaeger in a
production Kubernetes cluster is via the Jaeger Operator.

- [[https://github.com/jaegertracing/helm-charts]] Helm charts
- [[https://github.com/jaegertracing/jaeger-kubernetes]] k8s templates - Deprecated
- [[https://github.com/jaegertracing/jaeger-operator]] Jaeger Operator - Can be installed via Chart

** Jeager operator

- [[https://www.jaegertracing.io/docs/1.20/operator/]]
- [[https://github.com/jaegertracing/jaeger-operator]]

It's the preferred way of installation.

With helmfiles:

    - name: jaeger
      namespace: monitoring
      chart: jaeger/jaeger-operator
      version: 2.14.2
      values:
        - ../helm_vars/monitoring/jaeger/jaeger-dev-global.yaml

Documentation is not good at all.
It will automatically create the spark dependency when using the production
profile. Disable it this way:

    storage:
      type: elasticsearch
      options:
        es:
          server-urls: xxx
      esIndexCleaner:
        numberOfDays: 7
      dependencies:
        enabled: false # Disable spark dependencies enabled by default

Will create an ingress with *!
Change it:

    jaeger:
      create: true # Specifies whether Jaeger instance should be created
      spec:
        strategy: production
        ingress:
          annotations:
            kubernetes.io/ingress.class: nginx-ineed
          hosts:
            - jaeger.foo.com

I cannot disable the UI and they are 2 pods!





* Jaeger backend on ECS (old section)

This section is old plus it was from a time where NLB did not support UDP.

** UDP load balancing in AWS (not)

Sorry for the link but I didn't find it on the new repo

- [[https://github.com/jpkrohling/jaeger/blob/5567f02b285329ea40caecdaa6a723eb626a24f1/docs/deployment.md]]

    The agents can connect point to point to a single collector address, which
    could be load balanced by another infrastructure component (e.g. DNS) across
    multiple collectors. The agent can also be configured with a static list of
    collector addresses.
    On Docker, a command like the following can be used:
    docker run \
      --rm \
      -p5775:5775/udp \
      -p6831:6831/udp \
      -p6832:6832/udp \
      -p5778:5778/tcp \
      jaegertracing/jaeger-agent \
      /go/bin/agent-linux --collector.host-port=jaeger-collector.jaeger-infra.svc:14267

- [[https://www.reddit.com/r/aws/comments/8tirpw/udp_load_balancing_in_aws/]]
- [[https://www.reddit.com/r/aws/comments/82pvuf/udp_load_balancing_in_aws/]]

The only solution I can find it to use dns.
Since I can use dns, I will invest in that direction, I don't want to work with
lambda and floating ip.

- [[https://docs.aws.amazon.com/AmazonECS/latest/developerguide/create-service-discovery.html]]

NLB doesn not support UDP.

There is no support for discovery systems:

- [[https://github.com/jaegertracing/jaeger/issues/213]]

** Don't use host port

also this:

    service jaeger-agent was unable to place a task because no container
    instance met all of its requirements. The closest matching
    container-instance x-x-x-x-x is already using a
    port required by your task. For more information, see the Troubleshooting
    section.

if you are exposing a port the old task is locked in an INACTIVE status and you
can't release

Unless you put 0% availability during releases

** Agent

for lack of UDP routing capabilities I can't run an agent behind a lb in aws
this means I will run the agent as deamon on each node and use a normal lb over
it.

however still you have no way to route the traffic there at all

** No load balancers

so you have to ignore load balancers if you are using udp and agent

** Collector

I decided to run it as daemon to get performance (is tcp) and avoid using
another ENI. this will allow to use lower sizes as boxes

** Agent connection to collector

Agent needs to run on docker ip

    "command": [
        "--collector.host-port=172.17.0.1:14267"
    ],

- [[https://stackoverflow.com/questions/51184501/aws-ecs-containers-are-not-connecting-but-works-perfectly-in-my-local-machine]]

* Re/Sources

- VIDEO [[https://youtu.be/BWtNelj_XUc?t=362][Jaeger Deep Dive]] - 4 Sep 2020
- [[https://shkuro.com/books/2019-mastering-distributed-tracing/]] - BOOK - 2019
