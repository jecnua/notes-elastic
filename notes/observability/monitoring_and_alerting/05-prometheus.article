Prometheus (OBSOLETE - TO BE CLEANED)
|| Last update: 6 Aug 2019

* Intro

.image images/prometheus.png 100 _

Prometheus is a cloud aware monitoring and alerting tool.
Prometheus was the second project in [[https://www.cncf.io/][CNCF]] after k8s.
There is a lot of investments and backing behind it, being supported out of the
box by k8s, cAdvisor and other mainstream technologies.
It _focus_ on operation system monitoring.

** What it is

Prometheus is a time based monitoring ecosystem based on a pulls system.
It aims *to* give:

- instrumentation
- storage
- querying
- alerting (via alert manager)
- dashboards

** What it's not

Prometheus is not a time series database.

To quote someone I don't remember now:

    "Prometheus is not a time series database, it just happens to use one."

** Selling points

- key=value pair on any metrics (came form, pertaining to)
- PromQL query language
- Very efficient on a single node
- Operation simplicity (single static library and small config file)
- Cloud aware (getting the metadata from autodiscovery systems)

** What it doesn't do

- no collection of individual events (ip address, exact path and so on)
- no request tracing
- no magic anomaly detection
- no durable long-term storage
- no automatic horizontal scaling
- no user/auth management

** Limitations

- Only numeric time series

* Architecture

** Text over http

Prometheus chose to use http as a transport.

** Scaling?

It doesn't scale in the sense that it doesn't shard. Every instance is self
contained and autonomous. Run 1 or more, but they should be all the same if you
want HA.

** Query language

not a sql style query language
PromQL is very good to do arithmetics between rating
will join them up in the same metrics (?)

does only read

** Pull vs push

- automatic upness monitoring
- horizontal monitoring (tell to push to different things) WRONG
- more flexyble (production monitoring on your laptop ? ) NO, NOT SG
- simple HA - two Prometheus server pulling in the same one (many of the same configured service)
- less configuration (tell my mornitoring service - do that in any case) NO. zk
- scales

From version 2.0 Prometheus DOES NOT SUPPORT PROTOBUF anymore.

Prometheus push metrics to _something_else_
replay your historical metadata (???)
flag for turning local storage
just turn local storage and only push remotely

digital ocean
VULCAN
cassandra/elasticsearch

bosun didn't take it

** Storage

Local storage is epehemerial.
They don't reccomend keeping it for years

problem: not enough disk space

you can't dump and you can't redeploy

one workaroud:
many prometheus server that federates on a single ...
one Prometheus that pulls every x minues

** Alerts

Prometheus implements alerting via an aplication called alert manager.

- [[https://prometheus.io/docs/alerting/overview/]]

In this context an alert is a query that return a timeseries.
When you add a query this way you can have two results:

- no timeseries: all ok
- some timeseries: an alert for each one of them

Alert manager is a different service but part of the same binary.

To make alert manager to point to the correct Prometheus DNS (and not the pod
id/name) in Helm you need to set the option _baseURL_:

    server:
      baseURL: https://prometheus.mydomain.com

- [[https://github.com/helm/charts/tree/master/stable/prometheus]]

** Pull method

run on every network segment
fewer step as possible

- open port in firewll/router
- open VPN tunnel

** Uber exporters or per process exporters

main uber exporter

not only one (for them)

- operation bottleneck
- spof, no isolation
- can't scrape selectively (not because is only us)
- harder to associate metadaat

Prometheus by discovering your node
with auto-discovery
so can put metadata from this process
they advice: *one*exporter*per*process*

* Exposition formats

- [[https://prometheus.io/docs/instrumenting/exposition_formats/]]
- [[https://github.com/RichiH/OpenMetrics/blob/master/markdown/protobuf_vs_text.md]]

From version 2.0 Prometheus DOES NOT SUPPORT PROTOBUF anymore.

** Why not json

- [[https://www.youtube.com/watch?v=4DzoajMs4DM&feature=youtu.be&t=11m59s]]

- json is bad for both
- json is not really streamable
- with text you can start processing line by line
- json you need it all together

** client libraries

client libs keep state but not much (no history, only current state)

    - Counter: cumulative metric that represents a single numerical value that only ever goes up
    - Gauge: represents a single numerical value that can arbitrarily go up and down
    - Histogram: samples observations and counts them in configurable buckets
    - Summary: similar to a histogram, samples observations, it calculates configurable quantiles over a sliding time window
    Source: https://blog.risingstack.com/node-js-performance-monitoring-with-prometheus/

rates not computes!!! data as raw as possible and you do in on Prometheus side

metrics for you
manage metrics for you
pre-aggregation is more efficient

* Gathering metrics

When we speak about collecting metrics we consider two types:

- _whitebox_ instrumentation: applications/systems expose their metrics
- _blackbox_ instrumentation: use exporter (or batch jobs)

There is some limitations when working with a scraping system:

CON

- *ONLY* the latest metric of a type and *NO*HYSTORY*

PRO

- idempotent
- many Prometheus scraping the smae node have the same data

Prometheus can gather data by any source that exposes metrics in it's format.
When this is not available, _exporters_ can be written to do the heavy
lifting of _translating_ the data from another format (or gather the data
directly).

Examples of application exposing metrics natively for Prometheus:

- [[https://github.com/google/cadvisor][cAdvisor]]: Container metrics
- [[http://kubernetes.io/][Kubernetes]]

** From an exporter

An examples of exporters is the [[https://github.com/prometheus/node_exporter][Node exporter]]
 to gather node metrics.

All exporters can be found [[https://prometheus.io/docs/instrumenting/exporters/][here]].

** From an application

If you are creating a new app and you want Prometheus to gather your metrics,
the convention is to expose a */metrics* endpoint with the metrics you want.

    The mentality:
    Dev own the metrics

** Prometheus federation: data from another Prometheus

Prometheus can grab metric from another instance using what it's called
[[https://prometheus.io/docs/operating/federation/][federation]]. This allow to
have redundancy (HA) in the system or machines _specialised_ in a
subset of the metrics.

*** A Prometheus on every laptop?

DEVs can spin their own Prometheus to read their own metrics. Let's say you
have a team working on a project. They can spin a Prometheus instance locally
pulling the data ONLY from these services/endpoints.

This can help if you want to isolate environments (e.s. DEVs cannot see some
metrics) or just to have a less cluttered view of what's out there.

* How to approach Prometheus

- Start with grafana
- Show how HA works
- Share the success stories
- Help DEVs see and understand how to use the tool

** Download the Grafana dashboards

They are just json so you can just pull from the grafana.net website or any
github repo. You can use the api to push them on the app or do it manually.

** Monitoring nodes (OBSOLETE - 2016)

You may (_will_) have many exporter on a single node, each of them exposing the
metrics of a single application that is not natively aware of Prometheus.
However, one exporter you want for sure is the *node-exporter*.

The node-exporter _exports_ the metrics of the node and is indeed something
you want to have on any of your machine.

* Labels and cardinality

- [[https://prometheus.io/docs/practices/naming/#labels]]
- [[https://prometheus.io/docs/practices/instrumentation/#use-labels]]

Prometheus embrace the _label_data_model_ and NOT the _hierarchical_data_model_.
It is considered:

- more flexible (allow cross cutting)
- more efficient
- explicit dimension

What you have to do with grafite is like this:

    api-server.*.*.post.*

What is cardinality:

    Cardinality in the context of monitoring systems is defined as the number of
    unique metric time series stored in your monitoring systemâ€™s time series
    database (TSDB). Generally, a metric time series (MTS) is the unique
    combination of a metric name and any number of key-value pairs known as
    dimensions or labels.
    Source: https://www.signalfx.com/blog/high-cardinality-monitoring-is-a-must-have-for-microservices-and-containers/

Having a high cardinality is dangerous:

    High cardinality is hard to manage because it increases both the number of
    time series that need to be stored by your time series database, and the
    size of queries that have to be made to it on a regular basis. Queries to
    the database become more computationally expensive, because there are now
    more time series, and any significant event (e.g. a code push, burst of user
    traffic) will result in a flood of simultaneous writes to the database as
    well.
    The documentation for many monitoring systems actually warns users not to
    send in dimensions with a high number of potential values, or to keep
    dimension values below a hard limit to avoid performance penalties.
    Source: https://www.signalfx.com/blog/high-cardinality-monitoring-is-a-must-have-for-microservices-and-containers/

To know which metrics are using the most resources it'd be good to count how
many time series each has:

    topk(10, count by (__name__)({__name__=~".+"}))
    # aggregate by jobs
    topk(10, count by (__name__, job)({__name__=~".+"}))
    # which job have more time series
    topk(10, count by (job)({__name__=~".+"}))
    Source: https://www.robustperception.io/which-are-my-biggest-metrics

A more complex query (and heavier is):

    sort_desc(label_replace( count by(__name__) ({__name__!="", __name__!="card_count"}), "metric_name", "$1", "__name__", "(.+)" ))

Another possible approach is to use two prom metrics:

- scrape_samples_scraped: It reports the numbers of samples that the target produced, before metric_relabel_configs is applied.
- scrape_samples_post_metric_relabeling

For example:

    topk(10, scrape_samples_post_metric_relabeling)
    Source: https://www.robustperception.io/which-targets-have-the-most-samples

For each *metric* any unique combinations of key-value creates new *time*series*
in Prometheus. Prometheus doesn't like is high-cardinality labels.

    Prometheus considers each unique combination of labels and label value as a
    different time series. As a result if a label has an unbounded set of
    possible values, Prometheus will have a very hard time storing all these
    time series. In order to avoid performance issues, labels should not be used
    for high cardinality data sets (e.g. Customer unique ids).
    Source: https://blog.pvincent.io/2019/05/prometheus-workshops-follow-up-frequently-asked-questions/

You should be careful with label and do not exaggerate with them:

    CAUTION: Remember that every unique combination of key-value label pairs
    represents a new time series, which can dramatically increase the amount of
    data stored. Do not use labels to store dimensions with high cardinality
    (many different label values), such as user IDs, email addresses, or other
    unbounded sets of values.
    Source: https://prometheus.io/docs/practices/naming/#labels

- [[https://prometheus.io/docs/practices/instrumentation/#do-not-overuse-labels]]

So the question is what is the right number for cardinality. It depends, but there
are some guidelines:

    As a general guideline, try to keep the cardinality of your metrics below 10,
    and for metrics that exceed that, aim to limit them to a handful across your
    whole system. The vast majority of your metrics should have no labels.
    If you have a metric that has a cardinality over 100 or the potential to
    grow that large, investigate alternate solutions such as reducing the number
    of dimensions or moving the analysis away from monitoring and to a
    general-purpose processing system.
    Source: https://prometheus.io/docs/practices/instrumentation/#do-not-overuse-labels

    For very simple applications with little logic that only do one thing, I'd
    expect on the order of 100 time series.
    [...]
    For complex applications with lots of moving parts, I'd expect on the order
    of 1000 time series.
    [...]
    When an application exposes more than that, getting up towards 10,000 time
    series that's an indication that you may have a cardinality issue and might
    want to cut back on labels a bit. This is however unavoidable sometimes for
    cases such as reverse proxies where there's many many backend services, or
    databases where there's many many tables and you need information for each.
    Be wary of going too much above 10,000, Prometheus is designed for many
    small scrapes, not a handful of massive ones.
    Source: https://www.robustperception.io/how-many-metrics-should-an-application-return

- [[https://www.robustperception.io/how-many-metrics-should-an-application-return]]

Some max cardinality from monitoring systems:

- Datadog : 1000 tag/dimension values per metric - specifically warning that exceeding this limit will incur performance penalties
- Datadog APM: Infinite cardinality (LOL)
- [[https://docs.newrelic.com/docs/agents/manage-apm-agents/agent-data/collect-custom-metrics][New relic]]: 2000
- SignalFx: 50000

Cardinality is the reason why
[[https://www.robustperception.io/why-are-prometheus-histograms-cumulative][histogram are cumulative]].

You can calculate how much memory they use for cardinality and ingestion here
[[https://www.robustperception.io/how-much-ram-does-prometheus-2-x-need-for-cardinality-and-ingestion]]

To be sure your application is not killed by an explosion of cardinality, you
can use a tool like
[[https://blog.freshtracks.io/bomb-squad-automatic-detection-and-suppression-of-prometheus-cardinality-explosions-62ca8e02fa32][bomb squad]].

- [[https://github.com/open-fresh/bomb-squad]]

or use sample limit [[https://www.robustperception.io/using-sample_limit-to-avoid-overload]].

- [[https://www.robustperception.io/using-tsdb-analyze-to-investigate-churn-and-cardinality]]

** Best practices

- Labels should rarely have more than 100 possible values
- The set of label values should be bounded (think of them as enums, and not open ended sets of strings)

* Profiling prometheus usage

- [[https://www.robustperception.io/optimising-go-allocations-using-pprof]] 2017
- [[https://www.robustperception.io/optimising-startup-time-of-prometheus-2-6-0-with-pprof]] 2019
- [[https://www.robustperception.io/optimising-prometheus-2-6-0-memory-usage-with-pprof]] 2019

* Prometheus on k8s

** Helm charts

There are three chart of interest to manage prometheus with operators:

- [[https://github.com/helm/charts/tree/master/stable/prometheus-operator]]
- [[https://github.com/coreos/prometheus-operator]] - DON'T USE
- [[https://github.com/coreos/kube-prometheus]] - ???

The correct prometheus operator chart to use it the one in charts/stable and
not the one on the coreos github account. The stable one is based on the one
from coreos.

You can install prometheus simply by running:

    helm install stable/prometheus

*** First installation

However there are some things to consider when running it for the first time.
First disable the TLS proxy:

    tlsProxy:
      enabled: false # NOTE: Default to true
      image:
        repository: squareup/ghostunnel
        tag: v1.4.1
        pullPolicy: IfNotPresent
      resources: {}

If you don't Prometheus installation will fail since it expects a secret
with the SSL cert already present on the system.

    8s          Warning   FailedMount         Pod          MountVolume.SetUp
    failed for volume "tls-proxy-secret" : secrets "prometheus-operator-admission" not found

- [[https://github.com/helm/charts/search?q=tls-proxy-secret&unscoped_q=tls-proxy-secret]]

Unless you plan to have that extra step first, set the enabled to false.

Also when you run the first time, you can run only prometheus and the operator
without any of the other functionalities.
To do so disable all the components:

    enabled: false

With the exception of:

- prometheusOperator
- prometheus

Also this fail in our setup:

    admissionWebhooks:
      failurePolicy: Fail
      # enabled: true
      enabled: false
