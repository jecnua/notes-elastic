CKA notes
|| Last update: 25 Oct 2018

* To clarify

- k8s ports
- k8s HA
- cgroup drivers
- no need for cni package
- https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/
- CRI https://github.com/kubernetes/community/blob/master/contributors/devel/container-runtime-interface.md
- cri-o
- vx-lan

* Kubernetes course: CKA certified kubernetes administrator

* Intro

.image images/k8s_high_level.png _ 800
.caption Source: [[https://www.safaribooksonline.com/videos/oscon-2017/][Kubernetes hands-on - Kelsey Hightower (Google) - Part 1]]

* The physical cluster

At high level there are two types of servers: _masters_ and _nodes_. In HA there
can be more than one master node but only one of them leader.

        For fault tolerance purposes, there can be more than one master node in
        the cluster. If we have more than one master node, they would be in a HA
        (High Availability) mode, and only one of them will be the leader,
        performing all the operations. The rest of the master nodes would be
        followers.
        Source: https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS158x+1T2018

The master node hosts the *control*plane*.
All master nodes connect to *etcd*.

** Nodes memory

Kubernetes master and node servers are expected to have *swap*disabled*.
This is the recommended deployment. If swap is not disabled, kubelet service
will not start unless a specific flag is passed at startup time.

To disable swap on a node:

    # As root
    swapoff -a

and then remove it from the _/etc/fstab_ file.

You could disable it by passing _--fail-swap-on=false_ but is not advised.

    The kubelet now fails if swap is enabled on a node. To override the default
    and run with /proc/swaps on, set --fail-swap-on=false. The experimental
    flag --experimental-fail-swap-on is deprecated in this release, and will be
    removed in a future release.
    Source: https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.8.md#before-upgrading

- [[https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/]]
- [[https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.8.md#before-upgrading]]
- [[https://kubernetes.io/docs/setup/independent/]]
- [[https://github.com/kubernetes/kubernetes/issues/53533]]

** Nodes components

Nodes will require three services:

- A container runtime
- kubelet
- kube-proxy

A forth one is optional and only for cloud environments:

- (beta) cloud controller manager

*** Node health

*kube-controller-manager* assigns CIDR blocks to a new node, keep track of the
nodes health and check with the cloud provider is the node is still there.

After _40s_ it will go in condition *unknown* if the heartbeat fails. After _5m_
it will start to evict pods. It will check the status every few seconds. There
is a flag called:

    --node-monitor-period duration     Default: 5s
    The period for syncing NodeStatus in NodeController.
    Source: https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/

It will also check what percentage of nodes in that zone is unhealty.
There is an unhealthy zone threshold and there are rules for each AZ.

    --unhealthy-zone-threshold float32     Default: 0.55
      Fraction of Nodes in a zone which needs to be not Ready (minimum 3)
      for zone to be treated as unhealthy.

The eviction rate is parametric. 0.1 per second means that no more than one pod
per node every 10s.

    --node-eviction-rate float32     Default: 0.1
      Number of nodes per second on which pods are deleted in case of node
      failure when a zone is healthy (see --unhealthy-zone-threshold for
      definition of healthy/unhealthy). Zone refers to entire cluster in
      non-multizone clusters.

The eviction rate is *reduced* to the secondary node.

    --secondary-node-eviction-rate float32     Default: 0.01
      Number of nodes per second on which pods are deleted in case of node failure
      when a zone is unhealthy (see --unhealthy-zone-threshold for definition of
      healthy/unhealthy). Zone refers to entire cluster in non-multizone clusters.
      This value is implicitly overridden to 0 if the cluster size is smaller
      than --large-cluster-size-threshold.

If you want to go multi-az you can shift the workload to a new zone if the first
one gets unhealthy. If the system sees ALL nodes as unhealthy it will
_stop_all_evictions_.

From 1.8+ the node controller can *taint* node to respect their status.

- [[https://kubernetes.io/docs/concepts/architecture/nodes/]]

*** (BETA) Cloud controller manager

NEEDS MORE RESOURCES

- [[https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/]]

Deamon that manages cloud related resources in a control loop.
It can implement:

- Node controller: Updated the nodes status by coordinating with the cloud API.
- Service controller: Listen to service create, udpate and delete and responsible for LoadBalancer types
- Route controller: Responsible for setting up network routes (only on gce)
- Persistent volume labels controller: Sets zone and region labels on PersistenVolumes

To run this beta functionality you will however need to:

- NOT specify the --cloud-provider flag in kube-apiserver and kube-controller-manaher
- Run kubelet with the flag --cloud-provider=external
- The kube-apiserver should not run the PersistentVolumeLabel admission controller
- Set the correct InitializerConfiguration to label persistent volumes

** Network and CNI

Kubernetes made some interesting choices on how networking should be implemented.
It defines the network model but leaves it to third party plugins a way to
implement it.

k8s specify that there shouldn't be any Network Address Translation (NAT) while
doing the Pod-to-Pod communication across hosts. To do so or you make toutable pods
and nodes like on google and amazon or you use a _Software_Defined_Networking_.
A CNI plugin is an *executable* invoked by the container management system.

- [[https://github.com/containernetworking/cni]]

    CNI (Container Network Interface), a Cloud Native Computing Foundation
    project, consists of a specification and libraries for writing plugins
    to configure network interfaces in Linux containers, along with a number
    of supported plugins. CNI concerns itself only with network connectivity of
    containers and removing allocated resources when the container is deleted.
    Because of this focus, CNI has a wide range of support and the specification
    is simple to implement.
    Source: https://github.com/containernetworking/cni

It's responsible for:

- Inserting the network interface (one end of a [[http://man7.org/linux/man-pages/man4/veth.4.html][vETH pair]]) in the container network namespace
- Making changes to the host: like attaching the other end of the vETH pair to the bridge network
- Assign the IP, put up the routes and apply IPAM rules depending on your topology

Some of the plugins that implement this functionalities are:

- [[https://coreos.com/flannel/docs/latest/]]
- [[https://github.com/projectcalico/calico-cni]]
- [[https://github.com/weaveworks/weave]]

    The container runtime offloads the IP assignment to CNI, which connects to
    the underlying configured plugin, like Bridge or MACvlan, to get the IP
    address. Once the IP address is given by the respective plugin, CNI forwards
    it back to the requested container runtime.
    Source: https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS158x+1T2018/

*** Flannel

When you configure flannel you will need to pass the CIDR block your k8s cluster
will need to use and assign it to the nodes.

You can define this CIDR with:

    --pod-network-cidr=10.244.0.0/16

Flannel makes use of an overlay network, usually
[[https://en.wikipedia.org/wiki/Virtual_Extensible_LAN][Virtual Extensible LAN (VXLAN)]].

- [[https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/]]

* Kubernetes master node and Control plane

.image images/arch.png _ 800
.caption Source: [[https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS158x+1T2018]]

The kuberneyes control plane consists of a collection of processes running on
the cluster. It's a collection of *three*processes* designed to run on a single node
in your cluster. They are:

- kube-apiserver: [[https://kubernetes.io/docs/admin/kube-apiserver/]]
- kube-controller-manager: [[https://kubernetes.io/docs/admin/kube-controller-manager/]]
- kube-scheduler: [[https://kubernetes.io/docs/admin/kube-scheduler/]]

Administrative tasks are made by communicating with the api-server. Commands are
send in *JSON* via *REST* and the api validates and process the requests. The result
is saved on etcd.

The scheduler schedule work to different nodes. It takes this decisions using a
whole lot of metrics it gets from the nodes plus the user requirements.
It schedules the work in terms of Pods and Services.

The controller-manager mages different non-terminating control loops. Each of this
manage a specific object and watch their current state though the API server.
If the state differs from the desired one it will take actions to change it.

The control plane maintains a record of all k8s objects and run a continous
control loop to manage the object state.

It will react to changes in the system and make sure the desired state match the
expected one.

** apiserver

The apiserver is the entrypoint to all k8s commands. As long as you have access
to the node and the right credentials, you can query the api.

The HTTP API space of k8s can be divided into three independent groups:

- Core group (/api/v1): basic objects
- Named group: Objects in the /apis/$NAME/$VERSION format
- System wide: Like /healthz, /logs, /metrics, etz

When "kubectl proxy" is configures, you can send requests to localhost on the
proxy port.

    Without kubectl proxy configured, we can get the Bearer Token using kubectl,
    and then send it with the API request. A Bearer Token is an access token
    which is generated by the authentication server (the API server on the
    master node) and given back to the client. Using that token, the client can
    connect back to the Kubernetes API server without providing further
    authentication details, and then, access resources.
    Source: https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS158x+1T2018/

* Kubernetes worker node

.image images/worker.png  _ 800
.caption Source: [[https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS158x+1T2018]]

Each individual _non-master_ node runs two processes:

- kubelet: [[https://kubernetes.io/docs/admin/kubelet/]] : Communicates with the controller node
- kube-proxy: [[https://kubernetes.io/docs/admin/kube-proxy/]] : A network proxy which reflects the kubernetes networking services on each node 

It will also need a container runtime like:

- [[http://cri-o.io/][cri-o]]
- [[https://containerd.io/][containerd]] (more notes on another page)
- [[https://coreos.com/rkt/][rkt]]
- [[https://linuxcontainers.org/lxd/][lxd]]

** Metadata

The *metadata* is what _uniquely_ identify a resource. All objects have unique:

- name: client, max 253 char, alphanumeric lower case, you can use - and .
- UID: generated by the control plane, spatially and temporary unique

- [[https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/]]

You can use either labels or annotation to attach metadata to k8s objects.

- *annotations* are *not* used to indentify or select objects
- *labels* can be used to select objects and to find collections of object that satisfy certain condition

They are both key value maps.

*** Annotations

- [[https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/]]

Annotations are used to attach *non-identifying*metadata* to object.
Tools can retrieve this metadata to do anything they need/want.

The metadata in an annotation can be small or large, structured or unstructured,
and can include characters not permitted by labels.

*** Labels and selectors

Labels are key-value pairs attached to objects.
They are used to specify identifying attributed that can be used to select a
workload.

* kubelet

The kubelet is an *agent* and runs as a *deamon* on every server (master and nodes).
It will communicate with the master node and runs containers as stated on the
saved state. It will also make sure all containers are healthy at all times.

The kubelet interfaces with the containers via the Container Runtime Interface (CRI).

** Container Runtime Interface (CRI)

- [[https://github.com/kubernetes/community/blob/master/contributors/devel/container-runtime-interface.md]]

.image images/cri.png  _ 800
.caption Source: [[https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/]]

*NOTE*: This image is not totally correct. You need a CRI shim only if your container
runtime is not CRI compatible.

Kubernetes 1.5 introduced an internal plugin API named Container Runtime
Interface (CRI) to provide easy access to different container runtimes. CRI
enables Kubernetes to use a variety of container runtimes without the need to
recompile. In theory, Kubernetes could use any container runtime that implements
CRI to manage pods, containers and container images.

    Abstracting the runtime interface
    One of the innovations taking advantage of this standardization
    is in the area of Kubernetes orchestration. As a big supporter
    of the Kubernetes effort, CoreOS submitted a bunch
    of patches to Kubernetes to add support for communicating
    and running containers via rkt in addition to the upstream
    docker engine. Google and upstream Kubernetes saw that
    adding these patches and possibly adding new container
    runtime interfaces in the future was going to complicate the
    Kubernetes code too much. The upstream Kubernetes team
    decided to implement an API protocol specification called the
    Container Runtime Interface (CRI). Then they would rework
    Kubernetes to call into CRI rather than to the Docker engine,
    so anyone who wants to build a container runtime interface
    could just implement the server side of the CRI and they
    could support Kubernetes. Upstream Kubernetes created a
    large test suite for CRI developers to test against to prove
    they could service Kubernetes.
    Source: open-source-yearbook-2017

    kubelet (grpc client) connects to the CRI shim (grpc server) to perform
    container and image operations.
    Source: https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS158x+1T2018/

CRI implements two services:

- ImageService: responsible for image related operations
- RuntimeService: responsible for pod and container-related operations

Some of this implementations are:

- OBSOLETE - [[https://github.com/kubernetes/kubernetes/tree/master/pkg/kubelet/dockershim][dockershim]]: it interfaces with docker and docker will use containerd in the backend
- OBSOLETE - [[https://github.com/containerd/cri][cri-containerd]]: k8s interfaces directly with containerd (obsolete)
- contained 1.1
- cri-o

.image images/multi-cri.png  _ 800
.caption Source: [[http://blog.realjf.com/archives/210]]

For more information about containerd look at the note page about it.

*** crictl

- [[https://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.md]]

Crictl is a CLI for all CRI container runtimes helpful for application and system
troubleshooting. The objective is to make it easy to use for debug and
development scenarios. It replaces the docker cli for debugging purposes.

For any CRI-compatible container runtime crictl is the reccomended tool.
crictl works consistently across all CRI-compatible container runtimes!

The scope of crictl is limited to troubleshooting.

A containerd namespace mechanism is employed to guarantee that Kubelet and
Docker Engine won’t see or have access to containers and images created
by each other. This makes sure they won’t interfere with each other.
Doker ps can't see k8s pods and k8s can't see docker pods.

*** cri-containerd (end of life)

- [[https://github.com/containerd/cri]]

.image images/cri-containerd.png _ 800
.caption Source: [[https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS158x+1T2018/]]

Since by default containerd (< 1.2) did not support CRI, engineers worked on
creating a CRI interface for containerd. This project allowed a kubernetes
cluster to use containerd as an underlying runtime without using docker.

.image images/cri-containerd-deep.png _ 800
.caption Source: [[https://kubernetes.io/blog/2017/11/containerd-container-runtime-options-kubernetes/]]

cri-containerd is an implementation of CRI for containerd. It operates on the same
node as the kubelet and the container runtime. This process (deamon) handles all the
requests from the kubelet and uses contained to manage the container lifecycle
in the backend.

Compared with the old docker CRI implementation it eliminates an extra hop in the
stack. cri-containerd manages the pod networking via CNI.

    Let’s use an example to demonstrate how cri-containerd works for the case
    when Kubelet creates a single-container pod:
    1.Kubelet calls cri-containerd, via the CRI runtime service API, to create
    a pod;
    2.cri-containerd uses containerd to create and start a special pause container
    (the sandbox container) and put that container inside the pod’s cgroups and
    namespace (steps omitted for brevity);
    3.cri-containerd configures the pod’s network namespace using CNI;
    4.Kubelet subsequently calls cri-containerd, via the CRI image service API,
    to pull the application container image;
    5.cri-containerd further uses containerd to pull the image if the image is
    not present on the node;
    6.Kubelet then calls cri-containerd, via the CRI runtime service API, to
    create and start the application container inside the pod using the pulled
    container image;
    7.cri-containerd finally calls containerd to create the application container,
    put it inside the pod’s cgroups and namespace, then to start the pod’s new
    application container. After these steps, a pod and its corresponding
    application container is created and running.
    Source: https://kubernetes.io/blog/2017/11/containerd-container-runtime-options-kubernetes/

However, cri-containerd and containerd 1.0 were still 2 different daemons which
interacted via grpc. The extra daemon in the loop made it more complex for users
to understand and deploy, and introduced unnecessary communication overhead.

Works with containerd 1.0.

*** ContainerD plugin (containerd 1.1+)

- [[https://github.com/containerd/cri/blob/release/1.11/docs/config.md]]

While cri-containerd as a step forward from dockershim, now that containerd
implements the CNI interface it's even better.

.image images/containerd-plug.png _ 800

    In containerd 1.1, the cri-containerd daemon is now refactored to be a containerd
    CRI plugin. The CRI plugin is built into containerd 1.1, and enabled by default.
    Unlike cri-containerd, the CRI plugin interacts with containerd through direct
    function calls. This new architecture makes the integration more stable and
    efficient, and eliminates another grpc hop in the stack. Users can now use
    Kubernetes with containerd 1.1 directly
    Source: https://kubernetes.io/blog/2018/05/24/kubernetes-containerd-integration-goes-ga/

The CRI GRPC interface listens on the same socket as the containerd GRPC
interface and runs in the same process.

You can configure to use [[https://katacontainers.io/][kata containers]].

Works with containerd 1.1 and kubernetes 1.10+.

*** CRI-O

- [[http://cri-o.io/]]
- [[https://github.com/kubernetes-sigs/cri-o]]

.image images/cri-o-arch.png _ 800
.caption Source: [[http://cri-o.io/]]

CRI-O works with any Open Container Initiative (OCI) compatible runtime (any one
that is compliant can be plugged in). It supports:

- runC
- ClearContainers

* Kubernetes objects

Kubernetes has a rich _object_model_. These objects represents different
_persistent_entities_ in the cluster.
When you work with k8s you use these api objects to describe the desired
cluster's state.
You can interact with the api via kubectl or the api directly from
inside/outside k8s.

Objects are abstractions that represents the state of your system.
Once an object is set, the control plane works to make sure the actual state
will match the desired state.

- [[https://kubernetes.io/docs/concepts/abstractions/overview/]]

There are some _basic_ k8s objects like:

- Pod [[https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/]]
- Service [[https://kubernetes.io/docs/concepts/services-networking/service/]]
- Volume [[https://kubernetes.io/docs/concepts/storage/volumes/]]
- Namespace [[https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/]]

And _high-level_abstraction_ called *controllers*.
Controllers are build upon the basic objects and provide additional
functionalities and convenient features. Among them:

- ReplicaSet [[https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/]]
- Deployment [[https://kubernetes.io/docs/concepts/workloads/controllers/deployment/]]
- StatefulSet [[https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/]]
- DaemonSet [[https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/]]
- Job [[https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/]]

** spec and status

Inside an object we declare our intent in the spec field. The actual state is
instead recorded by k8s control plane in the status field.
The two nested object fields are:

- object *spec*: This is your configuration defining the *desired* state. We provide it to k8s.
- object *status*: Supplied and updated by the control plane and represent the *actual* state.

k8s reacts to the _difference_ between state and status and tries to match them.

To create an object we provide the spec field to k8s API server together with
the apiVersion, kind and metadata.

The apiVersion specify the API endpoint we want to connect to. With the kind we
specify the object type. With the metadata we attach basic information to objects
like the name.

Once the object is created the control plane adds the status.

** Communication

We need to provide the spec field in json format.

When you use kubectl, the command converts the information from yaml to json
when making the API request.

** Namespace

Namespaces can be seen as virtual clusters. Provides a scope for names (names of
resources need to be unique within a namespace but not across) and allow
to divide the cluster in subsets of resources (*resource*quotas*). All objects
in the same namespace shares the access control policies.

By default you have three namespaces:

- default: Is a catch-all namespace for all objects that do not specify one.
- kube-system: Reserved to objects created by the kubernetes system.
- kube-public: Readable by all users even the non-authenticated.

The namespace is also part of the DNS name that will be created with a service.
When you try to do a dns resolution of a service with just the name, it will
resolve in the _local_ namespace.

    <service>

If you want to cross namespace you will need
to specify it.

    <service>.<namespace>

- [[https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/]]

** Pods

.image images/pod.png
.caption Source: [[https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS158x+1T2018/]]

Unit of deployment representing a single instance of an application. Is a logical
collection of one or more containers. They share:

- Same host
- Same natwork namespace
- Mount the same external storage (volumes)

Pods are *ephemeral*. They are not able to self-heal. It is up to the Controllers
to handle pod replication and fault tolerance.

You can attach the POD specification to another object using the Pod Template.

** Services

Instead of accessing the pods directly, a construct called _service_ is used.
A service groups pods using labels as selectors and loadbalance access to them.

*kube-proxy* is a *network*proxy* which runs on every worker node and listens
on the API for each service endpoint change. For each service it sets up a route
so that it can reach to it.

** Config map

Decouples the configuration from the app

* ETCD

etcd is a key-value store based on the Raft Consensus Algorithm. There are two
roles: master and follower. At any given time one of the nodes will be the
master while the others are followers.

In k8s etcd store the cluster state but also configuration details like
ConfigMaps and Secrets.

* Re/Sources (not already referenced directly)

- [[https://linuxacademy.com/linux/training/course/name/certified-kubernetes-administrator-preparation-course]]
- [[https://www.safaribooksonline.com/videos/oscon-2017/][Kubernetes hands-on - Kelsey Hightower (Google) - Part 1]]
- [[https://kubernetes.io/blog/2018/05/24/kubernetes-containerd-integration-goes-ga/]] - May 2018
- [[https://kubernetes.io/blog/2017/11/containerd-container-runtime-options-kubernetes/]] - Nov 2017
