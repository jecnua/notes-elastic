ElasticSearch [v1.7+]
|| Last update: 21 June 2018

* Nodes

A node is a running instance of elasticsearch.
Nodes can store data locally or not.
If you have set the app to store data, shards can and will be allocated locally.

** Data nodes

Holds the data in shards (which are lucene indexes).
Performs indexing and search queries.

    node.data = true
    node.masters = false

** Coordinating nodes (known before as client nodes)

Smart load balancers that expose a rest interface.
It is responsible for routing queries to nodes with relevant shards and aggregating results.
It is never rerouted to another client node.

The reason behind the creation of dedicated clients is (no data on them) is to dedicate all 
their computational power to the *scatter/gather* phase of the search.

    node.data = false
    node.masters = false

You can actually _shut_down_ the HTTP transport on the other nodes to make sure they don't 
reply HTTP at all (I didn't thought, I call them sometimes to check stuff).

    ## You can still communicate through the transport module (TCP)
    http.enabled = false

*NOTE*: It is reccomended to set a long lived http req to the client nodes.

** Master nodes

Lightweight operational (cluster management) responsability.
A master olds the _cluster_state_, and handles the shard distribution.
The master node *isn't* involved in any search or document change of any type.

    node.data = false
    node.masters = true

Is good practice to have a set of _dedicated_ master nodes.
Since they work through an election system you need an odd number of them (min 3).

** Nuts and bolts

By default, all nodes are data nodes. What you want is divide the
responsabilities. Separating concerns is good and let us optimise each type of
node for it's particular workload.

Client only nodes:

- https req parsing
- network overload (avoid)
- perform gather process

Data nodes only:

- search
- index

* Cluster

A cluster consists in one or more nodes with the same cluster names.

* Index

An index is just a *logical*namespace* that point to phisical shards.
Read requestes can be handled by a _primary_ or _replica_ shard. More copies
of the data you have, the more search throughput you can handle.

NOTE: Once all replica shards report success, success is reported to the user.

** Indexing

New documents to ES pass to primary shards, add to storage and add to inverted
index.

* Search

We don't know which document will match the query... it could be in any shard.
A search request cosults *a*copy*of*every*shard*. There are two phases:

- query (scatter and gather): in this phase replicas increase throughput
- fetch: the _coordinating_node_ multi-get all that does it need

The coordinating node perform a _merge_sort_ on the combined result from other
shards. Is CPU intensive and memory intensive.

* Scaling horizontally

- Routing
- Sharding
- Time/Pattern based index creation and query

* Performance

You can increase performances at the cost of data security:

- Replication sync (default) or async: Wait or not for hack from the replicas.
- Consistency: Is the quorum. How many shards need to report to the primary
before any write request is accepted (protects again split brain problems).

* AWS

- Same sg to all nodes (port 9300)
- Spot(?)

Use a small number of data nodes not on spot, to have safety of the data.
Use spot instances to boost performances.

* TO KNOW

Currently ingest node only executes ingest preprocessing on the index thread pool. For both index requests and bulk requests. This should be changed, if pipelines are specified on bulk requests the bulk TP should be used for ingest preprocessing.

* Road to ES 5.0

- [[https://www.elastic.co/guide/en/elasticsearch/reference/master/breaking-changes-5.0.html]] breaking-changes-5.0

No node.client!

- [[https://github.com/elastic/elasticsearch/pull/16963]]

Query cache settings deprecated! ?

- [[https://github.com/elastic/elasticsearch/pull/15592]]

- API for listing index file sizes
- Enable the indices request cache by default (when you do a req with size 0) [[https://github.com/elastic/elasticsearch/pull/16870]]
- Added ingest statistics to node stats API

The ingest stats include the following statistics:

    ingest.ingest_total- The total number of document ingested during the lifetime of this node
    ingest.ingest_time_in_millis - The total time spent on ingest preprocessing documents during the lifetime of this node
    ingest.ingest_current - The total number of documents currently being ingested.
    ingest.ingest_failed - The total number ingest preprocessing operations failed during the lifetime of this node
    Also the ingest stats contain a break down of the above stats on a per pipeline basis. These stats are automatically updated if pipelines are added or removed. This information is useful to give insight how much nodes spent on ingest related activities.

- Add ingest_took to bulk response
- Add ingest info to node info API, which contains a list of available processors
- Ingest: use bulk thread pool for bulk request processing (was index before)
- Enable acked indexing
- Add points to SegmentStats
- new jvm option files (jvm.options)
- Add GC overhead logging
- STATS: Add I/O statistics on Linux (WAO)

New: /_cluster/allocation/explain

- [[https://github.com/elastic/elasticsearch/pull/17305]]

** Test and fix

Check difference in settings between 2.4 and 5

- changed the version
- set the right amount of memory

[[https://www.elastic.co/guide/en/elasticsearch/reference/5.2/heap-size.html]]

    TESTED (OK)
    # docker logs 4454ad8a30e7 | grep 'heap size'
    [2017-02-02T15:56:40,739][INFO ][o.e.e.NodeEnvironment    ] [xxx] heap size [29.9gb], compressed ordinary object pointers [true]

TESTED (OK)

    'ES_JAVA_OPTS="-Xms30g -Xmx30g -XX:+UnlockDiagnosticVMOptions -XX:+PrintCompressedOopsMode"',
    # docker logs 4454ad8a30e7 | grep 'heap address'
    heap address: 0x0000000080000000, size: 30720 MB, Compressed Oops mode: Zero based, Oop shift amount: 3

FIX
using [1] data paths, mounts [[/usr/share/elasticsearch/data (/dev/xvda1)]], net usable_space [13gb], net total_space [15.6gb], spins? [possibly], types [ext4]

FIX (FIXED)
[2017-02-02T15:48:17,474][WARN ][o.e.d.p.d.e.Ec2DiscoveryPlugin] Using discovery.type setting to set hosts provider is deprecated. Set "discovery.zen.hosts_provider: ec2" instead

FIX
[2017-02-02T15:48:18,410][WARN ][o.e.d.s.g.GroovyScriptEngineService] [groovy] scripts are deprecated, use [painless] scripts instead

FIX
java.security.AccessControlException: access denied ("javax.management.MBeanServerPermission" "findMBeanServer")

FIX
[2017-02-02T15:49:30,754][ERROR][o.e.x.m.AgentService     ] [es5-data-qa-qa-10-96-32-225-1c69bbf5.ec2.internal] exception when exporting documents
org.elasticsearch.xpack.monitoring.exporter.ExportException: failed to flush export bulks


* Puppet

** Useful settings

    config => {
      'node.zone'                                       => $::ec2_placement_availability_zone,
      'node.data'                                       => ,
      'node.master'                                     => ,
      'cluster.routing.allocation.awareness.attributes' => 'zone', #This will force shard allocation for AZs
      'cloud.aws.region'                                => ,
      'discovery.type'                                  => 'ec2',
      'discovery.ec2.groups'                            => ,
      'discovery.ec2.tag.cluster'                       =>
      'discovery.ec2.tag.env'                           => ,
      'discovery.zen.minimum_master_nodes'              => ,
      'http.compression'                                => true,
      'network.host'                                    => '0.0.0.0',
      'transport.bind_host'                             => '0.0.0.0',
      'threadpool.bulk.queue_size'                      => '300',
      'index.unassigned.node_left.delayed_timeout'      => '5m',
      'indices.queries.cache.size'                      => '30%',
      'action.disable_close_all_indices'                => true,
      'action.disable_delete_all_indices'               => true,
      'action.disable_shutdown'                         => true,
      'action.destructive_requires_name'                => true,
    },

** Protections against destructive commands

In puppet:

    'action.disable_close_all_indices'                => true,
    'action.disable_delete_all_indices'               => true,
    'action.disable_shutdown'                         => true,
    'action.destructive_requires_name'                => true,

.link https://www.elastic.co/guide/en/elasticsearch/reference/2.3/_parameters.html action.destructive_requires_name

* Re/Sources

- [[https://www.npmjs.com/package/elasticdump2]]
- [[http://blog.kiyanpro.com/2016/03/11/elasticsearch/Export-Index-Using-Elasticsearch-Dump]]
- [[http://blog.tamasboros.com/export-elasticsearch-database-to-json-dumps/]]
- [[https://github.com/taskrabbit/elasticsearch-dump]]
- [[https://hub.docker.com/r/khanhicetea/elasticsearch-dump/~/dockerfile/]]
