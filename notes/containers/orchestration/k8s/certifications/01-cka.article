CKA notes
|| Last update: 5 Dec 2018

* To clarify

- k8s ports
- k8s HA
- cgroup drivers
- no need for cni package
- https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/
- CRI https://github.com/kubernetes/community/blob/master/contributors/devel/container-runtime-interface.md
- cri-o
- vx-lan
- externalIP
- env from
- headless service
- how can the pod see in the env pod name and other info
- downward api
- pod presets
- pod priority class
- Proportional scaling? for deployments?
- batch jobs???
- are pod ip in statefulset static?
- proxy-mode
- Search for QUESTION

* Kubernetes course: CKA certified kubernetes administrator

* Intro

.image images/k8s_high_level.png _ 700
.caption Source: [[https://www.safaribooksonline.com/videos/oscon-2017/][Kubernetes hands-on - Kelsey Hightower (Google) - Part 1]]

Kubernetes is all about decoupling services. Based on 15 years of experience at
google building Borg. Communication from outside or internal services is API
driven. All configuration is saved in JSON format but often users write it in
YAML. k8s will transform it in json before sending it to the API.

- [[https://ai.google/research/pubs/pub43438][Borg paper]]

Orchestration is managed by controllers (a series of watch-loops). All of them
interrogates the API server for a particular object state and act accordingly.

* The physical cluster

At high level there are two types of roles: _masters_ and _nodes_. In HA there
can be more than one master node but only one of them can be a _leader_.

        For fault tolerance purposes, there can be more than one master node in
        the cluster. If we have more than one master node, they would be in a HA
        (High Availability) mode, and only one of them will be the leader,
        performing all the operations. The rest of the master nodes would be
        followers.
        Source: https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS158x+1T2018

The master node hosts the *control*plane*.
All master nodes connect to *etcd*.

** Nodes memory

Kubernetes master and node servers are expected to have *swap*disabled*.
This is the recommended deployment. If swap is not disabled, kubelet service
will not start unless a specific flag is passed at startup time.

To disable swap on a node:

    # As root
    swapoff -a

and then remove it from the _/etc/fstab_ file.

You could disable this check by passing _--fail-swap-on=false_ but it's not
advised.

    The kubelet now fails if swap is enabled on a node. To override the default
    and run with /proc/swaps on, set --fail-swap-on=false.
    Source: https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.8.md#before-upgrading

- [[https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/]]
- [[https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.8.md#before-upgrading]]
- [[https://kubernetes.io/docs/setup/independent/]]
- [[https://github.com/kubernetes/kubernetes/issues/53533]]

** Nodes components

Nodes will require three services:

- A container runtime
- kubelet
- kube-proxy

A forth one is optional and only for cloud environments:

- (beta) cloud controller manager

*** Node health

*kube-controller-manager* assigns CIDR blocks to a new node, keep track of the
nodes health and check with the cloud provider is the node is still there.

After _40s_ it will go in condition *unknown* if the heartbeat fails. After _5m_
it will start to evict pods. It will check the status every few seconds. There
is a flag called:

    --node-monitor-period duration     Default: 5s
    The period for syncing NodeStatus in NodeController.
    Source: https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/

It will also check what percentage of nodes in that zone is unhealty.
There is an unhealthy zone threshold and there are rules for each AZ.

    --unhealthy-zone-threshold float32     Default: 0.55
      Fraction of Nodes in a zone which needs to be not Ready (minimum 3)
      for zone to be treated as unhealthy.

The eviction rate is parametric. 0.1 per second means that no more than one pod
per node every 10s.

    --node-eviction-rate float32     Default: 0.1
      Number of nodes per second on which pods are deleted in case of node
      failure when a zone is healthy (see --unhealthy-zone-threshold for
      definition of healthy/unhealthy). Zone refers to entire cluster in
      non-multizone clusters.

The eviction rate is *reduced* to the secondary node.

    --secondary-node-eviction-rate float32     Default: 0.01
      Number of nodes per second on which pods are deleted in case of node failure
      when a zone is unhealthy (see --unhealthy-zone-threshold for definition of
      healthy/unhealthy). Zone refers to entire cluster in non-multizone clusters.
      This value is implicitly overridden to 0 if the cluster size is smaller
      than --large-cluster-size-threshold.

If you want to go multi-az you can shift the workload to a new zone if the first
one gets unhealthy. If the system sees ALL nodes as unhealthy it will
_stop_all_evictions_.

From 1.8+ the node controller can *taint* node to respect their status.

- [[https://kubernetes.io/docs/concepts/architecture/nodes/]]

*** (BETA) Cloud controller manager

NEEDS MORE RESOURCES

- [[https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/]]

Daemon that manages cloud related resources in a control loop.
It can implement:

- Node controller: Updated the nodes status by coordinating with the cloud API.
- Service controller: Listen to service create, udpate and delete and responsible for LoadBalancer types
- Route controller: Responsible for setting up network routes (only on gce)
- Persistent volume labels controller: Sets zone and region labels on PersistenVolumes

To run this beta functionality you will need to:

- NOT specify the --cloud-provider flag in kube-apiserver and kube-controller-manaher
- Run kubelet with the flag --cloud-provider=external
- The kube-apiserver should not run the PersistentVolumeLabel admission controller
- Set the correct InitializerConfiguration to label persistent volumes

** Network and CNI

Kubernetes made some interesting choices on how networking should be implemented.
It defines the network model but leaves it to third party plugins a way to
implement it.

k8s specify that there shouldn't be any Network Address Translation (NAT) while
doing the Pod-to-Pod communication across hosts. To do so or you make toutable pods
and nodes like on google and amazon or you use a _Software_Defined_Networking_.
A CNI plugin is an *executable* invoked by the container management system.

- [[https://github.com/containernetworking/cni]]

    CNI (Container Network Interface), a Cloud Native Computing Foundation
    project, consists of a specification and libraries for writing plugins
    to configure network interfaces in Linux containers, along with a number
    of supported plugins. CNI concerns itself only with network connectivity of
    containers and removing allocated resources when the container is deleted.
    Because of this focus, CNI has a wide range of support and the specification
    is simple to implement.
    Source: https://github.com/containernetworking/cni

It's responsible for:

- Inserting the network interface (one end of a [[http://man7.org/linux/man-pages/man4/veth.4.html][vETH pair]]) in the container network namespace
- Making changes to the host: like attaching the other end of the vETH pair to the bridge network
- Assign the IP, put up the routes and apply IPAM rules depending on your topology

Some of the plugins that implement this functionalities are:

- [[https://coreos.com/flannel/docs/latest/]]
- [[https://github.com/projectcalico/calico-cni]]
- [[https://github.com/weaveworks/weave]]

    The container runtime offloads the IP assignment to CNI, which connects to
    the underlying configured plugin, like Bridge or MACvlan, to get the IP
    address. Once the IP address is given by the respective plugin, CNI forwards
    it back to the requested container runtime.
    Source: https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS158x+1T2018/

*** Flannel

- [[https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/]]

When you configure flannel you will need to pass the CIDR block your k8s cluster
will need to use and assign it to the nodes.

You can define this CIDR with:

    --pod-network-cidr=10.244.0.0/16

Flannel makes use of an overlay network, usually
[[https://en.wikipedia.org/wiki/Virtual_Extensible_LAN][Virtual Extensible LAN (VXLAN)]].

Flannel however is not able to enforce network policies like Calico and Weave Net.

* Kubernetes master node and Control plane

.image images/arch.png _ 800
.caption Source: [[https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS158x+1T2018]]

The kuberneyes control plane consists of a collection of processes running on
the cluster (daemon). It's a collection of *three*processes* designed to run on
a single node in your cluster. They are:

- kube-apiserver: [[https://kubernetes.io/docs/admin/kube-apiserver/]]
- kube-controller-manager: [[https://kubernetes.io/docs/admin/kube-controller-manager/]]
- kube-scheduler: [[https://kubernetes.io/docs/admin/kube-scheduler/]]

Administrative tasks are made by communicating with the *api-server*. Commands
are send in *JSON* via *REST* and the api validates and process the requests.
The result is saved on etcd. the api-server scale horizontally depending on the
traffic.

The *scheduler* schedule work to different nodes. It takes this decisions using
a whole lot of metrics it gets from the nodes plus the user requirements.
It schedules the work in terms of Pods and Services. It listens to the api-server
for the creation of new pods.

The *controller-manager* manages different non-terminating control loops.
Can be seen as a set of different master processes with the intent of matching
the desired state and actual state.
Each of this manage a specific object and watch their current state though the
API server. If the state differs from the desired one it will take actions to
change it.

It can be present in one of this two flavours:

- cloud-controller-manager (cloud env)
- kube-controller-manager (phisical)

Different controllers depending on the tasks:

- node controller
- replication controller
- route controller
- volume controller

Different controllers runs inside a cloud controller:

- node controller
- route controller
- service controller
- volume controller

The control plane maintains a record of all k8s objects and run a continuous
control loop to manage the object state.

It will react to changes in the system and make sure the desired state match the
expected one.

** kube-apiserver

- [[https://kubernetes.io/docs/admin/kube-apiserver/]]

Kubernetes exposes an API via the kube-apiserver.
The apiserver is the entrypoint to all k8s interactions. As long as you have
access to the node and the right credentials, you can query the api.

The HTTP API space of k8s can be divided into three independent groups:

- Core group (/api/v1): basic objects
- Named group: Objects in the /apis/$NAME/$VERSION format
- System wide: Like /healthz, /logs, /metrics, etz

The apiVersion field in the object's configuration file deifine the API
endpoint to connect the API server.

When "kubectl proxy" is configures, you can send requests to localhost on the
proxy port.

    Without kubectl proxy configured, we can get the Bearer Token using kubectl,
    and then send it with the API request. A Bearer Token is an access token
    which is generated by the authentication server (the API server on the
    master node) and given back to the client. Using that token, the client can
    connect back to the Kubernetes API server without providing further
    authentication details, and then, access resources.
    Source: https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS158x+1T2018/

* Kubernetes worker node

.image images/worker.png  _ 800
.caption Source: [[https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS158x+1T2018]]

We can see three major components.
Each individual _non-master_ node runs two processes:

- kubelet: [[https://kubernetes.io/docs/admin/kubelet/]] : Communicates with the api, run containers, manage resouces and watches over status.
- kube-proxy: [[https://kubernetes.io/docs/admin/kube-proxy/]] : A network proxy which reflects the kubernetes networking services on each node by managing network rules 

It will also need a container runtime like:

- [[http://cri-o.io/][cri-o]]
- [[https://containerd.io/][containerd]] (more notes on another page)
- [[https://coreos.com/rkt/][rkt]]
- [[https://linuxcontainers.org/lxd/][lxd]]

* Metadata

- [[https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/]]
- [[https://kubernetes.io/docs/concepts/overview/working-with-objects/names/]]
- [[https://github.com/kubernetes/community/blob/master/contributors/design-proposals/architecture/identifiers.md]]

The *metadata* is what _uniquely_ identify a resource. All objects have unique:

- UID: System generated
- Name: Client-given

    UID: A non-empty, opaque, system-generated value guaranteed to be unique in
    time and space; intended to distinguish between historical occurrences of
    similar entities.
    Source: https://github.com/kubernetes/community/blob/master/contributors/design-proposals/architecture/identifiers.md

    Name: A non-empty string guaranteed to be unique within a given scope at a
    particular time; used in resource URLs; provided by clients at creation time
    and encouraged to be human friendly; intended to facilitate creation
    idempotence and space-uniqueness of singleton objects, distinguish distinct
    entities, and reference particular entities across operations.
    Source: https://github.com/kubernetes/community/blob/master/contributors/design-proposals/architecture/identifiers.md

    Every object created over the whole lifetime of a Kubernetes cluster has a
    distinct UID. It is intended to distinguish between historical occurrences of
    similar entities.
    Source: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/

You can use either labels or annotation to attach metadata to k8s objects.

- *annotations* are *not* used to indentify or select objects
- *labels* can be used to select objects and to find collections of object that satisfy certain condition

They are both key value maps.

To show all the pods with a specific label run:

    kubectl get pods -L labelnameone,labelnametwo,etc

You can also use it as a selector:

    kubectl get pods -l labelnameone==thisvalue

** Annotations

- [[https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/]]

Annotations are used to attach arbitrary *non-identifying*metadata* to object
in a key-value format. Tools can retrieve this metadata to implement additional
functionality. They cannot be used by kubernetes commands.

The metadata in an annotation can be small or large, structured or unstructured,
and can include characters not permitted by labels. They can be larger than
labels.

An example of an annotation is a 'description' or git commit. Contents of the
annotation field is displayed when running the _describe_ command.

** Labels

- [[https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/]]

Labels are key-value pairs that can be attached to any _Object_. They are defined
under the metadata field.

    Labels are intended to be used to specify identifying attributes of objects
    that are meaningful and relevant to users.
    [...]
    We don’t want to pollute labels with non-identifying, especially large
    and/or structured, data.
    Source: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/

They can be used to organize and select a subset of objects.
They *do*not*provide*uniqueness* to objects. Each key needs to be unique for a
given object! Non-identifying information should be recorded using _annotations_.

Labels are generally used to map organizational/logic structures onto the system
objects. Allow cross-cutting operations (more flexyble than the hierarchical one).

    Valid label keys have two segments: an optional prefix and name, separated by a
    slash (/). The name segment is required and must be 63 characters or less.
    The prefix is optional. If specified, the prefix must be a DNS subdomain: a
    series of DNS labels separated by dots (.), not longer than 253 characters in
    total, followed by a slash (/). If the prefix is omitted, the label Key is
    presumed to be private to the user. The kubernetes.io/ prefix is reserved for
    Kubernetes core components.
    Valid label values must be 63 characters or less.
    Source: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/

Internally ks8 index and reverse-index labels for efficient queries and watches.

** Label selectors

- [[https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors]]

Label selectors allow to indentify a set of objects. It is considered the core
grouping primitive of k8s.

There are two types of _selectors_:

- Equality-based: Allow filtering of objects based on labels keys and values. Three kinds of operators are supported: =, ==, or !=
- Set-based: Allow filtering of objects based on a set of values. Three kinds of operators are supported: in, notin, and exist(only the key identifier)

    In the case of multiple requirements, all must be satisfied so the comma
    separator acts as a logical AND (&&) operator.
    Source: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors

Example Equality-based:

    environment=production,tier!=frontend

Example Set-based:

    environment in (production, qa)
    tier notin (frontend, backend)
    partition
    !partition
    partition in (customerA, customerB),environment!=qa

Very important:

    Note: the label selectors of two controllers must not overlap within a
    namespace, otherwise they will fight with each other.
    Source: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors

*Set*operators*are*more*expressive* and allow the *OR* operator.

    $ kubectl get pods -l environment=production,tier=frontend
    $ kubectl get pods -l 'environment in (production),tier in (frontend)'

Set operators are supported by the newer resources but not old ones like
ReplicationController.

* Cluster to/from Master communication

- [[https://kubernetes.io/docs/concepts/architecture/master-node-communication/]]

Communication from the cluster to the master is SSL encrypted.
The only service _exposed_ to external traffic is the api-server.
The API server listens on port 443 for HTTPS traffic. A form of AUTH should be
enabled.

    Nodes should be provisioned with the public root certificate for the cluster
    such that they can connect securely to the apiserver along with valid client
    credentials. For example, on a default GKE deployment, the client credentials
    provided to the kubelet are in the form of a client certificate.
    [...]
    Pods that wish to connect to the apiserver can do so securely by leveraging
    a service account so that Kubernetes will automatically inject the public
    root certificate and a valid bearer token into the pod when it is
    instantiated. The kubernetes service (in all namespaces) is configured with
    a virtual IP address that is redirected (via kube-proxy) to the HTTPS
    endpoint on the apiserver.
    Source: https://kubernetes.io/docs/concepts/architecture/master-node-communication/

Master to nodes is much less secure. There are two main types: apiserver to kubelet
and apiserver to nodes/pods/services.
For the cases when the apiserver needs to communicate with the cluster (kubelet)
like:

- Getting Logs
- Attaching to running pods
- Providing the port-forwarding

It is not secure. The certificate is not verified by default and is vulnerable to
man in the middle attach. Don't run them on public network. Possible solutions are:
use the flag --kubelet-certificate-authority or do an ssh tunnel.

Communications to  nodes/pods/services are totally unsafe because they are plain
http (neither authenticated or encrypted). Possible solution ssh tunneling.

** Certificates in kubernetes

- [[https://www.youtube.com/watch?v=gXz4cq3PKdg][Certifik8s: All You Need to Know About Certificates in Kubernetes [I] - Alexander Brand, Apprenda]]

Certificates allow party in a conversation to authenticate each other.
For example use a third party both can trust (CA). CA issue cert for all these
party.

API server is the only component that talks to ETCD.

All interaction with it needs to be secure.

1) We need to create a cluster certificate authority. It needs to be the
*trusted*root* throughout the cluster. All the cert in the cluster need to be
signed by this CA. This way went the receive a cert than can check it's signed
by the CA and trust it.

NOTE: For HA (multiple API server) you want to be sure the IPs of the LB and the
DNS is part of the certificate Subject Alternative Name (SAN) field. Otherwise
They are going to complain. Each master has it's own cert.

2) First thing to secure is the API server. We need to protect the https. Flag
are --tsl-cert-file and --tls-private-key-file.

3) Kubelet api (internal private api) needs to be secured. Kubelet exposes an
HTTPDS api. Consumed by API server to get logs, metrics, exec, etc.
Is also protected by authentication and authorization. Also client (the api
server) needs to have a client cert.

X.509 Client Cert Authentication

Is the strategy for authenticating requests that present a client certificate.
Used by k8s components but also end users. Any request that present a clietn
certificate signed by the Cluster CA is authenticated. User is obtained from
*Common*Name*(CN)* field. Groups are obtained from *Organization*field*.
User information and group memebership!

Each component will have a different cert because they will have a different
access to the cluster. So they have their own identify. Only kubelet have an
organization and the hostname is part of the certificate.
You want to make sure that each node (kubelet) in the cluster have it's own
identity. This different treatment will allow you to user specific things like
*Node*authorizationzer* and *Node*Restriction*Admission*Plugin*. Limit access to
the cluster from the kubelet. Without, the kubelet would be able to change any
resource of the cluster via API. So basically limits the read/write access to the
resources that are related to the node itself and pods bound to the node.

- Controller manager, common name: system:kube-controller-manager
- Scheduler, common name: system:kube-scheduler
- Kube Proxy, common name: system:kube-proxy
- Kubelet, common name: system:node:${hostname}, Organization: system:nodes

Options:

- Manually create it
- There is a certificate generation API

kubelet need a client cert to speak with the API and a cert for it's own api.
The kubelet CAN request a certificate as it starts up. This is build on top of
the Certficate API and bootstrap token authenticator.

- [[https://github.com/apprenda/kismatic]] To look at

*** certificates.k8s.io

- [[https://db-blog.web.cern.ch/blog/lukas-gedvilas/2018-02-creating-tls-certificates-using-kubernetes-api]]

The API is certificates.k8s.io/v1beta1. Clients create a ccertifiate signing
requesr and send it to the API. The requesting user is stored in the CSR.
They remain pending until approved by the admin. Cert is issued once the CSR
is approved.

The controller manager signs the cert so it needs access to the CA private key.

* The kubelet

- [[https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/]]

The kubelet is a node *agent* and runs as a *Daemon* on every server (master and
nodes). It thinks and works in term of _PodSpec_.
It will communicate with the master node and runs containers as stated on the
desired state. It will also make sure all containers are healthy at all times.
EXTREMELY configurable via flags, allows you to pass _feature_flags_
to allow ALPHA and BETA functionalities.

The kubelet by default serve on port 10255.

              TO CHECK
              When the kubelet starts, it will seek out the master node and try to listen (???).
              Reports success or failure to the master (???)
              TO CHECK

NOTE: The kubelet could also access PodSpec via FIle (path), HTTP endpoint and
HTTP server. However they are rarely used.

The kubelet interfaces with the containers via the Container Runtime Interface
(CRI).

** Container Runtime Interface (CRI)

- [[https://github.com/kubernetes/community/blob/master/contributors/devel/container-runtime-interface.md]]

.image images/cri.png  _ 800
.caption Source: [[https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/]]

*NOTE*: This image is not totally correct. You need a CRI shim only if your container
runtime is not CRI compatible.

Kubernetes 1.5 introduced an internal plugin API named Container Runtime
Interface (CRI) to provide easy access to different container runtimes. CRI
enables Kubernetes to use a variety of container runtimes without the need to
recompile. In theory, Kubernetes could use any container runtime that implements
CRI to manage pods, containers and container images.

    Abstracting the runtime interface
    One of the innovations taking advantage of this standardization
    is in the area of Kubernetes orchestration. As a big supporter
    of the Kubernetes effort, CoreOS submitted a bunch
    of patches to Kubernetes to add support for communicating
    and running containers via rkt in addition to the upstream
    docker engine. Google and upstream Kubernetes saw that
    adding these patches and possibly adding new container
    runtime interfaces in the future was going to complicate the
    Kubernetes code too much. The upstream Kubernetes team
    decided to implement an API protocol specification called the
    Container Runtime Interface (CRI). Then they would rework
    Kubernetes to call into CRI rather than to the Docker engine,
    so anyone who wants to build a container runtime interface
    could just implement the server side of the CRI and they
    could support Kubernetes. Upstream Kubernetes created a
    large test suite for CRI developers to test against to prove
    they could service Kubernetes.
    Source: open-source-yearbook-2017

    kubelet (grpc client) connects to the CRI shim (grpc server) to perform
    container and image operations.
    Source: https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS158x+1T2018/

CRI implements two services:

- ImageService: responsible for image related operations
- RuntimeService: responsible for pod and container-related operations

Some of this implementations are:

- OBSOLETE - [[https://github.com/kubernetes/kubernetes/tree/master/pkg/kubelet/dockershim][dockershim]]: it interfaces with docker and docker will use containerd in the backend
- OBSOLETE - [[https://github.com/containerd/cri][cri-containerd]]: k8s interfaces directly with containerd (obsolete)
- contained 1.1
- cri-o

.image images/multi-cri.png  _ 800
.caption Source: [[http://blog.realjf.com/archives/210]]

For more information about containerd look at the note page about it.

*** crictl

- [[https://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.md]]

Crictl is a CLI for all CRI container runtimes helpful for application and system
troubleshooting. The objective is to make it easy to use for debug and
development scenarios. It replaces the docker cli for debugging purposes.

For any CRI-compatible container runtime crictl is the reccomended tool.
crictl works consistently across all CRI-compatible container runtimes!

The scope of crictl is limited to troubleshooting.

A containerd namespace mechanism is employed to guarantee that Kubelet and
Docker Engine won’t see or have access to containers and images created
by each other. This makes sure they won’t interfere with each other.
Doker ps can't see k8s pods and k8s can't see docker pods.

*** cri-containerd (end of life)

- [[https://github.com/containerd/cri]]

.image images/cri-containerd.png _ 800
.caption Source: [[https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS158x+1T2018/]]

Since by default containerd (< 1.2) did not support CRI, engineers worked on
creating a CRI interface for containerd. This project allowed a kubernetes
cluster to use containerd as an underlying runtime without using docker.

.image images/cri-containerd-deep.png _ 800
.caption Source: [[https://kubernetes.io/blog/2017/11/containerd-container-runtime-options-kubernetes/]]

cri-containerd is an implementation of CRI for containerd. It operates on the same
node as the kubelet and the container runtime. This process (Daemon) handles all the
requests from the kubelet and uses contained to manage the container lifecycle
in the backend.

Compared with the old docker CRI implementation it eliminates an extra hop in the
stack. cri-containerd manages the pod networking via CNI.

    Let’s use an example to demonstrate how cri-containerd works for the case
    when Kubelet creates a single-container pod:
    1.Kubelet calls cri-containerd, via the CRI runtime service API, to create
    a pod;
    2.cri-containerd uses containerd to create and start a special pause container
    (the sandbox container) and put that container inside the pod’s cgroups and
    namespace (steps omitted for brevity);
    3.cri-containerd configures the pod’s network namespace using CNI;
    4.Kubelet subsequently calls cri-containerd, via the CRI image service API,
    to pull the application container image;
    5.cri-containerd further uses containerd to pull the image if the image is
    not present on the node;
    6.Kubelet then calls cri-containerd, via the CRI runtime service API, to
    create and start the application container inside the pod using the pulled
    container image;
    7.cri-containerd finally calls containerd to create the application container,
    put it inside the pod’s cgroups and namespace, then to start the pod’s new
    application container. After these steps, a pod and its corresponding
    application container is created and running.
    Source: https://kubernetes.io/blog/2017/11/containerd-container-runtime-options-kubernetes/

However, cri-containerd and containerd 1.0 were still 2 different daemons which
interacted via grpc. The extra daemon in the loop made it more complex for users
to understand and deploy, and introduced unnecessary communication overhead.

Works with containerd 1.0.

*** ContainerD plugin (containerd 1.1+)

- [[https://github.com/containerd/cri/blob/release/1.11/docs/config.md]]

While cri-containerd as a step forward from dockershim, now that containerd
implements the CNI interface it's even better.

.image images/containerd-plug.png _ 800

    In containerd 1.1, the cri-containerd daemon is now refactored to be a containerd
    CRI plugin. The CRI plugin is built into containerd 1.1, and enabled by default.
    Unlike cri-containerd, the CRI plugin interacts with containerd through direct
    function calls. This new architecture makes the integration more stable and
    efficient, and eliminates another grpc hop in the stack. Users can now use
    Kubernetes with containerd 1.1 directly
    Source: https://kubernetes.io/blog/2018/05/24/kubernetes-containerd-integration-goes-ga/

The CRI GRPC interface listens on the same socket as the containerd GRPC
interface and runs in the same process.

You can configure to use [[https://katacontainers.io/][kata containers]].

Works with containerd 1.1 and kubernetes 1.10+.

*** CRI-O

- [[http://cri-o.io/]]
- [[https://github.com/kubernetes-sigs/cri-o]]

.image images/cri-o-arch.png _ 800
.caption Source: [[http://cri-o.io/]]

CRI-O works with any Open Container Initiative (OCI) compatible runtime (any one
that is compliant can be plugged in). It supports:

- runC
- ClearContainers

* Kubernetes objects

Kubernetes has a rich _object_model_. These objects represents different
_persistent_entities_ in the cluster.
When you work with k8s you use these api objects to describe the desired
cluster's state.
You can interact with the api via kubectl or the api directly from
inside/outside k8s.

Objects are abstractions that represents the state of your system.
Once an object is set, the control plane works to make sure the actual state
will match the desired state.

- [[https://kubernetes.io/docs/concepts/abstractions/overview/]]

There are some _basic_ k8s objects like:

- Pod [[https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/]]
- Service [[https://kubernetes.io/docs/concepts/services-networking/service/]]
- Volume [[https://kubernetes.io/docs/concepts/storage/volumes/]]
- Namespace [[https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/]]

And _high-level_abstraction_ called *controllers*.
Controllers are build upon the basic objects and provide additional
functionalities and convenient features. Among them:

- ReplicaSet [[https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/]]
- Deployment [[https://kubernetes.io/docs/concepts/workloads/controllers/deployment/]]
- StatefulSet [[https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/]]
- DaemonSet [[https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/]]
- Job [[https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/]]

ReplicaSet add to the Pod the _scaling_ and _healing_ concept.
Deployments add _versioning_ and _rollbacks_.
Services adds a static (non-ephemereal) IP and networking.

** spec and status

Inside an object we declare our intent in the spec field. The actual state is
instead recorded by k8s control plane in the status field.
The two nested object fields are:

- object *spec*: This is your configuration defining the *desired* state. We provide it to k8s.
- object *status*: Supplied and updated by the control plane and represent the *actual* state.

k8s reacts to the _difference_ between state and status and tries to match them.

To create an object we provide the spec field to k8s API server together with
the apiVersion, kind and metadata.

The apiVersion specify the API endpoint we want to connect to. With the kind we
specify the object type. With the metadata we attach basic information to objects
like the name.

Once the object is created the control plane adds the status.

** Communication

We need to provide the spec field in json format.

When you use kubectl, the command converts the information from yaml to json
when making the API request.

** Namespaces

- [[https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/]]
- [[https://kubernetes.io/docs/tasks/administer-cluster/namespaces/]]

Namespaces can be seen as _virtual_clusters_. They Provide a scope for names
(names of resources need to be unique within a namespace but not across) and allow
to divide the cluster in subsets of resources (*resource*quotas*) (within the namespace).
All objects in the same namespace shares the access control policies.

By default you have three initial namespaces:

- default: Is a catch-all namespace for all objects that do not specify one.
- kube-system: Reserved to objects created by the kubernetes system.
- kube-public: Readable by all users even the non-authenticated. Usually used for special purposes like bootstrapping a cluster.

The namespace is also part of the DNS name that will be created with a service.
When you try to do a dns resolution of a service with just the name, it will
resolve in the _local_ namespace.

    <service>

If you want to cross namespace you will need
to specify it.

    <service>.<namespace>

Few things like nodes and persistenVolumes do not live in namespaces.

*** Resource quotas

- [[https://kubernetes.io/docs/concepts/policy/resource-quotas/]]

To limit the amount of resources an user can use in a namespace, Administrators
can create ResourceQuota objects. They allow you to manage:

- Compute Resource Quota: Limit the total sum of resources (CPU, memory, etc)
- Storage Resource Quota: Limit the total sum of storage resources that can be requested
- Object Count Quota: Restrict number of object of a specific type

** Pod

.image images/pod.png
.caption Source: [[https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS158x+1T2018/]]

Unit of deployment. Is a logical collection of one or more containers. They
share:

- Host (same node)
- Network namespace (IP address and ports)
- Mounts (they mount the same external storage - volumes)
- Namespace
- PID namespace??

Pods are *ephemeral*. They are not able to self-heal. It's up to _controllers_
to handle pod replication and fault tolerance.

You can attach the POD specification to another object using the Pod Template.

NOTE: The number of the *restarts* value is the number of container restarts not
pod restarts.

Pods allow to put resource limits.

- [[https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/]]

There are two:

- resources.requests: What you ask for.
- resources.limits: Your limit.

They both allow _cpu_ and _memory_ field. cpu is defined in _m_. 1000m is a vCPU.
So basically 250m is 1/4 of CPU and so on. memory is in Mi (megabytes). 64Mi means
64 megabytes.

The possible status phase of a pod are:

- Pending
- Running
- Succeeded
- Failed
- Unknown

Reastart policies are:

- Always
- On-failure
- Never

It's valid for all containers in a pod.

** Priority

Will impact the:

- Scheduling order
- Preemption: allows graceful termination period

Also pod distruption budget.

You need to create a priority object and add the class in the pod definition.

** Scheduler manipulation via selectors and affinity

*** Node selector

nodeSelector work by attaching labels to nodes and then using selectors in the
pod definition. However this choice is binding. If no nodes is available with
that specific flag, the workload won't be allocated.
Plus the nodes will need to have ALL the labels in the pod selectors.

*** Taints and Tolerance (WRITE BETTER)

- [[https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/]]

Nodes can have taints to discourage Pods assignment on that specific node.
They only way for a pod to the scheduled there is with tolerations.

    $ kubectl explain pods.spec.tolerations

if the key is correct and exists but different taint, no effect
if you use exists you don't need a value

    DESCRIPTION: If specified, the pod's tolerations. The pod this Toleration is
    attached to tolerates any taint that matches the triple <key,value,effect>
    using the matching operator <operator>.

With taints you mark a node and only pods that have that tolerance can be
assigned there.

NOTE: The node controller will add specific taint when some conditions occur
on nodes like:

- memory-pressure
- disk-pressure
- out-of-disk

When a node is tainted it won't remove the ones already running on that node.

*** Affinity and Anti-Affinity (BETA)

- [[https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity]]

Node Affinity/Anti-affinity: Steer the affinity towards a node. Is *soft* so it
will if it can. For anti-affinity for nodes you need to use *taints*.
To be surer a certain pod don't go to certain nodes you need to use taint.

Pod Affinity/Anti-affinity: Steer towards/away from another pod.

*** Probes

- [[https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes]]

There are two types of probes:

- Liveness
- Readiness

The *liveness*probe* checks on the application health. If the health check fails
k8s will restart the container. It is defined by:

- Liveness command: a command which return value will define the health
- Liveness HTTP request: and http request (you can define custom headers)
- TCP liveness probe

If it returns failure, the control plane will kill the container and the restart
policy will be applied. If this probe does not exists, the default value will be
success.

The usecase if kill and restart a container if the check fails.

Readiness probe is a check to define when a pod can start accepting requests.
A Pod that do not report ready status will not receive traffic from Kubernetes
Services.

If the readiness probe fails, the pod will be removed from the services to receive
traffic. Default is failure unless is not provided (if it's not provided it's
success).

Readiness is used to create containers that taken themselves down. Send traffic
only if this succeed.

Google in their example have two endpoint in the container:

    /started
    /healthz


** Downward API

- [[https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/]]

Is a type of volume.

This allows you to have all the labels and annotations (?? more ??) accessible
inside the pod. To do so you need to define them and each path you define in the
volume will be mapped in /etc/podinfo/<parh> inside the container.

all the element and annotation of the pod will have to me mounted in a file
called "path"

There are two files mounted on the pod:

- /etc/podinfo/labels
- /etc/podinfo/annotations

There will be lines inside each file and each line will be a label/annotation.

This is implemented via symlink. if you ls /etc/podinfo you can see them. the
files are actually symlink to some files with a generated name. k8s update this
file and change the symlink to update the file informations. this allow to
refresh on the fly.

** ReplicationControllers (OBSOLETE)

- [[https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/]]

Is a controller part of the controller manager and it makes sure that the
specified number of replicas for a Pod is running at any given point in time.

    NOTE: A Deployment that configures a ReplicaSet is now the recommended way
    to set up replication.
    Source: https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/

RC only supports equality-based selectors.

It also had some functionality to do rolling upgrades.

** ReplicaSet

- [[https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/]]

Is the next generation ReplicationController. Support both Equality-based and
Set-based selectors.
While they could be used independently, they are mostly used by Deployments to
orchestrate the Pod management.

NOTE: If you change the pod definition of a ReplicaSet nothing will happen. Only
when one of the Pod dies the new one (and only this new one) will be the new
version.

The RS add the concept of _number_of_replicas_ and _self_healing_ to pods.
Encapsulate a pod template definition inside their definition.
You can also think it gives you scaling but you need to change the number manually.

The RS selctors allows to match pods that the RS will work it. It allows loose
coupling between the two.

You can delete a rs without affecting the pod by using the command:

    kubectl delete --cascade=false

When you adopt pre-existing pods with a new rs, this won't be updated to the
new spec! they need to die first.

*** Horizontal pod autoscaler

- [[https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/]]

It's an object that select a ReplicSet. It can point to RC, RS or deployments.
It can scale on two policies:

- CPU Utilization
- Custom-metrics

To prevent trashing you can set a downscale delay and a upscale delay.
Define intervals between successive operations of the same kind.

** DaemonSets

Will run a pod (and only one) at all times on each node of the cluster.
You can however user label selector to restrict which nodes it will run on.

** StatefulSets

Is used to manage applications that requires an identity like name, network,
strict ordering, etc.
The StatefulSet controller provides identity and guaranteed ordering of
deployment and scaling to Pods.

These pods are not interchangeable. They have an _indentifier_ that maintains
across any rescheduling.

Ordered deployment, scaling, deletion, upgrade and termination. They are useful
with apps that needs persistent storage. Stable unique network identifier.
Storage MUST be persistant.

Deleting a statefuset won't delete the volume.

NUmbers from 0 to n-1 so they require a headless service.
They will be deleted from n-1 to 0.

** Deployments

- [[https://kubernetes.io/docs/concepts/workloads/controllers/deployment/]]

Deployments provide declarative updates to Pods and ReplicaSets (it manages Pods
via ReplicaSets). The DeploymentController is part of the master node's
controller manager.

Modifying the "Pod template" will trigger a *Deployment*rollout* while operations
like scaling the deployment do not trigger this type of event.

.image images/deployments.png
.caption Source: [[https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS158x+1T2018/]]

Changes like these will create a new revision of the deployment and they will
allow you to revert it via a *rollback* operation. However when you roll back
is the pod template part that is rolled back, not the other options like
the number of pods.

    kubectl rollout undo deployment <name>
    #or
    kubectl rollout undo deployment <name> --to-revision=2

Other advanced functionality of the deployments are:

- Autoscaling
- Proportional scaling
- Pausing and resuming (for example to fix bugs): When a deployment is paused changes to the pod spec do not cause a new deployment change
- Versioning
- Rollback
- Rolling deployments
- Blue-green
- Canary

NOTE: If you tell a new deployment to point with labels to an existing rs and
pods it will take control of the rs and will be able to manage it (scaling).
When you change the pod definition a new rs will be created with the correct
naming convention this time.

In API version apps/v1, a Deployment’s label selector is immutable after
it gets created.

You can pass:

    -- record

to create an history of all the commands you run on the cluster.

    kubectl rollout history deployment <name>

NOTE: BEWARE. You don't want the same pod to be managed by two deployments and
there is no k8s check to tell you that you are doing it.

Strategy for updates:

- Recreate: All pods will be killed before creating the new ones
- RollingUpdate: (default) You can set _maxUnavailable_ and _maxSurge_

Other optnions:

- progressDeadlineSeconds how long to wait before it is considered fails
- minReadySeconds hoe many seconds to wait running with no container crasdhing to condier it Ready
- rollbackTo
- revisionHistoryLimit (default to 10)
- paused

** Job & CronJob

- [[https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/]]

A job creates one or more Pods to perform a task. If a pod fails it will be
restarted. Once the task is over all the pods are terminated. Is a one-job
functionality.

There are three types of jobs:

- Non-parallel jobs: force one pod to run successfully
- Parallel jobs with fixed competition count: job completes when x amount of pods complete successsfully.
- Parallel jobs with work queue: required coordination

After completition the state is set to terminated the pods are not delete
(but not shown in get pods either). To show them you need to do:

    kubectl show pods -a
    # or
    kubectl get pods --show-all

You can set a deadline with the flag activeDeadlineSeconds field.

- [[https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/]]
- [[https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/]]

A cron job is similar with the difference that the Job will run always at a
specific time. Is at least once.

** Service

- [[https://kubernetes.io/docs/concepts/services-networking/service/]]

.image images/services.png _ 600
.caption Source: [[https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS158x+1T2018]]

Pods are ephemeral and resources like IP addresses allocated to it cannot be
static.

To solve this problem, instead of accessing the pods directly a construct called
_service_ is used.
A service groups pods using Labels and Selectors and load-balance traffic access
to them. The service lifespan is decoupled from the pods. Coupling is loose.
You can define the port the service is going to listen to and the _targetPort_
(the port the container is listening to). A tuple of Pods IP addresses along
with the targetPort is referred to a service *endpoint*.

    $ kubectl get endpoints --all-namespaces
    NAMESPACE     NAME                      ENDPOINTS             AGE
    default       kubernetes                192.168.39.107:8443   26s
    kube-system   default-http-backend      172.17.0.4:8080       16s
    kube-system   kube-controller-manager   <none>                24s
    kube-system   kube-dns                  <none>                18s
    kube-system   kube-scheduler            <none>                24s
    kube-system   kubernetes-dashboard      172.17.0.3:9090       16s

Each service is associated with a endpoint object. It is a dynamic list of all
the pods that are selected by a service label selector at any point in time.

An IP is attached to the service in a different CIDR of the Pods and it's called
ClusterIP.

QUESTION: How do you choose this two CIDR sets?

kube-proxy is a network proxy which runs on every worker node and listens
on the API server (only - no scheduler or controller manager) for each service
endpoint change. For each service it sets up an
iptable rule to capture the traffic for it's ClusterIP and forwards it to one
of the _endpoints_.

Some of the service type are:

- ClusterIP: (default) Static lifetime IP only accessible from within the cluster
- NodePort: The service is exposed on each node on a static port in the range 30000-32767
- LoadBalancer: Only works in a cloud env and will create a lb on the cloud provider, open a random port on every node and accept traffic
- ExternalName: Map service to an external endpoint residing outside the cluster. Has no selector and define no endpoint. It returns a CNAME of an external service.

ClusterIP is a way to associate a *Virtual*IP* (VIP) to a service. The kube-proxy
will manage the traffic. They represent a standard stable network overview of
the system.

- [[https://kubernetes.io/docs/tutorials/stateless-application/expose-external-ip-address/]]

You can define externalIPs in the spec of any type of service. If incoming network
traffic has this IP has the destination and arrives to one of the service ports
then it will be routed to one of the service endpoint.

NodePort created a ClusterIP anyway.
LoadBalancer will create a ClusterIP AND open a random node port to attach to the LB.

NOTE: Giving the _port_ a name is very useful because it decouples pod and service
definition. You will be able to change the port you are listening to in the pod
without changing the service if you keep the same name!

We can have a service for which Kubernetes does not provide a ClusterIP and we
call it [[https://kubernetes.io/docs/concepts/services-networking/service/#headless-services][headless]].

A service can expose multiple ports, but in this case they need to be named.

*** Kube proxy

- [[https://kubernetes.io/docs/concepts/services-networking/service/#the-gory-details-of-virtual-ips]]

proxy-mode userspace
proxy-mode iptables (default)
proxy-mode ipvs added (added in 1.8.0)

*** Service discovery

k8s supports two methods of discovering a service:

- DNS (recommended): The DNS add-on will create a DNS record for each Service
- Environment variable: As soon as the pods start, env variable with all the existing services are added. They are *not*updated* after pod creation.

The DNS name is in the form of:

    <servicename>.<namespace>.svc.cluster.local

In the same namespace services can be referred by the service name only. If you
are on a different namespace you need to define the namespace too.
For headless services, a DNS resolution will resolve all the pods ip (endpoint).

You can define the hostname and subdomain of your service in the Pod
definition. [[https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/]]

    apiVersion: v1
    kind: Pod
    metadata:
      name: busybox2
      labels:
        name: busybox
    spec:
      hostname: busybox-2
      subdomain: default-subdomain
      containers:
      - image: busybox
        command:
          - sleep
          - "3600"
        name: busybox

The pod DNS policy (how the pod access the world) can be:

- Default: Inherit the node settings
- ClusterFirst: If the dns does not conform to the cluster domain suffix, it will be resolved with the upstream name server inherited from the node
- ClusterFirstWithHostNet: (only with host network = true) App can see the NIC of the host machine
- None: Ignore DNS setting of the kubernetes env

The DNS add-on also support SRV queries. A DNS SRV will be created for all
named ports of a service.

    <portname>.<portprotocol>.<servicename>.<namespace>.svc.cluster.local

Given port name, the port number is returned.

** Ingress

- [[https://kubernetes.io/docs/concepts/services-networking/ingress/]]

    "An Ingress is a collection of rules that allow inbound connections to reach
    the cluster Services."
    Source: https://kubernetes.io

.image images/ingress.png
.caption Source: https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS158x+1T2018/

While _services_ manage the routing to access the pods, the *Ingress* allows you
to decouple routing rules to access the service from the application.
It allows *inbound* connection to reach the cluster _service_ and is implemented
via a Layer 7 HTTP load balancer called *ingress*controller*.
With ingress you don't connect directly to a Service. When you reach it, the
request is forwarded to the respective service.
You could manage the external access with services (NodePort, LoadBalancer, etc)
but the Ingress is an abstraction/alternative for this process.

Some of the functionality the Ingress add there are:

- TLS
- Name-based virtual hosting: Uses domain name to route the traffic
- Path-based routing: Fan out ingress rule
- Custom rules

An *ingress*controller* is an application that watches the API for changes in the
Ingress resources and updates the layer 7 load balancer. An example of
implementation is the
[[https://github.com/kubernetes/ingress-nginx/blob/master/README.md][nginx ingress controller]]

** Volume

A volume can be seen as a directory backed by a storage medium defined by the
_Volume_type_. A volume is attached to a pod and is shared by all the
containers in the pod. It has the same lifespan of the pod and outlives the
containers (survives containers restart).

They address the problem of:

- Permanence: Storage that lasts longer than lifetime
- Shared state: Multiple pod share state/files

Volumes in general survive single container restarts but if you want also to
survive a pod restart, you need to use a persistenVolumes that is decoupled
for the pod lifecyle.

Some of the volume types are:

- emptyDir (default): non-persistant: local to the pod. if the pod dies it's lost. survive container restart.
- hostPath: mount a dir on the node. is uncommon because pods should be independed from the node: usecase cAdvisor or similar daemon
- nfs
- iscsi
- persistentVolumeClaim
- configmap
- gitRepo
- secret

Some cloud version:

- gcePersistentDisk (gcp)
- awsElasticBlockStore (aws)
- azureDisk (azure)
- azureFIle (azure)

Each volume needs to be explicitly mounted and defined in the pod. It will
appear as a directory or file on the filesystem.

*** PersistentVolume & PersistentVolumeClaim

PV api provides a way for user or administrator to manage and consume storage.
To manage the volume it uses the PersistenVolume API.
To consume it uses the PersistentVolumeClaim API.

There are two types of provisioning:

- Static: preallocated by the admin
- Dynamic: via storage class

A PV is a network attached storage provisioned beforehand by an admin or user
and defined as a pool to k8s.
PV _can_ be dynamically provisioned based on the *storage*class* resource. A
storage class contains pre-defined provisioner and parameters to create a
persistent volume. Using PersistentVolumeClaims, a user sends the request for
dynamic PV creation, which gets wired to the StorageClass resource.

.image images/pvc.png _ 600
.caption Source: [[https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS158x+1T2018/]]

    A PersistentVolumeClaim (PVC) is a request for storage by a user. Users request
    for PersistentVolume resources based on size, access modes, etc. Once a suitable
    PersistentVolume is found, it is bound to a PersistentVolumeClaim.
    Source: https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS158x+1T2018/

While creating a PersistentVolumeClaim, a user can specify the size and access
mode.

*** CSI

CSI was created to standardize the Volume interface; a volume plugin built using
a standardized CSI would work on different container orchestrators.

Kubernetes 1.9 added alpha support for CSI
With CSI, third-party storage providers can develop solutions without the need
to add them into the core Kubernetes codebase.

** Config map

-[[https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/]]

Decouples the configuration details from the container image. It can be consumed
by a Pod or a controller.

Can be created from:

- literal values
- files

    kubectl create configmap my-config --from-literal=key1=value1 --from-literal=key2=value2

with 'file' here we are referring to the object yaml.

This information can be accessed in two ways.
From:

- environment
- volume: the config map can be mounted as volume and for each  key we will have a file

** Secrets

- [[https://kubernetes.io/docs/concepts/configuration/secret/]]

    With Secrets, we can share sensitive information like passwords, tokens, or
    keys in the form of key-value pairs, similar to ConfigMaps; thus, we can
    control how the information in a Secret is used, reducing the risk for
    accidental exposures. In Deployments or other system components, the Secret
    object is referenced, without exposing its content.
    It is important to keep in mind that the Secret data is stored as plain
    text inside etcd. Administrators must limit the access to the API server
    and etcd.
    Source: https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS158x+1T2018/

From literal:

    kubectl create secret generic my-password --from-literal=password=mysqlpassword

By default it will be of type opaque (in base64) if you try to read it with
describe. To read it just run base64 --decode.

NOTE: base64 encoding does not do any encryption!

You can create a secret form file just base64 it before.

* Authentication, Authorization and Admission Control

Each request goes through the following three stages:

- Authentication: Logs in as a user
- Authorization: Authorizes the API request added by the logged in user
- Admission control: Some additional module that can modify or reject the request based on additional checks.

** Lyfecycle hook

- [[https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/]]

Containers in a pod can be aware on which steps they are of they lifecycle.
There are two lifecyle hooks:

- PostStart
- PreStop

PostStart: Immediately after the container is created. No guaranteed it will be
run BEFORE the entrypoint. No parameters can be passed.

PreStop: Run immediately before the container terminates. Is *BLOCKING*. No
parameters can be passed.

They can be used by registering *hook*handlers*. They are:

- Exec
- HTTP

Hook delivery is *at*least*once* so you need to manage idempotency inside the
function.

** Init containers

- [[https://kubernetes.io/docs/concepts/workloads/pods/init-containers/]]

Run before the app containers starts.
Always run-to-completion
Run serially and only when the one before them returns successfully (if not it
will continuously restart the pod until it succeed)

It is defined in the pod spec.

They have a different namespace/isolation form the app containers. Among the usages:

- include utilities or setup
- block or delay the start of an app container

* Authentication

- [[https://kubernetes.io/docs/reference/access-authn-authz/authentication/#authentication-strategies]]

While there is no _user_ object, k8s have two kind of user:

- Normal User: Managed outside of kubernetes via independent services (like certificates)
- Service Accounts: In-cluster processes communicate with the API. They are usually created via API. They are tied to a Namespace and mount credentials as secrets.

In some cases also anonymous requests are allowed.

K8s have different authenticator modules. You can enable multiple and the first
one to successfully authenticate the request short circuit the others.

The most commonly used are probably:

- Client certificates
- Service account tokens: Uses signed bearer token to verify the requests.
- OpenID account tokens: Allow to connect to OAuth 2 providers

    Client Certificates
    To enable client certificate authentication, we need to reference a file
    containing one or more certificate authorities by passing the
    --client-ca-file=SOMEFILE option to the API server. The certificate
    authorities mentioned in the file would validate the client certificates
    presented to the API server.

Service account token get attached to Pods using the ServiceAccount Admission
Controller, which allows in-cluster processes to talk to the API server.

Admission controllers can be used to enforce resource quotas.

* Authorization

All API requests needs to be authorized. TO do so the request is validated by
multiple authorization modules. It the evaluation is successful the request
will be allowed. If not will be denied. All the authorizer will be run and as
soon as one fails the request return immediately as failed.
To authorized the call all need to pass.

Among the modules, the most common will be:

- Role-Based Access Control (RBAC) Authorizer

With RBAC we can regulate access to resources based on the role of users. When
a role is created, access to a resource can be restricted to operations (verbs).
There are two kinds of role:

- Role: Grant access to specific namespace
- ClusterRole: Grant permission cluster wide

Once the role is created it can be binded. There are two types of binding.

- RoleBinding: Bind user at the same namespace of the Role.
- ClusterRoleBinding: Grant access to resources in ALL namespaces

This authorizer is enabled by passing --authorization-mode=RBAC to the api
server.

* ETCD

etcd is a key-value store based on the Raft Consensus Algorithm. There are two
roles: master and follower. At any given time one of the nodes will be the
master while the others are followers.

In k8s etcd store the cluster state but also configuration details like
ConfigMaps and Secrets.

* Re/Sources (not already referenced directly)

- [[https://linuxacademy.com/linux/training/course/name/certified-kubernetes-administrator-preparation-course]]
- [[https://www.safaribooksonline.com/videos/oscon-2017/][Kubernetes hands-on - Kelsey Hightower (Google) - Part 1]]
- [[https://kubernetes.io/blog/2018/05/24/kubernetes-containerd-integration-goes-ga/]] - May 2018
- [[https://kubernetes.io/blog/2017/11/containerd-container-runtime-options-kubernetes/]] - Nov 2017
