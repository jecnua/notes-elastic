Certified Kubernetes Security Specialist (CKS) notes
|| Last update: 8 Apr 2021

* Kubernetes CKS notes

* Quick links

- [[https://training.linuxfoundation.org/certification/certified-kubernetes-security-specialist/]]
- [[https://docs.linuxfoundation.org/tc-docs/certification/important-instructions-cks]]
- [[https://github.com/cncf/curriculum]]

* Intro

Security is a spectrum and the best way to secure a system is to use a layered
security approach and defence in depth. Provide as many hurdles between threat
and assets and DO NOT try to be DRY. Good security is redundant and not DRY. Also
in general limit the attack surface and implement the principle of least priviledge.
The exam will be around multiple topics:

- Node/Host security
- Network security
- Runtime security
- K8s cluster security
- Application security

- Cluster hardening
- System hardening
- Dockerfile validation
- YAML security
- Image scan
- Microservice security
- Base image validation
- System call access control

** Tips for the command line

    kubectl run nginx --image=nginx --dry-run=client -o yaml > po.yaml
    kubectl create deploy test --image=nginx --replicas=3 --port 80 --namespace test --dry-run=client -o yaml
    kubectl create service nodeport my-ns --tcp=5678:8080 --dry-run=client -o yaml
    kubectl create secret generic passcode --from-literal=pass=test --dry-run=client -o yaml
    # And explain recursively
    kubectl explain pod --recursive | grep -A5 tolerations

* Cluster setup

** Network security policies

Restrict cluster level access.

** Certificates

- [[https://kubernetes.io/docs/setup/best-practices/certificates/]]
- [[https://kubernetes.io/docs/reference/setup-tools/kubeadm/implementation-details/]]

PKI - Public key infrastructure

Unless set otherwise, when creating a new cluster with kubeadm or any other
system a new Certificate Authority (CA) is created and used to sign all the
certificates used in the cluster.

If you install Kubernetes with kubeadm, certificates are stored in
/etc/kubernetes/pki.

The CA is:

- The trusted root of all certificates inside the cluster
- All cluster certificates are signed by the CA

This allow components to validate each other.

- [[https://kubernetes.io/docs/setup/best-practices/certificates/#how-certificates-are-used-by-your-cluster]]

Some components have only a server-certificate other have also a
client-certificate. For example:

    Client certificates for the kubelet to authenticate to the API server
    Server certificate for the API server endpoint
    Client certificates for administrators of the cluster to authenticate to the API server
    Client certificates for the API server to talk to the kubelets
    Client certificate for the API server to talk to etcd
    Client certificate/kubeconfig for the controller manager to talk to the API server
    Client certificate/kubeconfig for the scheduler to talk to the API server.
    Client and server certificates for the front-proxy
    Source: https://kubernetes.io/docs/setup/best-practices/certificates/#how-certificates-are-used-by-your-cluster


Basically the components that have server certificate are:

- Kubelet
- Apiserver
- ETCD (special case because it can have a different one than the main)

Example output form kubeadm:

    $ cat /var/log/cloud-init-output.log | grep 'certs'
    [certs] Using certificateDir folder "/etc/kubernetes/pki"
    I0106 08:22:59.895334    8106 certs.go:110] creating a new certificate authority for ca
    [certs] Generating "ca" certificate and key
    I0106 08:23:00.132988    8106 certs.go:474] validating certificate period for ca certificate
    [certs] Generating "apiserver" certificate and key
    [certs] apiserver serving cert is signed for DNS names [xxx.eu-west-1.compute.internal kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 x.x.x.x]
    [certs] Generating "apiserver-kubelet-client" certificate and key
    I0106 08:23:00.882216    8106 certs.go:110] creating a new certificate authority for front-proxy-ca
    [certs] Generating "front-proxy-ca" certificate and key
    I0106 08:23:01.575075    8106 certs.go:474] validating certificate period for front-proxy-ca certificate
    [certs] Generating "front-proxy-client" certificate and key
    I0106 08:23:01.763267    8106 certs.go:110] creating a new certificate authority for etcd-ca
    [certs] Generating "etcd/ca" certificate and key
    I0106 08:23:02.133316    8106 certs.go:474] validating certificate period for etcd/ca certificate
    [certs] Generating "etcd/server" certificate and key
    [certs] etcd/server serving cert is signed for DNS names [xxx.eu-west-1.compute.internal localhost] and IPs [x.x.x.x 127.0.0.1 ::1]
    [certs] Generating "etcd/peer" certificate and key
    [certs] etcd/peer serving cert is signed for DNS names [xxx.eu-west-1.compute.internal localhost] and IPs [x.x.x.x 127.0.0.1 ::1]
    [certs] Generating "etcd/healthcheck-client" certificate and key
    [certs] Generating "apiserver-etcd-client" certificate and key
    I0106 08:23:02.971597    8106 certs.go:76] creating new public/private key files for signing service account users
    [certs] Generating "sa" key and public key
    I0106 08:23:04.877808    8106 certs.go:474] validating certificate period for CA certificate
    I0106 08:23:04.877915    8106 manifests.go:109] [control-plane] adding volume "ca-certs" for component "kube-apiserver"
    I0106 08:23:04.877938    8106 manifests.go:109] [control-plane] adding volume "k8s-certs" for component "kube-apiserver"
    I0106 08:23:04.938474    8106 manifests.go:109] [control-plane] adding volume "ca-certs" for component "kube-controller-manager"
    I0106 08:23:04.947028    8106 manifests.go:109] [control-plane] adding volume "k8s-certs" for component "kube-controller-manager”

** Sharing certs with the nodes

- [[https://docs.google.com/document/d/1SP4P7LJWSA8vUXj27UvKdVEdhpo5Fp0QHNo4TXvLQbw/edit?ts=5971498a#heading=h.5bejulk96xxi]]
- [[https://github.com/kubernetes/kubernetes/pull/49520]]

* Secure the servers (control plane and nodes)

** CIS k8s benchmark

- [[https://www.cisecurity.org/benchmark/kubernetes/]]

CIS is a community driven non-profit that releases controls and benchmark.
Is globally recognised to define security good practice guidance and industry
accepted hardening procedures.

To pass the cert you need to be able to use the CIS benchmark to review the
security configuration of k8s components (both control plane and data plane),
among which:

- ETCD
- kubelet
- kubedns
- kubeapi

There are multiple way to run the CIS tests on your cluster.

- [[https://github.com/aquasecurity/kube-bench][kube-bench]] (by qua security) - go application
- [[https://github.com/dev-sec/cis-kubernetes-benchmark]]

The best one I found is kube-bench. It supports two types of runs, one for masters
and one for nodes.

    # Masters
    sudo docker run --pid=host -v /etc:/etc:ro -v /var:/var:ro \
    -t aquasec/kube-bench:latest master --version 1.19 > /home/ubuntu/cis-results.txt
    # Nodes
    sudo docker run --pid=host -v /etc:/etc:ro -v /var:/var:ro \
    -t aquasec/kube-bench:latest node --version 1.19 > /home/ubuntu/cis-results.txt

** kubeadm and CIS on the Controllers

Some things are not enabled by default when using kubeadm, like the auditlogs:

    [FAIL] 1.1.12 Ensure that the etcd data directory ownership is set to etcd:etcd (Automated)
    [FAIL] 1.1.19 Ensure that the Kubernetes PKI directory and file ownership is set to root:root (Automated)
    [FAIL] 1.2.6 Ensure that the --kubelet-certificate-authority argument is set as appropriate (Automated)
    [FAIL] 1.2.16 Ensure that the admission control plugin PodSecurityPolicy is set (Automated)
    [FAIL] 1.2.21 Ensure that the --profiling argument is set to false (Automated)
    [FAIL] 1.2.22 Ensure that the --audit-log-path argument is set (Automated)
    [FAIL] 1.2.23 Ensure that the --audit-log-maxage argument is set to 30 or as appropriate (Automated)
    [FAIL] 1.2.24 Ensure that the --audit-log-maxbackup argument is set to 10 or as appropriate (Automated)
    [FAIL] 1.2.25 Ensure that the --audit-log-maxsize argument is set to 100 or as appropriate (Automated)
    [FAIL] 1.3.2 Ensure that the --profiling argument is set to false (Automated)
    [FAIL] 1.3.6 Ensure that the RotateKubeletServerCertificate argument is set to true (Automated)
    [FAIL] 1.4.1 Ensure that the --profiling argument is set to false (Automated

You can fix some this way:

- [[https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/control-plane-flags/]]

You can also ensure that the --kubelet-certificate-authority argument is set
by adding:

    apiVersion: kubeadm.k8s.io/v1beta2
    kind: ClusterConfiguration
    kubernetesVersion: v${k8s_deb_package_version}
    apiServer:
      extraArgs:
        [...]
        kubelet-certificate-authority: "/etc/kubernetes/pki/ca.crt"

** kubeadm and CIS for the Nodes

There is only one error by default in 1.19 (with a kubeadm installation):

    [FAIL] 4.2.6 Ensure that the --protect-kernel-defaults argument is set to true (Automated)

    4.2.6 If using a Kubelet config file, edit the file to set protectKernelDefaults: true.
    If using command line arguments, edit the kubelet service file
    /etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and
    set the below parameter in KUBELET_SYSTEM_PODS_ARGS variable.
    --protect-kernel-defaults=true
    Based on your system, restart the kubelet service. For example:
    systemctl daemon-reload
    systemctl restart kubelet.service

Looks like the flag needs some settings to make it works.
However the setting belows are not needed on ubuntu 20.04:

    cat > /etc/sysctl.d/90-kubelet.conf << EOF
    vm.overcommit_memory=1 # https://sysctl-explorer.net/vm/overcommit_memory/
    kernel.panic=10 # https://sysctl-explorer.net/kernel/panic/
    kernel.panic_on_oops=1 # https://sysctl-explorer.net/kernel/panic_on_oops/
    EOF
    sysctl -p /etc/sysctl.d/90-kubelet.conf
    Source: https://github.com/kubernetes/kubernetes/issues/66241#issuecomment-460832038

The only thing you need to add is this flag at the end of the kubelet config:

    protectKernelDefaults: true

Could be usefult to take a look at the EKS AMI:

- [[https://github.com/awslabs/amazon-eks-ami/pull/392/files]]

To print the defaults:

    kubeadm config print init-defaults --component-configs KubeletConfiguration
    kubeadm config print join-defaults --component-configs KubeletConfiguration

** k8s CIS For EKS

For EKS AWS they have their own:

- [[https://aws.amazon.com/blogs/containers/introducing-cis-amazon-eks-benchmark/]]

They have their own PDF since is a managed cluster.

NOTE: On AWS, AWS Security Hub creates checks for things like this too.

** Deployment mechanism

There is no official helm chart available for kube-bench.

- [[https://github.com/aquasecurity/kube-bench/issues/225]]

The problem is the software is supposed to run on every k8s node you want to test
in a cron-like way. In k8s there is no way at the moment to run a cronjob as a
deamonset and is actually an existing pain-point for many other reasons.

- [[https://github.com/kubernetes/kubernetes/issues/64623#issuecomment-609875003]]

There are 4 main ways in which you may solve a problem like this with k8s:

- A special controller that creates pods for you when need
- DaemonSet with pod having a loop around the command and a sleep
- DaemonSet with pod running cron and your command (via cron)
- Static pods on nodes (still with a wait/cron)

They all have problems.

The first solution requires installing an additional controller, which you may
not want to maintain.

You can have a cronjob docker that runs the command at a specific time or between
sleeps, but it will always keep the resource occupied on the node while doing
nothing (pod resource/limits).

You could use a static pods (with a cron) if you want to change the user data
when the node is created (but is not flexible).

I think a better solution would be to use AWS SSM RunCommand to run the docker
image only on request on all nodes without using k8s. This would solve the issue
of requests/limits and the problem of access (the pod needs access to the
filesystem to run the tests).

This can be done with an AWS SSM Document like this:

    schemaVersion: "2.2"
    description: "Kube-bench run"
    mainSteps:
      - action: "aws:runShellScript"
        name: "kubebench"
        inputs:
          runCommand:
            - "docker run --rm --pid=host -v /etc:/etc:ro -v /var:/var:ro -t \
            aquasec/kube-bench:latest node --version 1.18 --json"

However there are some issues:

    The command output displays a maximum of 2500 characters. You can view the
    complete command output in either Amazon S3 or CloudWatch logs, if you
    specify an S3 bucket or a CloudWatch logs group when you run the command.
    Source: AWS Console

If you want the output in a searchable way you will probably need to use the
setting to save the file on s3, having a lambda that reads from there and ingest
in elasticsearch or somewhere else for display/alerts. This means changing the
role of ALL the nodes you need to run on to be able to read/write on that bucket.

It will require some changes that are not straightforward and requires time.

* Secure the Control plane

- [[https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/control-plane-flags/]]

And for each component:

- [[https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/]]
- [[https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/]]
- [[https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/]]

** Protect ETCD (https, encryption at rest)

- [[https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/]]

All traffic to ETCD should be encrypted via https. It should be encrypted at rest and its access
restricted.

    # as sudo
    cat /etc/kubernetes/manifests/kube-apiserver.yaml
    ETCDCTL_API=3 etcdctl --key=/etc/kubernetes/pki/apiserver-etcd-client.key \
        --cert=/etc/kubernetes/pki/apiserver-etcd-client.crt \
        --cacert=/etc/kubernetes/pki/etcd/ca.crt \
        endpoint status

This is on a standard kubeadm installation.

    ETCDCTL_API=3 etcdctl --key=/etc/kubernetes/pki/apiserver-etcd-client.key \
        --cert=/etc/kubernetes/pki/apiserver-etcd-client.crt \
        --cacert=/etc/kubernetes/pki/etcd/ca.crt \
        get / --prefix --keys-only | grep secret

So if you created:

Find it with:

    k get secrets my-secret -o jsonpath='{.data.key1}' | base64 --decode
    #
    ETCDCTL_API=3 etcdctl --key=/etc/kubernetes/pki/apiserver-etcd-client.key \
        --cert=/etc/kubernetes/pki/apiserver-etcd-client.crt \
        --cacert=/etc/kubernetes/pki/etcd/ca.crt get /registry/secrets/default/my-secret

Create the config file for the encryption:

    sudo mkdir /etc/kubernetes/etcd-encryption
    sudo nano /etc/kubernetes/etcd-encryption/etcd-enc.yaml
    head -c 32 /dev/urandom | base64
    sudo nano /etc/kubernetes/etcd-encryption/etcd-enc.yaml

NOTE: Remember to mount the volume in the apiserver manifest!

** Using a kms plugin

- [[https://kubernetes.io/docs/tasks/administer-cluster/kms-provider/]]

** Audit logs for the api server

- [[https://kubernetes.io/docs/tasks/debug-application-cluster/audit/]]

The api server can generate logs for each api call it receives. It is an intensive process so expect higher memory
consumption.
Each request can be recorded with an associated stage.

    [...] The defined stages are:
    RequestReceived - The stage for events generated as soon as the audit handler receives the request, and before it is delegated down the handler chain.
    ResponseStarted - Once the response headers are sent, but before the response body is sent. This stage is only generated for long-running requests (e.g. watch).
    ResponseComplete - The response body has been completed and no more bytes will be sent.
    Panic - Events generated when a panic occurred.
    Source: https://kubernetes.io/docs/tasks/debug-application-cluster/audit/

The Panic stage is only present in case of errors.
The levels of logging (audit levels) for each request at each stage are:

    None - don't log events that match this rule.
    Metadata - log request metadata (requesting user, timestamp, resource, verb, etc.) but not request or response body.
    Request - log event metadata and request body but not response body. This does not apply for non-resource requests.
    RequestResponse - log event metadata, request and response bodies. This does not apply for non-resource requests.
    Source: https://kubernetes.io/docs/tasks/debug-application-cluster/audit/

They are evaluated in order and the first matching rules sets the log level.
You can totally ignore a stage by setting omitStages:

    apiVersion: audit.k8s.io/v1
    kind: Policy
    omitStages:
      - "RequestReceived"

In the example above, omitStages=RequestReceived means it will not log watch events.
In the audit policy you pass to the api server you can define which of these stages to log and what object/verb
combination.

There are different audit backends:

- Json log files
- Webhook
- Dynamic backend (auditsink API) - DEPRECATED

** Enable audit logs

- [[https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#audit-policy]]

You can enable audit logs for kubernetes component via the kubeadm config file:

    apiVersion: kubeadm.k8s.io/v1beta2
    kind: ClusterConfiguration
    kubernetesVersion: v${kubeadm_install_version}
    apiServer:
        extraArgs:
            audit-log-path: "/var/log/kube-audit/audit-log.json"
            audit-log-maxage: "30"
            audit-log-maxbackup: "10"
            audit-log-maxsize: "100"
            audit-policy-file: "/etc/kubernetes/kube-audit/audit-policy.yaml"
        [...]
        extraVolumes:
            - name: "k8s-audit"
            hostPath: "/etc/kubernetes/kube-audit"
            mountPath: "/etc/kubernetes/kube-audit"
            readOnly: true
            pathType: DirectoryOrCreate
            - name: "k8s-audit-logs"
            hostPath: "/var/log/kube-audit/"
            mountPath: "/var/log/kube-audit/"
            readOnly: false
            pathType: DirectoryOrCreate

It is important to notice that the audit-policy file is REQUIRED or not log file will be created.
That file can be mounted are readOnly in the api-server container but the destination for the logs needs to be
mounted as "write" (in the case above /var/log/kube-audit/).

** Disable profiling for components

Disable profiling for scheduler, api-server and controller manager.
The profiling flag is defined as this:

    Enable profiling via web interface host:port/debug/pprof/

You can disable it via kubeadm:

    apiVersion: kubeadm.k8s.io/v1beta2
    kind: ClusterConfiguration
    kubernetesVersion: v${kubeadm_install_version}
    apiServer:
      extraArgs:
        profiling: "false"
    controllerManager:
      extraArgs:
        profiling: "false"
    scheduler:
      extraArgs:
        profiling: "false"

* Accounts

** Normal users

- [[https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/#normal-user]]

There is no k8s user resource.
A user is someone who holds a certificate and key. This way outside services can integrate
with k8s identity manager.

If it has a certificate, the certificate needs to be signed by ghe cluster certificate authority (CA).
Also the common name (/CN) needs to be set with the user name:

    /CN=andrea

First create a CSR certificate signing request.
Then you include it in a k8s resource
then k8s use this and it's CA to sign it
update the certifivcate signing request resource CRT
the user can download it and distribuite to users.

You can do it without the api if you have access to the CA.

The resource is called *CertificateSigningRequest*.

There is no way to invalidate a certificate is valid as long as the expiry date.

If a cert is leaked:

- Remove all access via RBAC
or
- Regen the CA and distribuite it

Process:

- create key
- create CSR
- API
- Download CRT from API
- User crtl + key

    kubectl config set-credentials john --client-key=john.key --client-certificate=john.crt --embed-certs=true

This:

    --embed-certs=true

Will include the certs in the kubeconfig instead of pointing to a file.

** Secrets

- [[https://kubernetes.io/docs/concepts/configuration/secret/]]

Get comfortable with creating secrets and mounting them via volume and ENV.

** Service accounts

- [[https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin]]
- [[https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account]]

Used by pods, managed by k8s. There is a service account resource in k8s.

Are namespaced, there is a default in every namespace
can be used go talk to the api

the service account mounts a secret and have a token that can be used to speak to the api

        ╰─ k get sa,secrets
        NAME                     SECRETS   AGE
        serviceaccount/default   1         39m

        NAME                         TYPE                                  DATA   AGE
        secret/default-token-plj26   kubernetes.io/service-account-token   3      39m
        ╭─  /p/tmp  ☸ kind-kind:default                                                                                  14:31:11 
        ╰─ k describe sa default
        Name:                default
        Namespace:           default
        Labels:              <none>
        Annotations:         <none>
        Image pull secrets:  <none>
        Mountable secrets:   default-token-plj26
        Tokens:              default-token-plj26
        Events:              <none>

you can create a service account in any namespace.

by default the secret of the sa is mounted on the pod

        cat /run/secrets/kubernetes.io/serviceaccount/token

And then curl:

        curl https://kubernetes.default -k -H "Authorization: Bearer <TOKEN>"

NOTE: Unless the pod needs to speak with the API server is more secure to disable the automaunt.

        apiVersion: v1
        kind: ServiceAccount
        metadata:
          name: build-robot
        automountServiceAccountToken: false
        Source: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account

The default service account have no permission.
better to create your own and make only the pod you need use it.

You can test the SA power via:

        k auth can-i delete pod --as system:serviceaccount:default:default

* Request workflow

- [[https://kubernetes.io/docs/concepts/security/controlling-access]]

Authentication -> Authorization -> Admission control

Authentication: Who are you?
Authorization: What can you do? For example: Node, RBAC

        --authorization-mode=Node,RBAC

Admission control: Does it respect all the constraints? For example OPA.

        --enable-admission-plugins=NodeRestriction

It is possible by default to have anonymous access.

** Restrict anonymous access

- [[https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/#kubelet-authentication]]

kube api server have a flag called:

    --anonymous-auth=true|false

Is enabled by default, however if you use rbac you need to explicit authorize
anonymous to do things.

        root@kind-control-plane:/# curl https://localhost:6443 -k
        {
          "kind": "Status",
          "apiVersion": "v1",
          "metadata": {

          },
          "status": "Failure",
          "message": "forbidden: User \"system:anonymous\" cannot get path \"/\"",
          "reason": "Forbidden",
          "details": {

          },
          "code": 403
         }

This mean it accepts them!

We need to enable anonymous auth needs to the liveness probes which.

NOTE: Close insecure port (since k8s 1.20 insecure port is not possible anymore)

Request sent to the insecure port will skip authentication and authorization.


Check the IP the cert supports.

In the kind docker container:

        # openssl x509 -in apiserver.crt --text | grep 'X509v3 Subject Alternative Name' -A1
                    X509v3 Subject Alternative Name:
                        DNS:kind-control-plane, DNS:kubernetes, DNS:kubernetes.default, DNS:kubernetes.default.svc, DNS:kubernetes.default.svc.cluster.local, DNS:localhost, IP Address:10.96.0.1, IP Address:172.18.0.2, IP Address:127.0.0.1

** Restrict access from nodes to the API

- [[https://kubernetes.io/docs/reference/access-authn-authz/node/]]

- NodeRestriction admission controlled

Limits the node labels a kubelet can modify
limits the kubelet can do, can only modify it's own
and modify labels on pods running on itself

ensure secure workload isolation via label.

You can label only yourself (node) and only label that don't start with:

    node-restriction.kubernetes.io/<xxx>=<yyy>

* Pod Security Policies (PSP) - Deprecated in 1.21

- [[https://kubernetes.io/docs/concepts/policy/pod-security-policy/]]
- [[https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/]]
- [[SIG Auth Update and Deep Dive][https://www.youtube.com/watch?v=SFtHRmPuhEw]] - 22 Nov 2019
- [[https://rancher.com/blog/2020/pod-security-policies-part-2]]

It limits what pods and containers are allowed to do.

    Pod Security Policies enable fine-grained authorization of pod creation and updates.
    [...]
    The PodSecurityPolicy objects define a set of conditions that a pod must run with in order to
    be accepted into the system, as well as defaults for the related fields.
    Source: https://kubernetes.io/docs/concepts/policy/pod-security-policy/

    First, one or more PodSecurityPolicy resources are created in a cluster to define the requirements Pods must meet.
    Then, RBAC rules are created to control which PodSecurityPolicy applies to a given pod. If a pod meets the
    requirements of its PSP, it will be admitted to the cluster as usual. In some cases, PSP can also modify Pod fields,
    effectively creating new defaults for those fields. If a Pod does not meet the PSP requirements, it is rejected, and
    cannot run.
    Source: https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/

The pod security policy functionality is deprecated in 1.21 but it's still part of the CKS exam.
With PSP you can amongst other things stop privilege escalation.

    When multiple PodSecurityPolicies are available, the admission controller uses the first policy that successfully
    validates. Policies are ordered alphabetically, and the controller prefers non-mutating policies (policies that
    don't change the Pod) over mutating policies.
    Source: https://cloud.google.com/kubernetes-engine/docs/how-to/pod-security-policies

It also controls:

- Access to host filesystems
- Usage of volume types
- AppArmor, sysctl, and seccomp profiles

** Usage

Pod security policy control is implemented as an admission controller.
It can be enabled by adding it in the list of admission controllers in the api server.

    PodSecurityPolicies are enforced by enabling the admission controller, but doing so without
    authorizing any policies will prevent any pods from being created in the cluster.
    Source: https://kubernetes.io/docs/concepts/policy/pod-security-policy/

You need to be careful to enable it, because AFTER you do it no new deployment is going to work without setting some
RBAC rules.

For new deployment to work the serviceaccount they use needs to have the RABC access to "use" the PSP policy.

    When a PodSecurityPolicy resource is created, it does nothing. In order to use it, the requesting user or target
    pod's service account must be authorized to use the policy, by allowing the use verb on the policy.
    [...]
    Since the pod security policy API (policy/v1beta1/podsecuritypolicy) is enabled independently of the admission
    controller, for existing clusters it is recommended that policies are added and authorized before enabling the
    admission controller.
    Source: https://kubernetes.io/docs/concepts/policy/pod-security-policy/

If you create a single pod via CLI that is going to work because YOUR USER have access to the PSP policy.

    k create role psp-read --verb=use --resource=podsecuritypolicies
    k create rolebinging psp-read --role=psp-read --serviceaccount=<namespace>:<name>

There is a specific order in which policies will be applied:

    1. PodSecurityPolicies which allow the pod as-is, without changing defaults or mutating the pod, are preferred.
    The order of these non-mutating PodSecurityPolicies doesn't matter.
    2. If the pod must be defaulted or mutated, the first PodSecurityPolicy (ordered by name) to allow the pod is selected.
    Source: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#policy-order

The least restrictive policy you can create is here:

- [[https://kubernetes.io/docs/concepts/policy/pod-security-policy/#example-policies]]

** Replacements

- [[https://kubernetes.io/docs/concepts/security/pod-security-standards/]]

The new KEP at [[https://github.com/kubernetes/enhancements/pull/2582]] or:

- OPA/Gatekeeper: [[https://github.com/open-policy-agent/gatekeeper]]
- K-rail: [[https://github.com/cruise-automation/k-rail]]
- Kyverno: [[https://github.com/kyverno/kyverno/]]

* mTLS

* OPA

* Threat detection

** Runtime security tools

Find and identify malicious process

** Others

Falco/Apparmor/Seccomp on other files. Look under containers/.

* Re/Sources

- [[https://k8sfiles.com/episodes/09-certifications/]]
- VIDEO [[https://www.youtube.com/watch?v=wqsUfvRyYpw][Kubernetes Security Best Practices - Ian Lewis, Google]] - Dec 2019
- VIDEO [[https://www.youtube.com/watch?v=gXz4cq3PKdg][Certifik8s: All You Need to Know About Certificates in Kubernetes [I] - Alexander Brand, Apprenda]] - Dec 2017


