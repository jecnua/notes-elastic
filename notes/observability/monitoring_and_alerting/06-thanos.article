Thanos - NEED CLEANING
|| Last update: 5 Oct 2020

* Intro

- [[https://thanos.io/]]
- [[https://github.com/thanos-io/thanos]]

Created at improbable is a way to extend prometheus and have long term storage
in s3 and single global query view on multiple prometheus.

* Global query view

    Prometheus encourages a functional sharding approach. Even single Prometheus
    server provides enough scalability to free users from the complexity of
    horizontal sharding in virtually all use cases.
    Source: https://improbable.io/blog/thanos-prometheus-at-scale

    With Thanos, on the other hand, you can query and aggregate data from
    multiple Prometheus servers, because all of them are available from a single
    endpoint.
    Source: https://improbable.io/blog/thanos-prometheus-at-scale

You can run more than one query component pods not to have a POF.

* Architecture

- [[https://quay.io/repository/thanos/thanos?tab=info]]
- [[https://fosdem.org/2019/schedule/event/thanos_transforming_prometheus_to_a_global_scale_in_a_seven_simple_steps/]] - 3 Feb 2019

.image images/thanos_components.jpg

- Sidecar: connects to Prometheus and exposes it for real time queries by the Query Gateway and/or upload its data to cloud storage for longer term usage
- Query: implements Prometheus API to aggregate data from the underlying components (such as Sidecar or Store Gateway)
- Query-frontend: Caching layer able to split and merge queries.
- Store: exposes the content of a cloud storage
- Compact: compacts and down-samples data stored in cloud storage
- Replicate: will copy data from one remote storage to another

** Sidecar

Thanos uses a system of sidecars. Sidecars are added a each prometheus pod and
allow to be queries by the Thanos query component.
The sidecar also watch for new files being created by prometheus and upload in a
storage system like s3.
This way and in combination with the store component, you can reduce the
retention of the prometheus node and keep it light.

** Store

The component *store* will read the data from s3 and cache it.
It exposes a store API and is treated like any other sidecar from the querier.
You can define an external cache for the store component like Memcached to go
around the issue of performance and HA (store component are stateless).
You can share chunk information between store component.

    Store Gateway knows how to deal with the data format of the Prometheus storage
    engine. Through smart query planning and by only caching the necessary index
    parts of blocks, it can reduce complex queries to a minimal amount of HTTP
    range requests against files in the object storage. This way it can reduce
    the number of naive requests by four to six orders of magnitude and achieve
    response times that are, in the big picture, hard to distinguish from queries
    against data on a local SSD.
    Source: https://improbable.io/blog/thanos-prometheus-at-scale

*** Historical data

    Prometheus 2.0 helps a lot in this area, as a total number of time series no
    longer impact overall server performance (See Fabian’s KubeCon keynote about
    Prometheus 2). Still, Prometheus stores metric data to its local disk. While
    highly-efficient data compression can get significant mileage out of a local
    SSD, there is ultimately a limit on how much historical data can be stored.
    Source: https://improbable.io/blog/thanos-prometheus-at-scale

*** Downsampling

    The usual solution to that problem is called downsampling, a process of
    reducing the sampling rate of the signal. With downsampled data, we can
    “zoom out” to a larger time range and maintain the same number of samples,
    thus keeping queries responsive.
    Downsampling old data is an inevitable requirement of any long-term storage
    solution and is beyond the scope of vanilla Prometheus.
    Source: https://improbable.io/blog/thanos-prometheus-at-scale

** Query frontend

- [[https://thanos.io/tip/components/query.md/]]

You need to run at least 2 pods for each Thanos frontend to achieve HA.
Also having a pod disruption budget set to 1 minimum pod will help.

** Compactor

Another component is the *compactor*, that will take the data from s3, compact it
(also doing downsampling) and push it to s3. It allows better performance.
This job can run as a cron. It is not required.

    To produce downsampled data, the Compactor continuously aggregates series
    down to five minute and one hour resolutions. For each raw chunk, encoded
    with TSDB’s XOR compression, it stores different types of aggregations, e.g.
    min, max, or sum in a single block. This allows Querier to automatically
    choose the aggregate that is appropriate for a given PromQL query.
    Source: https://improbable.io/blog/thanos-prometheus-at-scale

The *replicate* component will copy the data from one remote storage to another.
This can be used for GDPR or compliance reasons.

*** Compactor: Overlapping blocks issue

- [[https://github.com/thanos-io/thanos/blob/main/docs/operating/troubleshooting.md#overlaps]]

If in your bucket view the old blocks are not all compacted, maybe your compactor is not working correctly. One of the
possible issue is called overlapping_blocks and is cause by a misconfiguration in how the blocks were updated on s3.

In the compactor logs you may see an error like this:

    caller=compact.go:375 msg="critical error detected; halting" err="compaction: group 0@14895098461932485396: pre
    compaction overlap check: overlaps found while gathering blocks. [mint: 1584972000000, maxt: 1584979200000, range: 2h0m0s, blocks: 5]:
    [...]

You can verify the problem is present by exec-in the thanos bucket pod and run the following:

    # /bin/thanos tools bucket verify --objstore.config-file=/etc/config/object-store.yaml
    level=info ts=2021-03-02T12:22:23.37961041Z caller=main.go:138 msg="Tracing will be disabled"
    level=info ts=2021-03-02T12:22:23.3796658Z caller=factory.go:46 msg="loading bucket configuration"
    level=warn ts=2021-03-02T12:22:23.379966031Z caller=verify.go:81 msg="GLOBAL COMPACTOR SHOULD __NOT__ BE RUNNING ON THE SAME BUCKET" issues=2 repair=false
    [...]
    msg="found overlapped blocks" group=0@10844680014357932111 overlap
    [...]
    level=info ts=2021-03-02T12:23:25.983952976Z caller=verify.go:100 msg="verify completed" issues=2 repair=false

You can also test from your own machine by creating a config file first like the following (for example called o.yaml):

    type: S3
    config:
      bucket: my-thanos-bucket # Bucket name
      endpoint: s3.eu-west-1.amazonaws.com

And then running:

    # You need the thanos executable on your machine
    ./thanos tools bucket verify --objstore.config-file=o.yaml

There is no way to automatically resolve the issue since Thanos has no way to understand WHICH block to choose among
the overlapping or why the problem is present.

- [[https://github.com/thanos-io/thanos/issues/2178#issuecomment-591822797]]

The solution is to manually remove all but one of the overlapping blocks.
When choosing between overlapping blocks, I would prioritise in order:

- the one with higher resolution
- the one with more metrics in it

Before doing this type of operations the compactor needs to be disabled to avoid multiple processes manipulating the
same dataset.

** Receive and Rule components

- [[https://www.youtube.com/watch?v=Wroo1n5GWwg][Thanos: Cheap, Simple and Scalable Prometheus ]] 4 Sep 2020

- Receive
- Rule

** Sidecar

- [[https://thanos.io/components/sidecar.md/]]

NOTE: This is the only place where I found a guide on how to create the secret
[[http://polarpoint.io/index.php/2019/06/17/devops-global-monitoring-using-prometheus-and-thanos/]]

    [it] runs alongside each Prometheus container that together form a cluster.
    Instead of querying directly to the Prometheus (this is the official plural
    according to Prometheus) you query the Thanos Query component.
    Source: https://medium.com/uswitch-labs/making-prometheus-more-awesome-with-thanos-fbec8c6c28ad

You need to add a sidecar to each prometheus
you are adding a separate gprc service called store API on each sidecar
it is for matching the data
it access the Prometheus directly for local data
add a label to Prometheus called

    thanos-store-api=true

special service that will gather all the store api
a service that expose storeapi under same domain

add some flags to allow variable substitution and another container.

** Thanos Store

Thanos store is knows to be memory hungry.

- [[https://github.com/thanos-io/thanos/issues/448]]

If you need to check the usage of memory:

    container_memory_usage_bytes{namespace="monitoring",container="thanos-store”}

Using the experimental flag:

- [[https://github.com/thanos-io/thanos/blob/b3ba9a49138b4a5fafb452c935a55d9bce975e1c/cmd/thanos/store.go#L81]]

That is giving us a much lighter memory footprint.

In case you need to cleanup a lot of evicted pods ;)

    kubectl get pods | grep Evicted | awk '{print $1}' | xargs kubectl delete pod

** Integration with alert manager

All metrics needs to be *unique* so:

    external_labels:
      replica: ${POD_NAME} # new

also alerting you need to remove uniqueness
to allow to alert manager to decouple alert

alert relabel configs
regex replica
action labeldrop

*** Manage space

When using s3 bucket make sure to use a lifecyle policy to manage the blocks.

you can add here the config to use the bucket.
You need to DISABLE the compaction when you use an external bucket because
you will have an external compaction.

thanos sidecar synchs all xx
prometheus generate a file every two hours
and it will synch every two hours

** Querier

querier is stateless

then you need to add the Querierusing the store api
you need to add a special servuce to select the label we added before
this app will be set to discover local store with the dnssrv
and then take some remote store via IP and PORT

queries have the decuplication button to not show all labels
in new one version 0.3.0 the added the stores page in which the show all the
stores they have access to. they have a min and max time

thanos ueries have the same api as prometheus

** Thanos ruler

metamonitoring is important
monitor your monitoring

** Service discovery

- [[https://thanos.io/service-discovery.md/]]

*NOTE*: The gossip protocol has been
[[https://thanos.io/proposals/201809_gossip-removal.md/][deprecated]] in
[[https://github.com/thanos-io/thanos/releases/tag/v0.5.0][0.5.0]]!

Three options are now available:

- Static Flags
- [[https://thanos.io/service-discovery.md/#file-service-discovery][File SD]]
- [[https://thanos.io/service-discovery.md/#dns-service-discovery][DNS SD]]

* Helm chart

- [[https://github.com/banzaicloud/banzai-charts/tree/master/thanos]]

There are three chart of interest to manage prometheus with thanos:

- [[https://github.com/helm/charts/tree/master/stable/prometheus-operator]] - USE THIS ONE
- [[https://github.com/coreos/prometheus-operator]] - DON'T USE
- [[https://github.com/coreos/kube-prometheus]] - ???
The correct prometheus operator chart to use it the one in charts/stable and
not the one on the coreos github account. The stable one is based on the one
from coreos.

Create a file called create thanos-test.yaml


The create the secret:

    kubectl create secret generic thanos-storage-config \
      --from-file=thanos.yaml=PATH_TO_FILE/thanos-test.yaml --namespace monitoring

And check:

    k get secrets thanos-storage-config -o jsonpath='{.data.thanos\.yaml}' | base64 --decode

Now add the file to the prometheus operator:

    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#thanosspec
    thanos:         # add Thanos Sidecar
      image: quay.io/thanos/thanos:v0.7.0
      # version:
      # tag: v0.6.0
      # sha:
      # resource:
      objectStorageConfig: # blob storage to upload metrics
        key: thanos.yaml
        name: thanos-storage-config

NOTE: Beware. When you are no using dockerhub you can't use _tag_. Define the
version in the image line itself.

The real important piece of information here is the config file.

Inside the prometheus pod you will find now new file on disk:

    -rw-rw-r--    1 1000     2000         20001 Sep 16 10:44 queries.active
    drwxr-sr-x    3 1000     2000          4096 Sep 16 10:01 thanos
    -rw-r--r--    1 1000     2000            68 Sep 16 10:46 thanos.shipper.json
    drwxrwsr-x    4 1000     2000          4096 Sep 16 10:35 wal

And:

    /prometheus $ cat thanos.shipper.json
    {
            "version": 1,
            "uploaded": [
                    "01DMWMSNS0KTRM2HEQEKPY18HH"
            ]
    }

** PROB 1: Be aware of the Prometheus compactor

If you are adding Thanos to an existing prometheus the sidecar won't pick up the
old compacted data.

- [[https://github.com/thanos-io/thanos/issues/206][GITHUB ISSUE sidecar: Allow Thanos backup when local compaction is enabled]]

    Early on we hardcoded the sidecar to only upload blocks of comapction level
    0, i.e. those that never got compacted.
    With the garbage collection behavior the compactor has nowadays, it should
    be safe though to also upload historic data and potentially double-upload
    some data without lasting consequences. Just didn't get to changing the
    bahavior yet.
    Source: https://github.com/thanos-io/thanos/issues/206

You will need to edit the sidecar flags and add:

    --shipper.upload-compacted
    Source: https://github.com/thanos-io/thanos/issues/348#issuecomment-524289341

This is not written anywhere.
After it finished

    /prometheus $ ls -la  | grep 01D | grep -av '.tmp'
    drwxrwsr-x    3 1000     2000          4096 Sep  6 15:05 01DM3HNZ4101MQXD9D01ACHWZ3
    drwxrwsr-x    3 1000     2000          4096 Sep  7 09:59 01DM5JHHB4X62BRSJGM7VRCKV0
    drwxrwsr-x    3 1000     2000          4096 Sep  8 03:01 01DM7D1B3BD2GMW7R453PRTK5E
    drwxrwsr-x    3 1000     2000          4096 Sep  8 21:00 01DM9ATTQZWGCF7NKHNMA5PFG7
    drwxrwsr-x    3 1000     2000          4096 Sep  9 15:00 01DMB8MHDSQJ04NG726Q0WRFD1
    drwxrwsr-x    3 1000     2000          4096 Sep 10 09:00 01DMD6E0HA8Q062BGK6WXP9SWY
    drwxrwsr-x    3 1000     2000          4096 Sep 11 03:00 01DMF47GTP9EC7N7MNXRX8NYTM
    drwxrwsr-x    3 1000     2000          4096 Sep 11 21:00 01DMH2147ARS1AQ38J60F9EPVM
    drwxrwsr-x    3 1000     2000          4096 Sep 12 15:00 01DMJZTQB457QKN4KH3K3A30D4
    drwxrwsr-x    3 1000     2000          4096 Sep 13 09:00 01DMMXM5XSMP3V4JP06DE631E0
    drwxrwsr-x    3 1000     2000          4096 Sep 14 03:00 01DMPVDQJAZ3BQW0PAAZ30D9SQ
    drwxrwsr-x    3 1000     2000          4096 Sep 14 21:00 01DMRS78ECMKEWT368PSW47ZQZ
    drwxrwsr-x    3 1000     2000          4096 Sep 15 15:00 01DMTQ0SYD8RX0PBPXQPMP8JV4
    drwxrwsr-x    3 1000     2000          4096 Sep 16 09:00 01DMWMSNS0KTRM2HEQEKPY18HH
    drwxrwsr-x    3 1000     2000          4096 Sep 16 10:51 01DMWMTB37C5N1JK666FX05V1E
    drwxrwsr-x    3 1000     2000          4096 Sep 16 11:00 01DMWVND10T5K7Y6NHWT3V0T95
    /prometheus $ ls -la  | grep 01D | grep -av '.tmp' | wc -l
    16

And you see the right number of elements on s3, you can remove the flag.

next add banzaicloud to your repo:

    - name: banzaicloud-stable
      url: https://kubernetes-charts.banzaicloud.com

Exemple of something that does nothing!

      image:
        repository: quay.io/thanos/thanos
        tag: v0.7.0
      store:
        enabled: false
      query:
        enabled: false
      compact:
        enabled: false
      bucket:
        enabled: false
      sidecar:
        enabled: false
      objstore:
        type: s3
        config:
          bucket: prometheus-data-thanos-dev
          endpoint: s3.eu-west-1.amazonaws.com

You can use the thanos service in the chart to point to the operator.

Add sidecar to the other container

You will need to disable prometheus compaction before adding the sidecar with
shipping functionality:

    # In the prometheus chart (not operator)
    # Disable compaction
    extraArgs:
      storage.tsdb.min-block-duration: 2h
      storage.tsdb.max-block-duration: 2h

The best value is *2h*.

- [[https://github.com/coreos/prometheus-operator/issues/2196]]
- [[https://github.com/coreos/prometheus-operator/issues/1375]]

This example is obsolete:

- [[https://github.com/thanos-io/thanos/tree/master/tutorials/kubernetes-helm]]


- [[https://github.com/helm/charts/blob/c7e31361a28a10e30b4143b73439080d5e9f7d8b/stable/prometheus/templates/server-deployment.yaml#L138]]

    server:
      sidecarContainers:
        - args:
          - sidecar
          - --prometheus.url=http://127.0.0.1:9090/
          - --tsdb.path=/prometheus
          - --grpc-address=[$(POD_IP)]:10901
          - "--objstore.config={type: s3, config: {bucket: xyz, endpoint: s3.eu-west-1.amazonaws.com}}"
          - --log.level=debug
          - --log.format=logfmt
          - --shipper.upload-compacted # DISABLE PROMETHEUS COMPACTOR FIRST
          env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/thanos/thanos:v0.7.0
          imagePullPolicy: IfNotPresent
          name: thanos-sidecar
          ports:
          - containerPort: 10902
            name: http
            protocol: TCP
          - containerPort: 10901
            name: grpc
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /prometheus
            name: storage-volume

BE CAREFUL OF THE VOLUME WHERE THE DATA IS.

NOTE: If you not disable the compactor you give you an error:

- [[https://github.com/thanos-io/thanos/blob/a07e91e05583a5002118f243d3f69a26f7035939/cmd/thanos/sidecar.go#L55]]

known issue: store after the compactor runs did not see old historical data! restart it

- [[https://github.com/thanos-io/thanos/issues/1492]]
- [[https://github.com/thanos-io/thanos/issues/631]]

.image images/thanos_ha.png


** PROB 2: Labels

- [[https://github.com/thanos-io/thanos/blob/973876f170032ecd38e2ac88f05ab38e5ed7f78b/docs/getting-started.md#external-labels]]

    Every Prometheus instance must have a globally unique set of identifying labels.
    Source: https://github.com/thanos-io/thanos/blob/973876f170032ecd38e2ac88f05ab38e5ed7f78b/docs/getting-started.md#external-labels

If you don't the sidecar will reject Prometheus servers without any external
labels all together. Otherwise you will see this:

    prometheus-server-7764c6fb85-x9gzz thanos-sidecar level=error ts=xxx
    caller=main.go:213 msg="running command failed" err="no external labels
    configured on Prometheus server, uniquely identifying external labels must
    be configured"

Add global labels

    serverFiles:
      prometheus.yml:
        global:
          external_labels:
            environment: {{ .Environment.Values | getOrNil "envname" | default "UNDEFINED" }}
            region: {{ .Environment.Values | getOrNil "region" | default "UNDEFINED" }}
            cluster: {{ .Environment.Values | getOrNil "cluster" | default "UNDEFINED" }}
            account: {{ .Environment.Values | getOrNil "account" | default "UNDEFINED" }}

** Process to move from old Prometheus

- create thanos secret manually with bucket (for now manually)
- create a prometheus operator instance with sidecar
- install thanos components: enable all BUT NOT COMPACTOR

- add sidecar to existing prometheus
- upload all data (also old with flag)
- redeploy without flag

* Thanos and Istio

- [[https://github.com/coreos/prometheus-operator/issues/2435]]

When you run prometheus operator with thanos and istio sidecar the thanos app
will fail to connect to prometheus.
The reason is that the argument generated by prometheus-operator for thanos sidecar
is:

    --grpc-address=[$(POD_IP)]:10901

So it binds to the pod ip. However istio forwards traffic to localhost.

- [[https://github.com/coreos/prometheus-operator/pull/2728]]
- [[https://github.com/coreos/prometheus-operator/pull/2728/files]]

There is a solution however. You can set `ListenLocal` on the Thanos spec and
this will solve the problem of Istio's forwarding of traffic to localhost.


  | listenLocal | ListenLocal makes the Thanos sidecar listen on loopback, so
  that it does not bind against the Pod IP. | bool | false |

- [[https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#thanosspec]]
- [[https://github.com/helm/charts/blob/master/stable/prometheus-operator/values.yaml]]

I still don't test this approach however.

* Re/Sources

- [[https://medium.com/uswitch-labs/making-prometheus-more-awesome-with-thanos-fbec8c6c28ad]] - 21 Nov 2018
- [[https://github.com/AICoE/prometheus-anomaly-detector]]
