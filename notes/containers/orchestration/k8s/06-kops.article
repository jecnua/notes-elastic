KOPS
|| Last update: 21 Jan 2021

* Kops

- [[https://kops.sigs.k8s.io/]]
- [[https://github.com/kubernetes/kops][kubernetes/kops]]

* Kops-controller

- [[https://kops.sigs.k8s.io/architecture/kops-controller/]]

After version [[https://kops.sigs.k8s.io/releases/1.18-notes/][1.18]] in KOPS
they moved the nodes label management from kubelet to a new component called
kops-controller (runs as a DaemonSet on the master nodes).

- [[https://github.com/kubernetes/kops/blob/master/docs/architecture/kops-controller.md]]

    kops-controller runs as a DaemonSet on the master node(s). It is a
    kubebuilder controller that performs runtime reconciliation for kops.
    Source: https://github.com/kubernetes/kops/blob/master/docs/architecture/kops-controller.md

** Issue in AWS CHINA region

As part of his control loop, kops-controller tries to understand in which AWS
REGION he is running in (if in AWS at all) by using a mechanism defined here:

- [[https://github.com/kubernetes/kops/blob/a5fc8895dc9b710b97db02abb0d6761a65341bfb/util/pkg/vfs/s3context.go#L323]]

This mechanism however fails in the CHINA and GOV regions as seen in the issue
(fixed in 1.18.2):

- [[https://github.com/kubernetes/kops/issues/10012]]

If you tail the logs you will see the following:

    $ stern kops-controller- -n kube-system
    [...]
    error" "error"="unable to load cluster object for node
    ip-x-x-x-x.cn-northwest-1.compute.internal: error loading Cluster
    \"s3://xxx/yyy/cluster.spec\": Unable to list AWS regions: AuthFailure:
    AWS was not able to validate the provided access credentials\n\tstatus code:
    401, request id: xxx-xxx-xxx-xxx-xxx"  "controller"="node"
    "request"={"Namespace":"","Name":"ip-x-x-x-x.cn-northwest-1.compute.internal"}
    [...]
    unable to read /sys/devices/virtual/dmi/id/product_uuid, assuming not running
    on EC2: open /sys/devices/virtual/dmi/id/product_uuid: permission denied
    [...]
    defaulting region to "us-east-1"

As you can imagine defaulting on "us-east-1" is not a good idea.
You can temporarily/manually fix it by adding:

        env:
        - name: AWS_REGION
          value: cn-northwest-1

to the kops-controller deamonset.

* Best practices

- IAM policies in Terraform only and not in KOPS

We want to be able to modify the Instance role of the master/nodes without using
kops. By referring in KOPS just to the IAM role we are also able to attach this
role on multiple clusters (blue/green / recovery).
Having the role in terraform will also avoid hardcoding of resources and divide
the policy in manageable chunks.Â 

- Subnets, NAT and routing should be defined in Terraform and not left to KOPS

* VPC and Subnets

- [[https://github.com/kubernetes/kops/blob/master/docs/run_in_existing_vpc.md]]

** Tags

When we use kops to create Subnets, one of the tag added is:

    KubernetesCluster

This tag however is deprecated and should not be used if you are actually
sharing the subnets.

- [[https://github.com/kubernetes/kops/issues/5606#issuecomment-412915506]]
- [[https://github.com/kubernetes/kops/issues/4767]]

Another of this deprecated tags is:

    AssociatedNatgateway

Is added only on the utility subnets.

- [[https://github.com/kubernetes/kops/issues/3000][AssociatedNatgateway tag applied to utility subnets but not private ones #3000]]

Also, if you are sharing the subnets you will need to tag them as _shared_:

    // ResourceLifecycleOwned is the value we use when tagging resources to indicate
    // that the resource is considered owned and managed by the cluster,
    // and in particular that the lifecycle is tied to the lifecycle of the cluster.
    ResourceLifecycleOwned = "owned"
    // ResourceLifecycleShared is the value we use when tagging resources to indicate
    // that the resource is shared between multiple clusters, and should not be destroyed
    // if the cluster is destroyed.
    ResourceLifecycleShared = "shared"
    - [[https://github.com/kubernetes/kubernetes/blob/103e926604de6f79161b78af3e792d0ed282bc06/staging/src/k8s.io/legacy-cloud-providers/aws/tags.go#L51]]

- [[https://github.com/kubernetes/kops/blob/master/docs/run_in_existing_vpc.md#subnet-tags]]

Kops will add some tags to your subnets when it starts up.
This tags will be dependent on the name of the cluster and it will conflict with
the ones you created in terraform.

- [[https://github.com/hashicorp/terraform/blob/master/website/docs/configuration/resources.html.md#lifecycle-lifecycle-customizations]]

For example:

    ~ tags                            = {
          "Description"                                      = "dev k8s public subnet 0"
          "Name"                                             = "utility-cn-northwest-1a.k8s.public"
          "SubnetType"                                       = "Utility"
          "environment"                                      = "dev"
        - "kubernetes.io/cluster/k8s.kops-test.k8s.local"    = "shared" -> null
          "kubernetes.io/role/elb"                           = "1"
      }
    }

You can tell terraform to ignore tags with:

    lifecycle {
      ignore_changes = [
        tags,
      ]
    }

*NOTE*: For some reasons kops won't remove the tag it added from the subnet when you
destroy the cluster but it won't complain either when you create it again.

- [[https://github.com/terraform-providers/terraform-provider-aws/issues/5974]]
- [[https://github.com/kubernetes/kubernetes/issues/73190]]

* IAM

- [[https://github.com/kubernetes/kops/blob/master/docs/iam_roles.md]]
- [[https://github.com/kubernetes/kops/blob/master/docs/iam_roles.md#use-existing-aws-instance-profiles]]

It is a good ides to move the instance profile for master and nodes to terraform
to be able to be configurable and managed outside of kops.
This will avoid hardcoding of resources (the need to add ids and arn hardcoded
via kops) and it will make it easy to split the credentials by application.
When kops runs it will be able to tell you if there is any difference between
the role it was trying to generate and yours, and this will help with upgrades.

This old type of syntax will need to be removed:

    spec:
      iam:
        allowContainerRegistry: true
        legacy: false

To become:

    spec:
      iam:
        profile: arn:aws:iam::1234567890108:instance-profile/kops-custom-node-role

*NOTE*: This goes into EVERY instance group you create, with the exception of
the bastion.

In china assume policy changes:

- [[https://docs.amazonaws.cn/en_us/general/latest/gr/rande.html]]

    {
        "Version": "2012-10-17",
        "Statement": [
            {
                "Action": "sts:AssumeRole",
                "Principal": {
                   "Service": "ec2.amazonaws.com.cn" # HERE
                },
                "Effect": "Allow",
                "Sid": ""
            }
        ]
    }

It will e required when using external IAM profiles to use this flag with EVERY
update command:

    --lifecycle-overrides IAMRole=ExistsAndWarnIfChanges,IAMRolePolicy=ExistsAndWarnIfChanges,IAMInstanceProfileRole=ExistsAndWarnIfChanges

If you don't put the flags you will receive an error like this:

    W1017 15:38:57.876383   95664 executor.go:130] error running task "IAMInstanceProfileRole/dev-k8s-node-5kadxa" (9m45s remaining to succeed): error cr
    eating IAMInstanceProfileRole: LimitExceeded: Cannot exceed quota for InstanceSessionsPerInstanceProfile: 1

* ETCD backup

- [[https://github.com/kubernetes/kops/blob/master/docs/etcd/backup-restore.md]]
- [[https://hub.docker.com/u/kopeio]]

* Addons

- [[https://github.com/kubernetes/kops/blob/master/docs/addons.md]]

Addons are available but please don't use it.
They are not really manageable.

** AMIs

I tried to search for them

    aws ec2 describe-images --owners 383156758163 | ag '"Name": "k8s-1.14'
    "Name": "k8s-1.14-debian-stretch-amd64-hvm-ebs-2019-09-14",
    "Name": "k8s-1.14-debian-stretch-amd64-hvm-ebs-2019-09-26",
    "Name": "k8s-1.14-debian-stretch-amd64-hvm-ebs-2019-09-15",
    "Name": "k8s-1.14-debian-stretch-amd64-hvm-ebs-2019-08-16",
    "Name": "k8s-1.14-debian-stretch-amd64-hvm-ebs-2019-06-21",


* Networking

- https://github.com/kubernetes/kops/blob/master/docs/networking.md

This is where you can find the config to pass

- [[https://github.com/kubernetes/kops/blob/master/pkg/apis/kops/cluster.go#L145]]
- [[https://github.com/kubernetes/kops/blob/master/pkg/apis/kops/componentconfig.go#L248]]

* k8s in China

- [[https://github.com/kubernetes/kops/blob/master/docs/aws-china.md]]

** China mirrors

- [[https://github.com/Azure/container-service-for-azure-china/blob/master/aks/README.md#22-container-registry-proxy]]

Docker mirror only works for dockerhub images.

- [[https://docs.docker.com/engine/reference/commandline/dockerd/#daemon-configuration-file]]

Cri-o support instead mirrors for every type of hub:

- [[https://github.com/containerd/cri/issues/1138]]

    [plugins.cri.registry]
      [plugins.cri.registry.mirrors]
        [plugins.cri.registry.mirrors."gcr.io"]
          endpoint = ["https://gcr.azk8s.cn"]

With kops we can't use cri-o:

- ask for gvidor support - [[https://github.com/kubernetes/kops/issues/10625]] - re-opened
- ask for cri-o support - [[https://github.com/kubernetes/kops/issues/9712]] - open

** Alternatives

This one wants certificates:

- [[https://github.com/rpardini/docker-registry-proxy]]

without certs you will need to threat the repos as insecure

    {
      "insecure-registries": [
        "k8s.gcr.io",
        "quay.io",
        "gcr.io"
      ],
      "registry-mirrors": [
        "http://192.168.66.72:5000"
      ]
    }

** The strange case of dockerhub in China

There was an official dockerhub mirror in the past at registry.docker-cn.com
which however was silently removed some time ago. I could not find any
announcement about it, only the PR of the documentation part about it in github:

- [[https://github.com/docker/docker.github.io/issues/8793]]
- [[https://github.com/docker/docker.github.io/commit/a765e84efccb4ff028e8a6b79a02bff53f1dd949]]

The URL now just redirect to the main website outside china
https://docker-cn.com/registry-mirror.

    $ curl -ik https://www.docker-cn.com
    HTTP/1.1 200 Connection established

    HTTP/1.1 301 Moved Permanently
    Content-length: 0
    Location: https://www.docker.com/

So in KOPS when setting the registryMirrors option, it will need to point to a
non official mirror:

  docker:
    registryMirrors:
    - https://<your-proxy-or-mirror>

** Limitations from KOPS

You can't really modify the images used in addons or in many other parts of
the code. This really forces you along the way of the proxy.

* Manipulate user data

- [[https://github.com/kubernetes/kops/blob/master/docs/instance_groups.md#additional-user-data-for-cloud-init]]

* How to create addons

- [[https://kops.sigs.k8s.io/operations/addons/#custom-addons]]

* Future plans

KOPS is planning to move away from protokube to kubeadm but the ticket is
stuck for a long time now.

- [[https://github.com/kubernetes/kops/issues/1842]]
