EKS
|| Last update: 9 May 2019

* Intro

EKS is AWS managed k8s offer.

It has some integrations:

- Load balancing with ELB
- ECR for storing images
- s3 for helm repos

For Ingress you can use nginx:

- [[kubernetes.github.io/ingress-nginx/deploy/#aws]]

Updates are still problematic for nodes.

- You will need to update DNS plugin and worker nodes

The control plane on AWS is completely opaque.

NOTE: Azure k8s managed service does not support calico and network plugin is still preview

* Quickstart

They released a quickstart and these are the links:

- [[https://aws.amazon.com/quickstart/architecture/amazon-eks/]]
- [[https://github.com/aws-quickstart/quickstart-amazon-eks]]
- [[https://s3.amazonaws.com/aws-quickstart/quickstart-amazon-eks/doc/amazon-eks-architecture.pdf]]

* Architecture

- [[https://learn.hashicorp.com/terraform/aws/eks-intro]]

** VPC CNI

- [[https://aws.amazon.com/blogs/opensource/vpc-cni-plugin-v1-1-available/]]

VPC CIDR choice is:

- [[https://aws.amazon.com/about-aws/whats-new/2019/05/amazon-eks-adds-support-for-public-ip-addresses-within-cluster-v/][no longer]]

restricted to [[https://en.wikipedia.org/wiki/Private_network][RFC1918]] only.
Before you used to receive something like this:

    * aws_eks_cluster.demo: error creating EKS Cluster (terraform-eks-demo):
    InvalidParameterException: CIDR(s) attached to your VPC is invalid.
    Your VPC must have only RFC1918 CIDRs. Invalid CIDRs: [12.0.0.0/16]
    status code: 400, request id: xxx-xxx-xxx-xxx-xxx

The CIDR blocks supported by Amazon VPC are

- [[https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html#VPC_Sizing][here]].
- [[https://aws.amazon.com/about-aws/whats-new/2018/10/amazon-eks-now-supports-additional-vpc-cidr-blocks/][Latest addition]]:

- 100.64.0.0/10
- 198.19.0.0/16

* Deploy time

It takes 10 minutes to be deployed.

* Authentication

By default the way to use kubectl is to be authenticated with the help of a go
binary.

You need the following steps:

    go get -u -v github.com/kubernetes-sigs/aws-iam-authenticator/cmd/aws-iam-authenticator

Beware in some tutorial you may find the old link to the plugin, but this one
doesn't work:

    go get -u -v github.com/heptio/authenticator/cmd/heptio-authenticator-aws

* Usage

** Service "LoadBalancer"

On EKS it will create an old ELB!

** Ingress

- [[https://kubernetes-sigs.github.io/aws-alb-ingress-controller/]]
- [[https://aws.amazon.com/blogs/opensource/kubernetes-ingress-aws-alb-ingress-controller/]] - 20 NOV 2018

    This project was originated by Ticketmaster and CoreOS as part of
    Ticketmaster's move to AWS and CoreOS Tectonic.
    [...]
    This project was donated to Kubernetes SIG-AWS to allow AWS, CoreOS,
    Ticketmaster and other SIG-AWS contributors to officially maintain the
    project. SIG-AWS reached this consensus on June 1, 2018.
    Source: https://kubernetes-sigs.github.io/aws-alb-ingress-controller/

Make sure the _nodes_ have the right credentials. This is something is not in
the normal EKS, so they need to be added after.

- [[https://kubernetes-sigs.github.io/aws-alb-ingress-controller/examples/iam-policy.json]]

Have subnets tagged correctly.Subnets tagged with:

    kubernetes.io/cluster/clusterName: owned
    or
    kubernetes.io/cluster/clusterName: shared

where clusterName is what you specified in controller flags via:

    --cluster-name=clusterName

Plus you need to specify which are the subnets for the internal subnet and
external subnet. Tags need to have value 1 and not "true".

- [[https://kubernetes-sigs.github.io/aws-alb-ingress-controller/guide/controller/config/#subnet-auto-discovery]]

Tag the subnets.

You need to add kubernetes.io/role/elb tag onto public subnets instead of
kubernetes.io/role/internal-elb to let controller auto-discovery it for
internet-facing ingresses.

To enable an ALB ingress on EKS you need to download and use the templates
for the ingress-controller and RBAC.

- [[https://kubernetes-sigs.github.io/aws-alb-ingress-controller/guide/controller/setup/]]

NOTE: You will need to change the cluster name inside the ingress-controller
template.


QUESTIONS:

- Why the ALB have the name of the first ingress?
- What about route53?
- What happens if i spin the cluster in a private subnet?

*** Possible misconfiguration

    E0305 21:14:12.385165 1 :0] kubebuilder/controller "msg"="Reconciler error"
    "error"="failed to build LoadBalancer configuration due to retrieval of
    subnets failed to resolve 2 qualified subnets. Subnets must contain the
    kubernetes.io/cluster/\u003ccluster name\u003e tag with a value of shared or
    owned and the kubernetes.io/role/internal-elb tag signifying it should be
    used for ALBs Additionally, there must be at least 2 subnets with unique
    availability zones as required by ALBs. Either tag subnets to meet this
    requirement or use the subnets annotation on the ingress resource to
    explicitly call out what subnets to use for ALB creation. The subnets that
    did resolve were []" "Controller"="alb-ingress-controller"...
    Sorce: https://github.com/kubernetes-sigs/aws-alb-ingress-controller/issues/886

You probably can't connect

** Events

The only events you can see from the hidden stuff is the leader election.

    $ kubectl --kubeconfig kube.conf get events --all-namespaces
    NAMESPACE      NAME                                         KIND         REASON              SOURCE                  MESSAGE
    kube-system   kube-scheduler.154babff8545dbc8              Endpoints     LeaderElection      default-scheduler       ip-10-0-123-194.ec2.internal_b180ef40-a219-11e8-8d39-0adc8704f5c4 became leader
    kube-system   kube-controller-manager.154babfffe808959     Endpoints     LeaderElection      controller-manager      ip-10-0-123-194.ec2.internal_b46ac32c-a219-11e8-aa37-0adc8704f5c4 became leader
    kube-system   kube-dns-64b69465b4.154bac008da66e90         ReplicaSet    SuccessfulCreate    replicaset-controller   Created pod: kube-dns-64b69465b4-qn59j
    kube-system   kube-dns.154bac008c601e32                    Deployment    ScalingReplicaSet   deployment-controller   Scaled up replica set kube-dns-64b69465b4 to 1
    kube-system   kube-dns-64b69465b4-qn59j.154bac008db3439e   Pod           FailedScheduling    default-scheduler       no nodes available to schedule pods

* Control plane logs

- [[https://docs.aws.amazon.com/eks/latest/userguide/control-plane-logs.html]]

Control plane logs are shipped to CWLogs since Apr 2019.

    Now, you can send control plane logs including: audit, API server,
    authenticator, controller-manager, and scheduler logs to Amazon CloudWatch
    Logs. Each selected log stream is sent to Amazon CloudWatch Logs within your
    account, and stored accordingly.
    Source: https://aws.amazon.com/about-aws/whats-new/2019/04/amazon-eks-now-delivers-kubernetes-control-plane-logs-to-amazon-/

* Costs

EKS is .20/hour.
150 dollars per cluster per month.

* Lack of advanced LB support

It doesn't support ALB/NLB yet.

* Limitations

Set node type of internet access at node level:

    Disable SNAT if you need to allow inbound communication to your pods from external
    VPNs, direct connections, and external VPCs, and your pods do not need to access
    the Internet directly via an IGW. In other words, disabling SNAT is incompatible
    with nodes running in a public subnet; your nodes need to run in a private subnet
    and connect to the internet through an AWS NAT Gateway or another external NAT device.
    Source: https://aws.amazon.com/blogs/opensource/vpc-cni-plugin-v1-1-available/

Reduce IP allocation overhead:

    The EKS CNI plugin creates a “warm pool” of IP addresses by pre-allocating IP addresses
    on EKS nodes to reduce scheduling latency. In other words: because the instance already
    has IP addresses allocated to it, Kubernetes doesn’t need to wait for an IP address to
    be assigned before it can schedule a pod. However, there are some tradeoffs in this
    approach: if your EKS nodes are larger instance types and can support larger numbers
    of IP addresses, you might find that your nodes are hogging more IP addresses than you
    want.
    You can use the WARM_IP_TARGET environment variable to tune the size of the IP address
    “warm pool.” You can define a threshold for available IP addresses below which
    L-IPAMD creates and attaches a new ENI to a node, allocates new IP addresses,
    and then adds them to the warm pool. This threshold can be configured using the
    WARM_IP_TARGET environment variable; it can also be configured in amazon-vpc-cni.yaml.
    Source: https://aws.amazon.com/blogs/opensource/vpc-cni-plugin-v1-1-available/

** Reduced visibility

AWS is hiding system pods and master nodes.
kubectl doesn't show them to you.

* Fargate

- [[https://aws.amazon.com/blogs/aws/amazon-eks-on-aws-fargate-now-generally-available/]]

Fargate for EKS exists but it's crazy.
You will need to run and pay also for pods like coredns and the pricing is high.
Plus will need time to spin up the nodes:

    With AWS Fargate, you pay only for the amount of vCPU and memory resources
    that your pod needs to run. This includes the resources the pod requests in
    addition to a small amount of memory needed to run Kubernetes components
    alongside the pod. Pods running on Fargate follow the existing pricing model.
    vCPU and memory resources are calculated from the time your pod’s container
    images are pulled until the pod terminates, rounded up to the nearest second.
    A minimum charge for 1 minute applies. Additionally, you pay the standard
    cost for each EKS cluster you run, $0.20 per hour.
    Source: https://aws.amazon.com/blogs/aws/amazon-eks-on-aws-fargate-now-generally-available/

You also pay the control plane.
