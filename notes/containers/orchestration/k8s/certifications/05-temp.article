TEMP

* Certificates

vault + tls bootstrapping
every 24h

etcd did not reload cers for clients connection using ip addresses
-> when you reconnect etcd re-read the cert in the background (no need to reload etcd)
_. it doesn't work with ip adress just with server name
it was using the server in the client hello to identify which cert to use
FIXED UPSTREAM

k8s master components don't reload Certificates
once a day they need to restart all component

vault agent + vault sidekick
every node when it starts need to read the token from vaut and if vault is not
there it will fail

* containerd

is lightweight and great dev team
bad is not as battle tested as docker, small issues, many tools assume docker
ugly: shim sometimes hangs and requires kill -9

the latest version of docker (14 oct 2018) use the same version of containerd
so basically same issues. side the shim.

.image images/healthchecks.png
.caption Source: [[https://www.youtube.com/watch?v=2dsCwp_j0yQ]]

every two minets do docker ps
if it doesn reply reply
and the same for the kubelet
all this on GKE

* network overlay

.image images/network_overlay.png
.caption Source: [[https://www.youtube.com/watch?v=2dsCwp_j0yQ]]

A lot of network overhead. consumes cpu.

.image images/native_pod_routing.png
.caption Source: [[https://www.youtube.com/watch?v=2dsCwp_j0yQ]]


.image images/pod_native_routing.png
.caption Source: [[https://www.youtube.com/watch?v=2dsCwp_j0yQ]]

going native pod routing
all cloud provider are going that direction

aws
cni plugin from lyft [[https://github.com/lyft/cni-ipvlan-vpc-k8s]]
using secondary interface on the container based on ipvlan
it allocates more network interfaces and gives them an ip
and directly forward them to the pods

they wanted to avoid overlays
avoid bridges (PTP or IPVLAN) -
they wanted to route traffic between k8s clusters without going to elb and the sort
but it's a beta features from the cloud provider


* ingresses and default

.image images/net_ingresses.png
.caption Source: [[https://www.youtube.com/watch?v=2dsCwp_j0yQ]]

ingresses you need to use nodeport
all nodes are register to the elb
and all traffic reach one of the node
sending traffic to a 1000k nodes
like kafka
sending traffic to a node port with another app that is latency sensitive
they wanted to have traffic directly from elb to pods

they connect lb directly to pods
more efficient (not all nodes to the lb) - but HA??? registration time?
very recent on - unstable cloud provider features

* kube-proxy

a lot of iptables
they moved to IPVS (huawei team)

faster traffic and refresg
cleaner almost no iptables rules

BUT accessing services from the host doesn't work
no localhost:nodeport
no gracerful temrination
ALL FIXED IN 1.12

* IPV6 and DNS

race condition in the conntrack code
disable ipv6 in kernel (giblibvc) - alpine use mozzle and that cannot be fixed
also had to disable native go resolution
both ipv4 and ipv6

* Cloud integration

different lb behaviour
if a node joins the cloud controller too fast and the cloud controller query aws but the node it still in staing up state it will remove it and the node never joins the cluster
cidr allocatior type only work on gce

aws provider almost no doc

localVolumeProvider and mount paths - if you don't use the disk uid in the mount path when you monunt local volume. if the nodes comes back with the same name the volumnes will collide
almost never tested on large cluster
cluster autoscaler doesn't work with more than 50ASG
kubed abandonmed (same configmaps across all namespaces) -0 using more than 10gb
metric server doesn't start up with a single server in NotReady state
kube-state-metrics: normal way to check stuf, pod node collector generates 100MB payload https://github.com/appscode/kubed
voyager ingress controller, map sort bug lead to continos pod creation. https://github.com/appscode/voyager


for 100 nodes to 1000

be carefulk with fs, cpu, MemorytargetRAM is useful (avoid OOM)

controller schedule run them in different nodes because they will conflix for resources
and only way it to scale them vertically (the node)
they compete for CPU
you need to be careful because localhost is hardcoded

connection with ectd are imbalance
they shuffle etcd endpoints for api servers
works well

coreDNS
a replica for every 16 nodes in the cluster

    nodesPerReplica: 16

memory limits leading to OOMkills (mem usage with 'pods: verified')
scale up vertically
cache a copy of all the pods in your cluster

create 200 depoyments with 100 pods each
maxing up what the scheduler can do
put it in a queue and then work through it

###############

deamonset DDOS yourself on big clusters
because they poll the api servers too much (also DDOS vault)
hit cloud providers API rate-limits
if you put ImagePullPolocy always and a deamons - if it crashes with will DDOS AWS
slow rollout 5% at the time otherwise is dangerous
possible solution 1.12 scheduler alpha

statefulset with persistent volumes
when you stop a pod
first freeze the cgroup and be sure than nothing is forking and happening
and then killed
but doesn't work if you are waiting for IO

local volumes
when you delete a node and another one comes with the same names
but to generate the name uses file, node and class

create a PV based on the local volume
and that PV is the same name
and k8s can't create a pv with the same name
local node provisioner need to put UUID in the mount path

ebs scheduler beware the zones

zombies
careful with *exec-based*probes*
difficult to debug issue with things started by exec and not started frm the start
use tini as pid 1 (or shared pid namespace)
https://github.com/krallin/tini

complex process trees and many open files are very hard
on the runtime so beware when porting monolith from servers into containers

OOM killer
limits too low will trigger cgrup oom
requests too low (or 0) will trigger system oom
defnitely put limits in cgroup
don't allow people to take everything

InitContainers
limit rangers are enforced on int containers

init containers are USUALLY not restarted

#####################

all api servers have audit Logs
use them
very good to debug issues on API servers

#####################

the developer create the ASG and choose the taints and toleration
what nodes they run and what workloads they run on that nodes

#####################

they use native pod communicatiuon to allow speaking across clusters
because they cannot make a single k8s cluster with 5K nodes.
using load balancer for all of this is too complicated.

- [[https://www.youtube.com/watch?v=2dsCwp_j0yQ]]
- [[https://www.slideshare.net/lbernail/kubernetes-at-datadog-the-very-hard-way]]
